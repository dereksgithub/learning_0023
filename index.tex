% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

<script src="site_libs/fitvids-2.1.1/fitvids.min.js"></script>
\KOMAoption{captions}{tableheading}
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={CASA0023 Remote Sensing Learning Diary},
  pdfauthor={Mengyu Ding},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{CASA0023 Remote Sensing Learning Diary}
\author{\href{https://github.com/dereksgithub}{Mengyu Ding}}
\date{2024-02-01}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[boxrule=0pt, interior hidden, borderline west={3pt}{0pt}{shadecolor}, breakable, sharp corners, enhanced, frame hidden]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\bookmarksetup{startatroot}

\hypertarget{overview}{%
\chapter*{Overview}\label{overview}}
\addcontentsline{toc}{chapter}{Overview}

\markboth{Overview}{Overview}

This is the learning diary for CASA0023 Remotely Sensing Cities and
Environments.

\hypertarget{table-of-content}{%
\section*{Table of Content}\label{table-of-content}}
\addcontentsline{toc}{section}{Table of Content}

\markright{Table of Content}

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{personal-introduction}{Personal Introduction}
\item
  Part I. Intro to Remote Sensing

  \begin{itemize}
  \tightlist
  \item
    \protect\hyperlink{week-1-introduction-to-remote-sensing}{Week 1
    Introduction to Remote Sensing}
  \item
    \protect\hyperlink{week-2-quarto-and-xaringan}{Week 2 Quarto and
    Xaringan}
  \item
    \protect\hyperlink{week-3-atmosphere-corrections}{Week 3 Atmosphere
    Corrections}
  \item
    \protect\hyperlink{week-4-remote-sensing-and-policy-implications}{Week
    4 Remote Sensing and Policy Implications}
  \item
    \protect\hyperlink{week-5-more-on-temperature-and-group-work-discussion}{Week
    5 More on Temperature and Group Work Discussion}
  \end{itemize}
\item
  Part II. Remote Sensing with GEE

  \begin{itemize}
  \tightlist
  \item
    \protect\hyperlink{week-6-introduction-to-google-earth-engine}{Week
    6 Introduction to Google Earth Engine}
  \item
    \protect\hyperlink{week-7-classification-with-google-earth-engine-i}{Week
    7 Classification with GEE Part 1}
  \item
    \protect\hyperlink{week-8-classification-with-google-earth-engine-ii}{Week
    8 Classification with GEE Part 2}
  \item
    \protect\hyperlink{week-9-synthetic-aperture-radar}{Week 9 Synthetic
    Aperture Radar}
  \end{itemize}
\item
  \protect\hyperlink{week-10-group-presentations}{Week 10 Group
  Presentation Week}
\item
  \protect\hyperlink{remote-sensing-term-recap}{Summary}
\item
  \protect\hyperlink{acronyms-and-key-concepts-in-remote-sensing}{Remote
  Sensing Acronyms}
\item
  \protect\hyperlink{references}{Reference}
\end{itemize}

\bookmarksetup{startatroot}

\hypertarget{personal-introduction}{%
\chapter*{Personal Introduction}\label{personal-introduction}}
\addcontentsline{toc}{chapter}{Personal Introduction}

\markboth{Personal Introduction}{Personal Introduction}

Hi, welcome to my CASA0023 Learning Diary. I'll be using this website
built with \href{https://quarto.org/}{Quarto} as my major recording
system for weekly practicals and writings. This could not be possible
without our amazing lecturer
\href{https://github.com/andrewmaclachlan}{Dr.~Maclachlan}, our awesome
PGTAs (Postgraduate Teaching Assisstants) and additional GEE support
(and cool GEE app demos) from
\href{https://oballinger.github.io/}{Dr.~Ballinger}.

My name is Mengyu Ding, I am an engineer by training. Graduated from
University of Alberta, Edmonton, Canada, in 2015 with a B.Sc in
Electrical Engineering. Afterwards, I have been lucky to work in the ITS
(Intelligent Transportation Systems) industry and engineering consulting
industry for many years. I've also got a Master of Information
Technology from Monash University, Melbourne, Australia, in 2019. After
my first master's degree, I have been working as a graduate engineer for
John Holland Group in Melbourne for a few years and then moved back to
China to join a few interesting start-ups and scale-ups as
product/solutions manager in the autonomous driving (V2X side) and smart
cities industry. Career-wise, I am interested in transportation and
smart cities mostly. I attend UCL to gear my career towards urban
mobility and smart cities. The support and learning at UCL has been
quite great. (and of course, the studying is sometimes sleep-depriving
too!)

Besides working, I like hiking, cooking and reading science literature
or science fictions. I also enjoy building small gadgets, for example, I
was into hand-made Corsi-Rosenthal boxes during 2021 and 2022.

GIS Crew and I (in the middle) Surveying Hu-Hang-Yong Highway in
Zhejiang Province for Installing RSUs (Road Side Units)

My Hand-made Portable Air-Filtering System (HEPA-13 + 2 x turbo fans +
Active charcoal Filter) Powered bn a Powerbank

\part{I. Intro to Remote Sensing}

\hypertarget{week-1-introduction-to-remote-sensing}{%
\chapter*{Week 1 Introduction to Remote
Sensing}\label{week-1-introduction-to-remote-sensing}}
\addcontentsline{toc}{chapter}{Week 1 Introduction to Remote Sensing}

\markboth{Week 1 Introduction to Remote Sensing}{Week 1 Introduction to
Remote Sensing}

\hypertarget{overview-1}{%
\section*{Overview}\label{overview-1}}
\addcontentsline{toc}{section}{Overview}

\markright{Overview}

For the first week, we are introduced to the concept of remote sensing
and how to download data and images collected by satellites and perform
basic analysis with SNAP and R. Remote sensing is part of GIS but it
involves more than just GIS.

\hypertarget{remote-sensing-overview}{%
\section*{Remote Sensing Overview}\label{remote-sensing-overview}}
\addcontentsline{toc}{section}{Remote Sensing Overview}

\markright{Remote Sensing Overview}

\begin{itemize}
\item
  Active vs Passive sensors

  \begin{itemize}
  \tightlist
  \item
    Active: Emits EM waves or other energy from its own sensors (radar,
    LiDar etc.) to measure and capture the landscapes/features of the
    area of interest.
  \item
    Passive: Detects and captures the natural energy signals radiated by
    of the area of interest.
  \end{itemize}
\item
  A key concept in remote sensingis the electromagnetic waves and their
  interaction with the atmosphere and the Earth's surface. I have also
  revisited the concept of
  \href{https://www.encyclopedie-environnement.org/en/zoom/diffusion-reflection-refraction-and-diffraction-of-light/}{diffusion,
  reflection, refraction and diffraction.} Understanding such
  interactions is pivotal for interpreting satellite imagery and other
  remote sensing data. Thus, correction of atmospheric interference
  accurately is one of the most important directions of work for remote
  sensing.
\item
  Data in Remote Sensing:

  There are many types of data in remote sensing, we can broadly
  categorise them into the following:

  \begin{itemize}
  \tightlist
  \item
    Spectral Data: Measure of light reflectance or emission across the
    electromagnetic spectrum (different wavelength).
  \item
    Spatial Data: Location information, size, shape or other features of
    the Earth's surface.
  \item
    Temporal Data:
    ``\href{https://desktop.arcgis.com/en/arcmap/latest/map/time/what-is-temporal-data.htm}{Simply
    data that represents a state in time}.'' i.e.~the remote sensing
    data with time stamps.
  \item
    Radiometric Data: The intensity of radiation captured by sensors,
    indicating the percentage of the sensed wavelength is being emitted
    or reflected.
  \item
    Resolution Data: Describing the raw data's spatial, spectral,
    temporal, and radiometric resolution.
  \item
    Multispectral Data: It means the data that contains two or more sets
    of EM frequencies.
  \item
    Hyperspectral Data: Data from all available bands on spectrum.
  \item
    Thermal Data: Data collected from thermal remote sensing, usually
    collected from a sensor's thermal band.
  \item
    LiDar Data: Utilise laser to collect point-cloud data thus capturing
    three-dimensional information about the Earth's surface.
  \item
    Radar Data: Data collected from sampling back-scattered
    electromagnetic waves.
  \item
    Optical Data: Data from the visible, near infrared and
    short-wave-infrared bands on the EM spectrum.
  \item
    Gravity and Magnetic Field Data: offering insights into the
    gravitational and magnetic conditions of the Earth.
  \item
    Metadata: information that describes a dataset, usually covering
    dataset's content, time of collection, quality, publisher, and other
    characteristics.
  \end{itemize}
\item
  Four resolutions: Remotely sensed data and applications will vary
  based on the four resolutions

  \begin{itemize}
  \tightlist
  \item
    Spatial: Refers to the size of one pixel on the ground (e.g.~20cm or
    30m) Higher Spatial resolution means finer details.
  \item
    Spectral: Describes the number and width of spectral bands the
    sensor records data in.
  \item
    Temporal: The frequency with which a sensor revisits the same
    location. A chart of temporal resolution
    \href{https://link.springer.com/article/10.1007/s10712-021-09637-5/figures/4}{for
    Visible/NIR Satellites}. (Sutlieff, Berthoud, and Stinchcombe 2021)
  \item
    Radiometric: identify differences in light or reflectance, in
    practice this is the range of possible values, for example, an 8-bit
    sensor has values between 0 and 255 (256 possibilities), captures
    much fewer energy levels than an 11-bit sensor has values between 0
    and 2047 (2048 possibilities), in this sense, the 11-bit sensors
    captures finer-grained data.
  \end{itemize}
\end{itemize}

\hypertarget{case-study-of-bristol}{%
\section*{Case Study of Bristol}\label{case-study-of-bristol}}
\addcontentsline{toc}{section}{Case Study of Bristol}

\markright{Case Study of Bristol}

For this practical, I picked Bristol as its distinct urban and suburb
area split. The data is downloaded from
\href{https://dataspace.copernicus.eu/}{EU Copernicus Data Portal} and
\href{https://earthexplorer.usgs.gov}{Earth Explorer}. The Sentinel 2A
data is from April 17th, 2022 and the Landsat 8 data is from Sept 07th,
2023.

First, I performed a scatter analysis for the greater Bristol area that
I have picked for the analysis, this includes a large portion of suburb
land, which indicates high biomass in the analysis.

\begin{figure}

{\centering \includegraphics[width=8.26042in,height=\textheight]{images/wk1/scatterplot.png}

}

\caption{Scatter Plot Analysis}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=7.66667in,height=\textheight]{images/wk1/histogram.png}

}

\caption{Histogram Analysis}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=7.65625in,height=\textheight]{images/wk1/sent2.png}

}

\caption{Sentinel 2 Image}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=7.64583in,height=\textheight]{images/wk1/Screenshot 2024-01-29 190738.png}

}

\caption{Sentinel and Landsat Side-by-side}

\end{figure}

Later on, I performed the down sampling for Sentinel-2A images to align
its resolution with Landsat images.

\begin{figure}

{\centering \includegraphics[width=5.15625in,height=\textheight]{images/wk1/resampling.png}

}

\caption{Down-sampling of Sentinel-2A Images}

\end{figure}

\hypertarget{results}{%
\section*{Results}\label{results}}
\addcontentsline{toc}{section}{Results}

\markright{Results}

After down-sampling Sentinel images and cross-analyzing it with LandSat
of the POI, downtown area of Bristol, the result is recorded below,
indicating high urban.

\begin{figure}

{\centering \includegraphics[width=6.73958in,height=\textheight]{images/wk1/Screenshot 2024-01-29 202650.png}

}

\caption{Week 1 Result}

\end{figure}

\hypertarget{literature-review-and-applications}{%
\section*{Literature Review and
Applications}\label{literature-review-and-applications}}
\addcontentsline{toc}{section}{Literature Review and Applications}

\markright{Literature Review and Applications}

Remote sensing is certainly more than what I have previously pictured,
which involves mainly a great deal of image processing. On the contrary,
it consists of a balanced amount of geophysics, actual physics, and last
but not least geography. (Navalgund, Jayaraman, and Roy 2007) In short,
it is an interdisciplinary realm comprised of decades of studies and
applications. While traditional image processing or computer vision
certainly plays an important role, (Wilkinson 1999) further development
in remote sensing certainly will demand a deep understanding of GIS,
atmospheric science, spectral analysis, environmental science, ecology,
and sensor technology etc. (Batty 2013) Recent development in the
research of transportation highlights the integration of remote sensing
data into hybrid models for specific urban topics such as travel
behaviour prediction.(Wang et al. 2024) The utility of remote sensing
has traditionally been rooted in agricultural applications, however, its
relevance to urban studies is increasingly being recognized, such as its
application in urban sustainability. (Sabri et al. 2022)

\hypertarget{reflections}{%
\section*{Reflections}\label{reflections}}
\addcontentsline{toc}{section}{Reflections}

\markright{Reflections}

Aside from the metaphysical overview of remote sensing as an interesting
area of research, I further browsed around
\href{https://dataspace.copernicus.eu/}{EU Copernicus Data Portal} and
\href{https://earthexplorer.usgs.gov}{Earth Explorer} to see what data I
can have access to, for future research uses within and beyond the
module. This week's introduction to remote sensing is intriguing, as
when I was young I was always curious about our planet and I used to
spend quite some time reading \href{http://www.dili360.com/nh/}{Nature
History} / \href{http://www.dili360.com/}{Chinese National Geography}.
(and later on \href{https://www.nationalgeographic.com/}{National
Geographic} in English) I have known that remote sensing enables the
modern map makers to create highly-accurate images but never had the
opportunity to dive deep into this realm.

Remote sensing plays a pivotal role in environment monitoring and
management which could potentially benefit many aspects of urban
development and research offering key insights for urban resilience and
combating climate change. Back when I was working with other ITS
engineers in China, I have witnessed how remote sensing and UAV sensing
is used for calibrating HD Maps. During that time, my tasks was
primarily centred on the Vehicle-to-Everything (V2X) implementations and
solution design. However, engaging in discussions about the application
of Light Detection and Ranging (LiDar) technology in Unmanned Aerial
Vehicle surveys, and the subsequent processing of the acquired raw data,
presented an intriguing aspect of the project. My initial industrial
knowledge of how satellite works and their communication bands etc. come
from \protect\hyperlink{personal-introduction}{the person standing} on
my left, Mr.~Ji, who was an experienced GIS engineer fluent in UAV
survey data processing. He was a happy engineer and always willing to
share his technical take, many of my UCL classmates has the same energy
as Mr.~Ji back in the days.

\hypertarget{week-2-quarto-and-xaringan}{%
\chapter*{Week 2 Quarto and Xaringan}\label{week-2-quarto-and-xaringan}}
\addcontentsline{toc}{chapter}{Week 2 Quarto and Xaringan}

\markboth{Week 2 Quarto and Xaringan}{Week 2 Quarto and Xaringan}

Here is the slide made with Xaringan for CloudSat and its Cloud
Profiling Radar (CPR).

\href{https://dereksgithub.github.io/practicalquarto0023/\#1}{Here is
the full screen slide!}

\hypertarget{week-3-atmosphere-corrections}{%
\chapter*{Week 3 Atmosphere
Corrections}\label{week-3-atmosphere-corrections}}
\addcontentsline{toc}{chapter}{Week 3 Atmosphere Corrections}

\markboth{Week 3 Atmosphere Corrections}{Week 3 Atmosphere Corrections}

This week we covered the corrections and enhancements for satellite
imagery. It is importance to select proper appropriate imagery
(Collection, Level, Tier) and understanding limitations like
Top-of-Atmosphere (TOA) vs.~Bottom-of-Atmosphere (BOA) reflectance. For
this week's diary, I \textbf{presented the practical walk-through first}
and then summarised the lecture content with slight extension to other
metrics that may be helpful. The application and literature review
follows the course content summary.

\hypertarget{case-study-of-bristol---continued}{%
\section*{Case Study of Bristol -
Continued}\label{case-study-of-bristol---continued}}
\addcontentsline{toc}{section}{Case Study of Bristol - Continued}

\markright{Case Study of Bristol - Continued}

This week's topic is on correction, the author carried on with the
Bristol study as the Bristol area happens to have overlap of Landsat 8
and 9. (Landsat 8 C1L1 for the first part of the practical time of
capture is Jan19 to Jan29, 2024)

\begin{figure}

{\centering \includegraphics[width=6.23958in,height=\textheight]{images/wk3/Screenshot 2024-02-17 173551.png}

}

\caption{Selecting the Landsat 8 and 9 Data with Overlap at Bristol for
the second part of practical}

\end{figure}

Landsat 8 image corrected (Dark Object Subtraction method) with adjusted
reflectance values that account for atmospheric effects, leading to a
more accurate image.

\begin{figure}

{\centering \includegraphics[width=6.73958in,height=\textheight]{images/wk3/wk3_01_band_2to4.png}

}

\caption{DN Results}

\end{figure}

The values of the estimated minimum digital number (DN) across the image
for Band 2, 3, and 4 that are considered to be affected by haze are 19,
12, and 8. Among the three, band 2 shows the most atmospheric scattering
effects while band 4 shows the least around the red part of the
spectrum.

\begin{figure}

{\centering \includegraphics[width=6.75in,height=\textheight]{images/wk3/000010.png}

}

\caption{Corrected Bristol Area for Landsat 8 (Or a large area that
includes Bristol)}

\end{figure}

\hypertarget{merging-images}{%
\subsection*{Merging Images}\label{merging-images}}
\addcontentsline{toc}{subsection}{Merging Images}

In the second part of the practical, I downloaded Landsat 8 and 9, L2,
with cloud cover 0 to 5\% taken at Nov/25/2023 and Sep/05/2023
respectively.

\begin{figure}

{\centering \includegraphics[width=6.1875in,height=\textheight]{images/wk3/ndvi_allplots.png}

}

\caption{NDVI at different levels}

\end{figure}

After the NDVI plots, I clipped the shapefile of the Bristol surrounding
area. (Again, it is acually a large area that includes Bristol and
multiple other cities/towns)

\begin{figure}

{\centering \includegraphics[width=4.80208in,height=\textheight]{images/wk3/Screenshot 2024-02-17 181559.png}

}

\caption{QGis Clipping}

\end{figure}

\begin{figure}

{\centering \includegraphics{images/wk3/wk3_pca_importance.png}

}

\caption{PCA Result Summary}

\end{figure}

After performing dimensioality reduction with PCA, the first 3
components contain 86.6\% of variances. To analyse the source data
collected here, the dimension can be reduced to PC1 to PC4 only, as
they, in total, explains 95.098\% of variance. Compared to the smaller
study area tailored towards the urban areas of Bristol in
\protect\hyperlink{week-6-introduction-to-google-earth-engine}{week 6},
this result is certainly less . This is because the area is largely
grass and forest, not building-dense, Bristol city areas.

\begin{figure}

{\centering \includegraphics[width=6.75in,height=\textheight]{images/wk3/wk3_1st_pca_plot.png}

}

\caption{PCA Result Plots}

\end{figure}

\hypertarget{glcm}{%
\subsection*{GLCM}\label{glcm}}
\addcontentsline{toc}{subsection}{GLCM}

Using GLCM (Grey-Level C0-occurrence Matrix) to perform texture analysis
for Landsat images. Homogeneity measures the closeness of the
distribution of elements in the GLCM to the GLCM diagonal, indicating
areas where pixel values are similar to their neighbors, while
reflecting uniform textures within the image. Second moment, or energy,
quantifies texture uniformity an indicator where higher values denote
areas with more consistent or smooth textures.

Correlation assesses how a pixel's value is predictably related to its
neighbors, with high values indicating a linear or predictable
relationship in gray-level values across the texture. For GLCM, Mean
refers to the average intensity or gray level within the analyzed
window, offering insights into the overall brightness or reflectance of
the area being studied. GLCM's applications are covered
\protect\hyperlink{week-6-introduction-to-google-earth-engine}{here}.

\begin{figure}

{\centering \includegraphics[width=6.75in,height=\textheight]{images/wk3/glcm.png}

}

\caption{GLCM 4 Methods}

\end{figure}

\hypertarget{summary-of-correction-indices}{%
\section*{Summary of Correction
Indices}\label{summary-of-correction-indices}}
\addcontentsline{toc}{section}{Summary of Correction Indices}

\markright{Summary of Correction Indices}

The area of correction methods or ratio enhancement in remote sensing is
rich and growing, I have summarised some key methods below:

Normalized Difference Vegetation Index (NDVI) is a remote sensing index
that measures the health and density of vegetation on the Earth's
surface. NIR represents the near-infrared light reflected by vegetation,
and RED represents the visible red light reflected by vegetation.

\begin{itemize}
\tightlist
\item
  \(NDVI = \frac{(NIR - RED)}{(NIR+RED)}\)
\end{itemize}

Aside from the NDVI, here are some other indicators that I have
investigated and summarized:

\begin{itemize}
\item
  \textbf{ARVI (Atmospherically Resistant Vegetation Index)} is commonly
  employed to tackle regions of high atmospheric aerosol content. It was
  originally proposed for remote sensing of EOS MODIS sensor. (Kaufman
  and Tanre 1992) The range for an ARVI is -1 to 1 where green
  vegetation generally falls between values of 0.20 to 0.80.

  \begin{itemize}
  \item
    \(ARVI = \frac{(NIR - RED - y * (RED - BLUE))}{(NIR + RED - y*(RED-BLUE))}\)
  \item
    According to ArcGIS:
    \href{https://www.arcgis.com/home/item.html?id=56a6db4cdd1c46988a1411d0365fd5f7}{``ARVI
    is a vegetation-based index that minimizes the effects of
    atmospheric scattering caused by rain, fog, dust, smoke, or air
    pollution.''}
  \item
    y is a constant to correct for the atmospheric effects caused by
    aerosol scattering in red channel.
  \end{itemize}
\item
  \textbf{NDMI (Normalized Difference Moisture Index)} helps monitoring
  and mapping water content in soil and vegetation. (Wilson and Sader
  2002) It is calculated as Near-Infrared's and Short-Wave Infrared's
  difference over sum.:

  \(NDMI = \frac{NIR - SWIR}{NIR + SWIR}\)

  \begin{itemize}
  \tightlist
  \item
    In Landsat 8-9: \(NDMI =\frac{(Band5 - Band6)}{(Band5+Band6)}​\)
  \end{itemize}
\item
  \textbf{SAVI (Soil-Adjusted Vegetation Index)} is originally
  constructed to minimize soil brightness influences on NDVI, especially
  useful in areas with sparse vegetation. (AR Huete 1988)

  \begin{itemize}
  \item
    \(SAVI =\frac{(NIR+Red+L)}{(NIR−Red)}​∗(1+L)\)
  \item
    SAVI introduces a soil brightness correction factor (L) to adjust
    the NDVI calculation, making it more accurate in sparse vegetation
    areas.
  \item
    In Landsat 8-9: \(SAVI - \frac{Band5 - Band4}{Band5 + Band4 +0.5}\)
  \end{itemize}
\item
  \textbf{MSAVI (Modified Soil-Adjusted Vegetation Index)} is an
  adjustment of SAVI, designed to minimize bare soil background effects
  more effectively and optimize vegetation monitoring. (Qi et al. 1994)

  \begin{itemize}
  \item
    \(MSAVI = \frac{2*NIR + 1 - \sqrt{(2*NIR+1)-8*(NIR-RED)}}{2}\)
  \item
    In Landsat 8-9:
    \(SAVI - \frac{2*Band5 + 1 - \sqrt{(Band5+1)^2 -8*(Band5-Band4)}}{2}\)
  \end{itemize}
\item
  \textbf{GNDVI (Green Normalized Difference Vegetation Index)} focuses
  on chlorophyll content, primarily on the green spectral region for
  enhanced sensitivity to vegetation density. (Gitelson, Kaufman, and
  Merzlyak 1996)

  \begin{itemize}
  \item
    \[NDVI = \frac{NIR - Green}{NIR +Green}\]
  \item
    Landsat 8-9: \(NDMI =\frac{Band5 - Band3}{Band5 + Band3}\)
  \end{itemize}
\item
  \textbf{EVI (Enhanced Vegetation Index)} was proposed to improve the
  sensitivity in high biomass regions and improves vegetation monitoring
  through a de-coupling of the canopy background signal and a reduction
  in atmospheric influences. (Alfredo Huete et al. 2002)

  \begin{itemize}
  \item
    \(EVI=G \frac{(NIR−Red)}{(NIR+C1⋅Red−C2⋅Blue+L)}\)
  \item
    In Landsat 8-9:
    \(EVI = 2.5 * \frac{Band5 - Band4}{Band5 + 6* Band4 - 7.5*Band2 + 1}\)
  \end{itemize}
\item
  \textbf{NDWI (Normalized Difference Water Index)} was formulated for
  the detection of liquid water and moisture content of vegetation and
  soil.(Gao 1996)

  \begin{itemize}
  \item
    \(NDWI_{veg​}= \frac{(NIR+Green)}{(NIR−Green)}\)
  \item
    In Landsat 8-9: \(NDWI = \frac{Band4-Band2}{Band4+band2}\)
  \end{itemize}
\end{itemize}

These indicators, are sufficient only for basic analysis demands, to
derive more hidden relations, more modern techniques such as deep
learning,(Ma et al. 2019) and Semi-supervised representation learning
(Yan et al. 2020) can also be employed.

\hypertarget{literature-review-and-applications-1}{%
\section*{Literature Review and
Applications}\label{literature-review-and-applications-1}}
\addcontentsline{toc}{section}{Literature Review and Applications}

\markright{Literature Review and Applications}

Integrating the most appropriate remote sensing metrics for urban
analytics is pivotal effective city planning or addressing other
built-environment challenges. Among these measures, NDVI as is widely
used for monitoring urban vegetation and urban green space,(Xue, Su, et
al. 2017) whereas NDWI plays a critical role in urban water body and
drought monitoring, it is also important for urban fire prevention.(Zhu
2017) Other metrics, such as ARVI, EVI, SAVI, MSAVI, and NDMI are still
relevant in the industry and contemporary urban science, but recent
studies call for both enhancement of the indices and nuanced
applications. As suggested in many publications, such correction methods
can be further refined to optimized the data contamination brought by
atmospheric or other factors. Zhou et al. (2021) proposed the use of
HANTS model (Harmonic Analysis of Time Series) to further optimize the
parameter settings for NDVI reconstruction at a global scale and over
longer period of time. Such methods are unique and well-rooted under
today's hype of LLM or ``trying DL/complex machine learning frameworks
on all problems''. Thus, investigations on further optimising the
traditional correction methods with explainable models future research
remains relevant and important.

\hypertarget{reflection}{%
\section*{Reflection}\label{reflection}}
\addcontentsline{toc}{section}{Reflection}

\markright{Reflection}

Reflecting on this week's learning, it's evident that the field of
remote sensing is both complex and immensely powerful. Although, I am
still seeing just seeing the tip of the iceberg by far, I do believe in
the future of interdisciplinary potentials of remote sensing with city
research. The process of selecting appropriate RS imagery, applying
correction techniques, and utilizing various indices for environmental
analysis is intricate but essential for extracting meaningful and
reliable insights from the satellite data.(Jensen 1996) This knowledge
not only enriched my technical skills but also enhanced myappreciation
for the meticulous work behind the scenes of environmental monitoring
and analysis.

In my continued case study of Bristol, served as a concept preview of
the corrections in remote sensing and its applicability in real-world
scenarios such as vegetation monitoring. The processed illustrated the
importance of careful data selection, processing, and analytically flow
in achieving robust outcomes. Furthermore, the discussion on correction
methods and the summary of details of various vegetation and soil
indices provided a comprehensive understanding of the tools and
techniques.

As we continue our journey through the remote sensing module, this
week's focus on image correction and enhancements has laid a solid
foundation for understanding the nuanced internal mechanics of satellite
data processing. It has also sparked my curiosity about how these
techniques can be applied to other areas of study, especially for urban
resilience, smart cities design and disaster management. The insights
gained this week will prepare us well regarding the basics for our
future remote sensing projects.

\hypertarget{week-4-remote-sensing-and-policy-implications}{%
\chapter*{Week 4 Remote Sensing and Policy
Implications}\label{week-4-remote-sensing-and-policy-implications}}
\addcontentsline{toc}{chapter}{Week 4 Remote Sensing and Policy
Implications}

\markboth{Week 4 Remote Sensing and Policy Implications}{Week 4 Remote
Sensing and Policy Implications}

In this week the lecture covered remote sesnsing and its involvement
with policies. We first reviewed and extended on the significance of
spectral bands and wavelength, then moved to remote sensing's
application in environmental monitoring applications, urban green
spaces, disaster responses, policy implications, remote sensing's
integration with GIS, and how spatial data analysis is incorporated into
policy making.

\hypertarget{summary-of-remote-sensing-and-policy-implications}{%
\section*{Summary of Remote Sensing and Policy
Implications}\label{summary-of-remote-sensing-and-policy-implications}}
\addcontentsline{toc}{section}{Summary of Remote Sensing and Policy
Implications}

\markright{Summary of Remote Sensing and Policy Implications}

The lecture first introduced the significance of multi-temporal land
cover mapping in understanding and managing environmental changes, where
remote sensing is particularly meaningful for researchers and
policymakers to identify land use changes, deforestation, and urban
sprawl. (Bhatta, Saraswati, and Bandyopadhyay 2010)

Another major application of remote sensing in terms of shaping the
policies, is environmental monitoring. With remote sensing measures such
as Synthetic Aprture Radar (SAR), for example, environmental researchers
and policy makers are able to monitor the changes of the forests (Hansen
et al. 2013) therefore formulate instruments that directly contributes
to containing illegal logging. Another key application for remote
sensing is for disaster monitoring and response, with high-quality and
timely remote sensing data, policymakers now have the capability to
tackle large-scale disastrous events such as droughts and forest fires.
(Van Westen 2000)

With the insights extracted from remote sensing, cities can now monitor
and construct strategies for complicated problems such as urban heat
island, (Stefanov and Brazel 2007) inner city floods, (Li, Xu, and Chen
2016) further increasing the urban resilience. (Ghaffarian, Kerle, and
Filatova 2018) By incorporating spatial data and remote sensing into the
making of policy instruments through aligning with the global
sustainability agendas such as the New Urban Agenda and the Sustainable
Development Goals (SDGs), stakeholders from all aspects can work
together to face challenges raised by rapid urbanisation.

\hypertarget{wildfire-monitoring-for-air-quality-with-remote-sensing}{%
\section*{Wildfire Monitoring for Air Quality with Remote
Sensing}\label{wildfire-monitoring-for-air-quality-with-remote-sensing}}
\addcontentsline{toc}{section}{Wildfire Monitoring for Air Quality with
Remote Sensing}

\markright{Wildfire Monitoring for Air Quality with Remote Sensing}

Remote sensing and policy implications are intricately linked, playing a
pivotal role in addressing some of our most pressing challenges. The
United Nations Sustainable Development Goals (UN SDGs) outline numerous
objectives for urban and built environments. Recently, mountain and bush
fires have significantly deteriorated the air quality in major parts of
the world, to make the matters worse, the smogs can travel far from time
to time, threatening the health and quality of life for a great many
residents.

Consequently, my case study, centred around Melbourne and its adjacent
suburbs, aims to explore the integration of remote sensing for wildfire
monitoring with urban air quality forecasting. This short study intends
identify the potentials of integrating remote sensing into the current
pipelines, thus provide policymakers and city councils with sufficient
lead time to implement measures before smog affects residential areas.
Potential policy integration could include distribution of air
filtration systems, masks or the evacuation of affected populations.
This pilot study seeks to investigate the policy ramifications of
establishing such operational frameworks.

\hypertarget{a-case-study-and-policy-implications-for-the-greater-melbourne-area}{%
\subsection*{A Case Study and Policy Implications for the Greater
Melbourne
Area}\label{a-case-study-and-policy-implications-for-the-greater-melbourne-area}}
\addcontentsline{toc}{subsection}{A Case Study and Policy Implications
for the Greater Melbourne Area}

The goal for this short study is to explore the forecasting of smoke
plume bush-fire to areas that may impact the residents and discuss
policy implications. Focusing on Melbourne, with framework that includes
remote sensing, intended to be integrated with sustainable future
planning inspired the use of remote sensing for monitoring urban growth
in Perth, WA. (MacLachlan et al. 2017)

As for the imagery, Sentinel 5P from Copernicus Program is offering
sufficient resolutions. In terms of spatial resolution it covers up to a
5.5km x 3.5km area, Sentinel 5P also offers a daily global coverage,
(revisits less than one day) providing enough temporal resolution. As
for spectral resolution, Sentinel 5P offers accurate quantification of
various atmospheric gases and pollutants including carbon monoxide(CO),
nitrogen dioxide (NO2), ozone(O3), formaldehyde(HCHO), sulfur
dioxide(SO2), methane(CH4) and aerosols thanks to its extensive spectral
coverage which includes UV, VIS and NIR bands.(Victoria 2019) It also
possesses radiometric resolution at satisfactory level which enables the
sensors on-board to distinguish different levels of signal intensity.
The following is a video summary of the 2019 to 2020 bushfire air
pollutant spread in Australia which resulted in low AQI in a great
portion of the country's residential areas not to mention destruction of
many homes. (Tiernan and O'Mallon 2020)

Utilizing the Sentinel 5P data(Trinder and Liu 2020), this proposed
project aligns with the following UN Sustainable Development Goals:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5167}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.0458}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4375}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Contribution (by Implementing Proposed Research/Development)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
SDG
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Contribute to the prevention of human respiratory damage caused by smogs
from burning forest. & UN SDG 3 & Good Health and Well-being, ensure
people lives a healthy environment, promote well-being for all ages. \\
Proper monitoring and timely alerts, contributes to the enhancement of
the built environment and overall community safety. & UN SDG 11 &
Sustainable, resilient, inclusive and safe cities/communities. \\
Actively monitoring wild fires and smogs produced can offer insights and
guidance for mitigation. & UN SDG 13 & Take urgent actions against
climate change. \\
This proposal can contribute to the sustainable of land resources and
terrestrial ecosystems. & UN SDG 15 & Sustainable management of land,
forests and biodiversity. \\
\end{longtable}

NASA's Fire Information for Resource Management System (FIRMS) can also
be integrated into the workflow. Using remote sensing for monitoring
wild fire monitoring has long been proposed for Eastern Australia.
(Milne 1986)

By extracting values from the daily Sentinel 5p data, we can monitor the
wildfire and combine the ground weather/air quality monitoring data, to
determine how the polluted air will impact the densely populated area
(mapping the air monitoring data over the building footprint
\href{https://github.com/microsoft/AustraliaBuildingFootprints}{in
Australia}) thus, provide intervention implications for distributing
devices and masks. In the literature, similar was workflow proposed for
monitoring wild fires in the US. (Kochanski et al. 2021)

\begin{figure}

{\centering \includegraphics[width=6.92708in,height=\textheight]{images/wk4/senti5p_no2_melb.png}

}

\caption{Sentinel 5P, Feb 01 to Feb 06, 2020, NO2 level Around
Melbourne}

\end{figure}

As in many regions of the world, residential AQI can be dramatically
affected by wildfires, therefore, it is crucial to have a integrated
pipeline for the monitoring of harmful aerosol spike which is also
incorporated into the policy stack and is dedicated with ample
resources.

\begin{figure}

{\centering \includegraphics[width=6.92708in,height=\textheight]{images/wk4/senti5p_melb_2019_12_01_to_2020_02_06.png}

}

\caption{Sentinel 5P image Overlay with Map around (well, again,
``around'' and covering VIC and NSW) Melbourne (Time window set to
December 01, 2019 to February 06, 2020)}

\end{figure}

\hypertarget{current-policy-stack-and-wofkflow}{%
\subsection*{Current Policy Stack and
Wofkflow}\label{current-policy-stack-and-wofkflow}}
\addcontentsline{toc}{subsection}{Current Policy Stack and Wofkflow}

As a vibrant metropolis in the state of Victoria, the local councils and
state government around the Greater Melbourne Area, has implemented air
quality policies aimed at safeguarding the health and well-being of
their residents.

\begin{figure}

{\centering \includegraphics[width=5.01042in,height=\textheight]{images/wk4/wk4_logic_flow.png}

}

\caption{Proposed Workflow Serving as an Add-on to the Current
Government Workflow and Policy Stack}

\end{figure}

\begin{itemize}
\item
  \textbf{Environment Protection Amendment Act 2018}: This act
  introduces a general environmental duty that requires proactive risk
  management to prevent harm to public health and the environment,
  including air quality impacts.
\item
  \textbf{EPA Victoria's AirWatch}: A platform that offers real-time air
  quality information to the public, including an AQI that categorizes
  air quality levels and provides health advisories, enabling residents
  to make informed decisions about outdoor activities.
\end{itemize}

\hypertarget{proposed-mitigation}{%
\subsection*{Proposed Mitigation}\label{proposed-mitigation}}
\addcontentsline{toc}{subsection}{Proposed Mitigation}

In light of this,this proposal aims to integrate state-of-art remote
sensing technology into Melbourne's current air quality management
workflow, offering a multifaceted approach to mitigate air pollution
risks:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Establishment of High-Frequency Sentinel Points}: By setting
  up strategically positioned sentinel points equipped with remote
  sensing capabilities, we propose to advance the forecasting of Points
  of Interest (POIs) for early air quality alerts. This system will
  enable the detection of potential air quality deterioration before
  polluted air approaches the residential vicinity, allowing for
  proactive measures to be taken.
\item
  \textbf{Deployment of Air Filtration and Respiratory Device Stations}:
  Recognizing the immediate health impacts of poor air quality, we plan
  to establish stations across key urban and suburban locations for the
  distribution of air filtering and respiratory devices. These stations
  will ensure that citizens have timely access to necessary protective
  equipment against smoke and pollutants. Ideally, the station can be
  established inside local hospitals or clinics.
\item
  \textbf{Enhanced Emergency Evacuation Planning}: Agent-Based Modeling
  (ABM) methods alongside satellite imagery analysis will be employed to
  refine emergency evacuation planning. By identifying optimal
  evacuation routes and safe zones in advance, the updated workflow will
  facilitate efficient relocation strategies during smoky conditions,
  ensuring public safety and minimizing chaos.
\end{enumerate}

\hypertarget{implementation-and-emergency-planning-techniques}{%
\subsection*{Implementation and Emergency Planning
Techniques}\label{implementation-and-emergency-planning-techniques}}
\addcontentsline{toc}{subsection}{Implementation and Emergency Planning
Techniques}

Our methodology leverages state-of-the-art remote sensing technology,
including the analysis of satellite data from platforms such as
Sentinel-5P, to offer unparalleled insights into air quality dynamics.
The integration of ABM methods will further enhance our predictive
capabilities, allowing for the simulation of various scenarios and the
formulation of robust emergency response strategies.

\hypertarget{conclusion}{%
\subsection*{Conclusion}\label{conclusion}}
\addcontentsline{toc}{subsection}{Conclusion}

By integrating advanced remote sensing technology into the city's
existing workflow, we aim not only to protect the well-being of
Melbourne's citizens but also to set a new standard for urban air
quality management worldwide.

\hypertarget{literature-review-and-applications-2}{%
\section*{Literature Review and
Applications}\label{literature-review-and-applications-2}}
\addcontentsline{toc}{section}{Literature Review and Applications}

\markright{Literature Review and Applications}

The exploration of policy and global agenda, combined with the review of
current literature, made this week's class and practical a proper
warm-up for the group assessment. I have also combed through the
literature and policy base briefly, to further analyze the interplay
between remote sensing, policy and air quality.

Early in the 2000s, studies have proposed the potentials of
incorporating remote sesing into the air quality policy frameworks.
(Veefkind et al. 2007) Aside from the potential of forecasting emergency
air pollution caused by wildfires or other incidents, recent literature
has also indicated the transformative potential of remote sensing in air
quality monitoring.(Sokhi et al. 2022) Other researchers has highlighted
the application of remote sensing for timely data on aerosol pollutants.
(Gupta et al. 2006) therefore, the integration of remote sensing air
quality monitoring, could also contribute to the approximate
verification and robust back-up for ground air quality data collection.
(Bechle, Millet, and Marshall 2013) To generalize the impact of urban
air quality monitoring with remote sensing, researchers also suggested
that remote sensing can be incorporated in the long-term exposure
assessment of air pollutants (Van Donkelaar et al. 2015) and be utilized
to discover hidden air pollution patterns. (Verma et al. 2023) Although
the current policy frameworks around the world are including satellite
images into air quality monitoring, the spatial and temporal resolution
of such observation data could be further improved, offering policy
makers and environmental agencies more detailed and timely knowledge on
the air quality. Potential research directions could explore finer
pollution detection,(Huang et al. 2022) establishing near real-time or
real time data acquisitions,(Geng et al. 2021) compatibility or fusion
with other key environmental metrics monitoring systems. (Prados et al.
2010) and a more hybrid approached in terms of acquiring air quality
data such as combining the satellite observations with UAV images.
(Budde et al. 2017)

\hypertarget{reflection-1}{%
\section*{Reflection}\label{reflection-1}}
\addcontentsline{toc}{section}{Reflection}

\markright{Reflection}

The implications of such research are profound for policy-making, it can
even contribute to a more data-driven paradigm shift as the data-driven
planning. By leveraging remote sensing data, policymakers can benefit
from timely and accurate information on the hazardous smoke plumes,
their potential movement patterns and impact on overall urban air
quality. From the test use of Sentinel 5P data shown above, we can
conclude that the resolution overall is inadequate for predicting the
movement of the pollutant clusters in the air. In addition, the remote
sensing detection and forecasting, ought to be a value-addition or
redundant measure to existing air quality measurement and monitoring
frameworks and policy instruments. It is also obvious that integrating
remote sensing data with real-time sensor data or other air quality
monitoring data can pose technique challenges.

In summary, the exploration of air quality monitoring, policy
interventions, and remote sensing, opens up a new realm of environmental
protection and policy implementation. It exemplifies the innovation
synergy between technology and policy by introducing remote sensing data
analysis to both local and global environmental strategies.

\hypertarget{concept-illustration-video}{%
\section*{Concept Illustration Video}\label{concept-illustration-video}}
\addcontentsline{toc}{section}{Concept Illustration Video}

\markright{Concept Illustration Video}

As a rather off-track side-quest, I also generated this with text-prompt
on \href{https://www.stablevideo.com/}{Stable Video}:

It is not a geographically accurate depiction as the forest distribution
is clearly off, but as a measure for quickly formatting initial
illustrations the current development is beyond my expectation.

\hypertarget{week-5-more-on-temperature-and-group-work-discussion}{%
\chapter*{Week 5 More on Temperature and Group Work
Discussion}\label{week-5-more-on-temperature-and-group-work-discussion}}
\addcontentsline{toc}{chapter}{Week 5 More on Temperature and Group Work
Discussion}

\markboth{Week 5 More on Temperature and Group Work Discussion}{Week 5
More on Temperature and Group Work Discussion}

In week 5, we had two sessions but mostly dedicated for group work
planning and discussions. This week's discussion on the temperature and
Urban Heat Island. There is no entry of learning diary required,
however, we did have most of the practical session working on our group
assessments.

\hypertarget{group-assessment-key-points}{%
\section*{Group Assessment Key
Points}\label{group-assessment-key-points}}
\addcontentsline{toc}{section}{Group Assessment Key Points}

\markright{Group Assessment Key Points}

Here are some key takeaways for group assessment references:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  How to perform the feasibility analysis? Approach from the following
  aspects:

  \begin{enumerate}
  \def\labelenumii{\roman{enumii}.}
  \tightlist
  \item
    Technical: Can we do it with the tech we have?
  \item
    Economic: Is it worth the hassle?
  \item
    Legal: Are we breaking any local or international laws?
  \item
    Operational: How will the project unravel?
  \item
    Scheduling: When to do what?
  \end{enumerate}
\item
  Common global agendas related to remote sensing and urban studies:

  \begin{enumerate}
  \def\labelenumii{\roman{enumii}.}
  \tightlist
  \item
    UN Sustainable Development Goals
  \item
    the Sendai Framework for Disaster Risk Reduction 2015-2030
  \item
    The New Urban Agenda (NUA)
  \item
    The Global Green New Deal (GND)
  \end{enumerate}
\item
  Other key points are that we agreed upon for this project is to
  develop the work covered in the pitch into Work-packs, perform SWOT
  analysis, design automated Data Pipelines, and adding a Gantt Chart.
\item
  The topic and scope is also settle with the help of Andy and our
  PGTAS! (But the brainstorming is utterly a thrill since everyone has
  contributed novel ideas to develop!)
\end{enumerate}

\part{II. Remote Sensing with GEE}

\hypertarget{week-6-introduction-to-google-earth-engine}{%
\chapter*{Week 6 Introduction to Google Earth
Engine}\label{week-6-introduction-to-google-earth-engine}}
\addcontentsline{toc}{chapter}{Week 6 Introduction to Google Earth
Engine}

\markboth{Week 6 Introduction to Google Earth Engine}{Week 6
Introduction to Google Earth Engine}

This is a very exciting week that I could not physically attend since I
was at the CUSP Data Dive (Datathon), GEE is something that interested
me before selecting this module. This week we are introduced to the
basic concepts of GEE and JavaScript.

\hypertarget{javascript-and-gee-basics}{%
\section*{JavaScript and GEE Basics}\label{javascript-and-gee-basics}}
\addcontentsline{toc}{section}{JavaScript and GEE Basics}

\markright{JavaScript and GEE Basics}

Google Earth Engine is a cloud platform developed by Google for
scientific data analysis. (Reis, Datia, and Pós-de-Mina Pato 2020) GEE
can handle large datasets in a relatively faster than many other
traditional commercial software, its mature integration with JavaScript
also enables new users to master the tool suite with a shallower
learning curve.(Cardille et al. 2022) JavaScript is the language used in
GEE code editor, it is an essential tool for front-end development,
enabling developers to implement complex and interactive websites.
JavaScript also interacts with HTML and CSS to style and build the user
interface. I have also summarised some useful introduction materials for
learning GEE \protect\hyperlink{remote-sensing-term-recap}{here in the
term summary}.

Here is a basic block to code a function:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{function} \FunctionTok{sayHi}\NormalTok{(name) \{}
    \FunctionTok{alert}\NormalTok{(}\StringTok{"Hi,"} \OperatorTok{+}\NormalTok{ name }\OperatorTok{+} \StringTok{"!"}\NormalTok{)}\OperatorTok{;}
\NormalTok{\}}

\FunctionTok{sayHi}\NormalTok{(}\StringTok{" guys, "}\NormalTok{)}\OperatorTok{;} \CommentTok{// Pops up "Hi, guys, !"}
\end{Highlighting}
\end{Shaded}

\hypertarget{gee-case-study-bristol}{%
\section*{GEE Case Study Bristol}\label{gee-case-study-bristol}}
\addcontentsline{toc}{section}{GEE Case Study Bristol}

\markright{GEE Case Study Bristol}

First, I gathered air quality data over Bristol. But the resolution from
Sentinel 5P was not ideal, thus I zoomed out in order to find places
with high NO2 levels, in the end, even London is included. At this
resolution, it is obvious that we will need more image processing and
better resolution to have fine-grained data on urban air quality

\begin{figure}

{\centering \includegraphics{images/wk6/wk06_NO2.png}

}

\caption{NO2 Level Over Bristol to London from Sentinel 5P}

\end{figure}

\hypertarget{air-quality-savi-arvi-ndvi-and-ndwi}{%
\subsection*{Air Quality, SAVI, ARVI, NDVI and
NDWI}\label{air-quality-savi-arvi-ndvi-and-ndwi}}
\addcontentsline{toc}{subsection}{Air Quality, SAVI, ARVI, NDVI and
NDWI}

In this introductory study of GEE, I have examined the NO2 from Sentinel
5P and calculated NDVI, NDWI, SAVI and ARVI for Bristol. Time from the
data used are Sentinel 2 and Sentinel 5P (for NO2) from 2021-Jan-01 to
2023-Dec-30, I made a date slider for the user to drag to select which
data from a 30-day window. Rarely, the selection won't be valid, that is
because the filter that I coded for the cloud is very low 5, and the
Sentinel 2 images tend to be more cloudy for Bristol, please refresh the
webpage and try another month. Most of the time it is working, due to
the confined time-frame, I will be optimising this app and its UI in the
future. For now, NO2 is not bounded, as the Sentinel 5P data is
low-resolution compared to Sentinel 2, thus NDVI, NDWI, SAVI and ARVI
are bounded within the Bristol image, where as NO2 layer is not.

\hypertarget{putting-everything-together}{%
\subsection*{Putting Everything
Together}\label{putting-everything-together}}
\addcontentsline{toc}{subsection}{Putting Everything Together}

Here is the
\href{https://ee-mengyu-derek-ding.projects.earthengine.app/view/brsitol-ndvindwiarvisavi-lookup}{link
to the lookup app} for Bristol, it is also embedded as follows:

For the legend in this GEE app, I have modified and derived the code
from:
\href{https://mygeoblog.com/2016/12/09/add-a-legend-to-to-your-gee-map/}{Open
Geo Blog}. The colour code gradients for the four metrics are gathered
from this \href{https://www.color-hex.com/}{website}. The boundary for
the app is Greater Bristol (or the Global Administrative Unit (GAUL)
Layer Level 2 that contains Bristol) which is larger than the shapefile
used to clip for the city boundary in the week 6 practical or the
following screenshots.

For week 6, the author downloaded the
\href{https://opendata.bristol.gov.uk/datasets/bcc::bristol-boundary/explore}{Bristol
City Boundary} shape file, uploaded to the assets and clipped out images
with only Bristol. This is smaller compared to the GAUL level 2 boundary
used in the GEE app.

\includegraphics[width=3.54167in,height=3.22917in]{images/wk6/wk6_ndvi.png}

NDVI, NDWI, SAVI and ARVI bounded by the Bristol Boundary

\hypertarget{principle-component-analysis}{%
\section*{Principle Component
Analysis}\label{principle-component-analysis}}
\addcontentsline{toc}{section}{Principle Component Analysis}

\markright{Principle Component Analysis}

GEE can run PCA for satellite data, but the browser froze for a few
minutes before the results are ready.

\includegraphics[width=3.54167in,height=3.22917in]{images/wk6/bristolbound.png}

Bristol Boundary

\includegraphics[width=3.54167in,height=3.22917in]{images/wk6/bristolclip.png}

Bristol Clip

\includegraphics[width=3.54167in,height=3.22917in]{images/wk6/glcm.png}

GLCM

\includegraphics[width=3.54167in,height=3.22917in]{images/wk6/PC1_and_PC2.png}

P1 and P2

\includegraphics[width=3.54167in,height=2.39583in]{images/wk6/PC1.png}

PC1

\includegraphics[width=3.55208in,height=\textheight]{images/wk6/PC2.png}

PC2

\includegraphics[width=3.54167in,height=\textheight]{images/wk6/PC5.png}

PC5

\includegraphics[width=3.54167in,height=2.39583in]{images/wk6/PC6.png}

PC6

PC1 explains 77.17\% of the variance in the dataset. With my visual
assessment of the GLCM and PCA results, it is obvious PC1 and PC2
captures high NDWI and highly building dense areas repectively, whereas
GLCM captures high-reflectance areas, it performs well on flat rooftops,
making it suitable for identifying buildings.(Christaki et al. 2022)
Such results are essential as the input of more complex models, such as
classification models with dimensionalities reduced to PC1, PC2 and PC3
(together they explain 95.88\% of variances). However, PC4 to PC6, or
the other PCs on condition that they are still meaningful and not
rounded to 0, can provide crucial insights to anomalies and noises.

\begin{longtable}[]{@{}lc@{}}
\caption{PCA Variance Results}\tabularnewline
\toprule\noalign{}
PC & Percentage Variance Explained \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
PC & Percentage Variance Explained \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
PC1 & 77.17 \\
PC2 & 11.40 \\
PC3 & 7.31 \\
PC4 & 2.41 \\
PC5 & 1.58 \\
PC6 & 0.11 \\
PC7 & 0.03 \\
PC8 - PC20 & 0.00 \\
\end{longtable}

\hypertarget{more-on-gee-and-literature-review}{%
\section*{More on GEE and Literature
Review}\label{more-on-gee-and-literature-review}}
\addcontentsline{toc}{section}{More on GEE and Literature Review}

\markright{More on GEE and Literature Review}

Google Earth Engine (GEE) offers a wide range of Earth data for
researchers to use with easy access, this has led to a surge in
publication using GEE. As a ECE major, I first explored the how GEE can
support a vast user group scattered around the planet, and then looked
into the recent trend in the scientific community with GEE.

\hypertarget{gee-capabilities-and-trend-in-scientific-research}{%
\subsection*{GEE Capabilities and Trend in Scientific
Research}\label{gee-capabilities-and-trend-in-scientific-research}}
\addcontentsline{toc}{subsection}{GEE Capabilities and Trend in
Scientific Research}

The
\href{https://www.sciencedirect.com/science/article/pii/S0034425717302900?via\%3Dihub}{system
architecture} of GEE is well designed to offer services at a large
scale. This design allows the GEE to host massive amount of data while
being scalable. Of course, being the birth-company for the famous
MapReduce, it is obvious that hadnling massive amount of data is what
Google does best. GEE offers a seamless integration with machine
learning, whether it is using the API from python or code editor, the
ML-workflow is well-integrated.

Due to its advantages,
\href{https://link.springer.com/article/10.1007/s12145-023-01035-2\#Fig6}{more
members from the scientific research}community have been adapting GEE
into their research in recent years.(Pham-Duc et al. 2023) GEE can
provide geo-spatial analysis at a large-scale for free and its ample
image series datasets are attractive features that drive a growing usage
from the researchers. Furthermore, GEE can meet the needs of urban
researcher since the nature or urban science research requires
multi-faceted data and spatiotemporal data at large scale. GEE's
abundant city bird-view images in with sufficient temporal richness,
encourages urban researcher to utilise it in conjunction with other
urban data, sparking novel research findings about our growing
metropolitan environment.

\includegraphics[width=3.54167in,height=3.02083in]{images/wk6/1-s2.0-S0034425717302900-gr2.jpg}

\includegraphics[width=3.54167in,height=3.02083in]{images/wk6/12145_2023_1035_Fig6_HTML.png}

(Gorelick et al. 2017) (Pham-Duc et al. 2023)

\hypertarget{gee-applications-with-glcm}{%
\subsection*{GEE Applications with
GLCM}\label{gee-applications-with-glcm}}
\addcontentsline{toc}{subsection}{GEE Applications with GLCM}

In the context of remote sensing for cities, GEE is employed in a
diverse range of new applications and publications.(Amani et al. 2020)
During the practical, I dived into the GLCM (Grey-Level Co-occurrence
Matrix) and its applications in identifying potentials buildings in
remote sensing datasets, this enabled me to have a wider range of
methodology design for both my future research and the group assessment.
There are many unique publications regarding GLCM, in the aspects of the
theoretical construct of GLCM methods, Lan et al. Lan and Liu (2018)
proposed a method to optimse the multi-scale parameters for the GLCM
texture window. On the application side, researchers developed methods
that combine the GLCM with SNIC (Simple Non-Iterative Clustering) to
process the data for classification input in LULC (Land Use and Land
Cover) research. (Tassi and Vizzari 2020) Other researchers have
extended the GLCM methods to process multi-layered data and combined the
modified GLCM with SVM (Support Vector Machine) to analyse collapsed
buildings.(Moya et al. 2019) These new developments highlight the
innovative usage and adaptation of GLCM for remote sensing research,
however, it also indicates that such numeric methods should be handled
with caution as its limitations may hinder the analysis accuracy if
proper fine-tuning are not performed.

\hypertarget{reflection-2}{%
\section*{Reflection}\label{reflection-2}}
\addcontentsline{toc}{section}{Reflection}

\markright{Reflection}

Employing Google Earth Engine (GEE) certainly rekindles memories of my
extensive collaborative efforts on diverse projects in Melbourne. During
those projects, I relied on Nearmap (which was really high-res!) to
coordinate with the design team and worked closely alongside engineers
and GIS specialists. Together, we organised design modifications and
participated the digital transformation of the construction processes.
Unlike those days when my role was more supervisory and collaborative, I
now find myself directly engaged with GEE, utilizing its powerful remote
sensing capabilities for urban analysis and environmental applications.

Although GEE offers a powerful tool suite for urban research, there may
be certain limitations. First, it fosters a level of vendor lock-in
where extensive use of GEE can lead to dependence on Google's
infrastructure, terms and conditions, posing risks to the research
workflow. Furthermore, GEE is challenging to work with offline.
Researchers with poor internet connection or working on remote sites may
struggle to gain access to GEE. Data sharing with GEE can be more
complicated than other platforms. Sharing custom-made shapefiles might
turn out to be a complex process due to GEE's data management policies
and permissions. Finally but not the least, GEE can be difficult for
non-coders to learn, what is available to it JavaScript library can be
occasionally counter-intuitive.

Despite GEE's limitations, its versatile and lightweight nature,
combined with its openness and collection of data, still weighs out its
limitations. Researching with GEE on urban topics is still promising as
the platform itself is robust and powerful for large datasets, besides,
the GEE community is open and vibrant, there are many
\protect\hyperlink{remote-sensing-term-recap}{resources} and people for
turn to for collaboration and assistance.

\hypertarget{week-7-classification-with-google-earth-engine-i}{%
\chapter*{Week 7 Classification with Google Earth Engine
I}\label{week-7-classification-with-google-earth-engine-i}}
\addcontentsline{toc}{chapter}{Week 7 Classification with Google Earth
Engine I}

\markboth{Week 7 Classification with Google Earth Engine I}{Week 7
Classification with Google Earth Engine I}

For this week's diary, I summarised a few classification methods and
studie on classification methods in remote sensing. The lecture covered
some common classification methodsm basics of classifying remote sensing
data, and its applications.

\hypertarget{classification-methods}{%
\section*{Classification Methods}\label{classification-methods}}
\addcontentsline{toc}{section}{Classification Methods}

\markright{Classification Methods}

In the context of remote sensing, there are several common
classification methods. These algorithms are designed to categorise data
points. CART (Classification and Regression Trees) builds a tree-like
structure where splits are based on the features that yield optimal
separation the data. Random Forest (RF) combines multiple CARTs, which
is a form of ensemble learning, to produce a more reliable outcome. SVM
(Support Vector Machine) finds a best-fit hyperplane that divides the
data into classes, offering good performance for high-dimensional data.
Here is a visualised tree that I have found
\href{https://wiki.q-researchsoftware.com/wiki/Machine_Learning_-_Classification_And_Regression_Trees_(CART)}{here}:

\hypertarget{classification-and-regression-trees-cart}{%
\subsection*{Classification and Regression Trees
(CART)}\label{classification-and-regression-trees-cart}}
\addcontentsline{toc}{subsection}{Classification and Regression Trees
(CART)}

CART was originally proposed by Breiman et al. Breiman et al. (1984) It
is a type of predictive algorithm that can be used for both
classification and regression. It splits data at each ``node'' with the
goal of subsetting the data into the target classes.

\begin{figure}

{\centering \includegraphics[width=3.15625in,height=\textheight]{images/wk7/tree.png}

}

\caption{Decision Tree used in the work by Winstral et al. (Winstral,
Elder, and Davis 2002) Pruned 16-node regression tree grown on Sx100
(°), D0 (dimensionless), elevation (m), net potential radiation index (W
m--2), and slope. None of the splits were based on slope. Values within
the ellipses and rectangles (terminal nodes) are the mean depth (m) of
all samples falling within that node.}

\end{figure}

Starting at the root node, CART determins which split is optimal by
examining all possible splits for each feature. Gini Impurity or MSE
(Mean Square Error) are usually used for measuring the quality of
classification or regression. Gini impurity is often measured to
evaluate the purity of nodes. Other metrics like accuracy, recall,
precision, and the F1 scores are also considered.

\hypertarget{random-forest}{%
\subsection*{Random Forest}\label{random-forest}}
\addcontentsline{toc}{subsection}{Random Forest}

An ensemble learning method for classification, there is also RF
regression, but I will focus on classification here. RF is often
constructed by building a large group of decision trees i.e.~CARTs. The
classification result is the class selected by most trees.

The model building of RF starts with Bagging or
\href{https://en.wikipedia.org/wiki/Bootstrap_aggregating}{Bootstrap
Aggregating}, this is essentially a re-sampling methods designed to
improve RF's stability. Each tree is then allowed to consider only a
subset of data, then the class with most trees' vote, was considered as
the model output.

The performance of RF is assessed by Out-of-Bag Error. OOB is usually
used for measuring models that involve Bagging. OOB records the average
prediction error on each training sample and are often referred to in
guiding the hyper-parameter-tuning regarding the trees. RF's performance
measurement also involves the ones derived from confusion matrices such
as accuracy, recall etc. similar to CART.

\hypertarget{support-vector-machine-svm}{%
\subsection*{Support Vector Machine
(SVM)}\label{support-vector-machine-svm}}
\addcontentsline{toc}{subsection}{Support Vector Machine (SVM)}

SVM is a popular yet relatively lightweight model, with linear and
nonlinear classification capacity. The fundamental principle for SVM is
to search for a optimal hyper plane that best separates the data
classes. SVM has many roots, many statisticians and computer scientists
have proposed hyperplane-like methods before, but it was formally
proposed and coined by Boser et al. (Boser, Guyon, and Vapnik 1992) in
1992.

To build a SVM, first we need to transform the data into a feature space
in which each data point was regarded as a vector. Then we construct the
hyperplane and adjust it till an optimal state (maximum margin
hyperplanes) where the distance from it to the nearest data point on
each side is maximised.

\hypertarget{confusion-matrix}{%
\subsection*{Confusion Matrix:}\label{confusion-matrix}}
\addcontentsline{toc}{subsection}{Confusion Matrix:}

A confusion matrix is calculated

to evaluate the performance of cl

Consumer's/User's accuracy refers to the probability that a pixel
classified into a given category actually represents that category on
the ground. It is calculated for each class by dividing the number of
correctly classified pixels (true positives) for that class by the total
number of pixels that were classified into that class (both true
positives and false positives).

Here's a more detailed breakdown:

\begin{itemize}
\item
  \textbf{Correct (True Positives)}: The number of instances where the
  model correctly predicted a specific class.
\item
  \textbf{Total (True Positives + False Positives)}: The sum of
  instances where the model predicted a specific class (correctly or
  incorrectly).
\end{itemize}

The Resubstitution Error Matrix, also known as a confusion matrix or
error matrix, is a specific type of confusion matrix generated by
evaluating the classification algorithm on the same dataset that was
used to train the model. In other words, it's the classification report
you get when you test your model on the training data itself.

Here's what it typically includes:

\begin{itemize}
\item
  \textbf{True Positives (TP)}: Correctly classified positive cases.
\item
  \textbf{True Negatives (TN)}: Correctly classified negative cases.
\item
  \textbf{False Positives (FP)}: Negative cases incorrectly classified
  as positive (Type I error).
\item
  \textbf{False Negatives (FN)}: Positive cases incorrectly classified
  as negative (Type II error).
\end{itemize}

The resubstitution Error Matrix is a tool for understanding the
performance of a classification model, and it's particularly useful for
identifying the types of errors a model is making. However, because it
uses the same data for both training and testing, it may not provide a
realistic estimate of the model's performance on unseen data due to over
fitting.

It's important to use this matrix with caution and to complement it with
additional validation techniques, such as cross-validation or a test on
a separate validation dataset, to evaluate the model's ability to
generalize to new, unseen data.

\hypertarget{practical-results-analysis}{%
\section*{Practical Results Analysis}\label{practical-results-analysis}}
\addcontentsline{toc}{section}{Practical Results Analysis}

\markright{Practical Results Analysis}

\begin{figure}

{\centering \includegraphics[width=3.54167in,height=3.02083in]{images/wk7/bristol_clip.png}

}

\caption{Greater Bristol Clip Median}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=3.54167in,height=3.02083in]{images/wk7/comparison.png}

}

\caption{Comparison of Satellite Layer from GEE and Classification
Result}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=3.54167in,height=3.02083in]{images/wk7/basic_classifier.png}

}

\caption{CART Classification}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=3.54167in,height=3.02083in]{images/wk7/pixel_wise_classifier.png}

}

\caption{RF with Pixel-wise Split}

\end{figure}

Overall results in GEE App:

\hypertarget{models-used-in-practical}{%
\subsection*{Models Used in Practical}\label{models-used-in-practical}}
\addcontentsline{toc}{subsection}{Models Used in Practical}

The pixel-wise approach divided the image pixel points randomly into
training (70\%) and validation (30\%) sets. The training set was then
fed to train a random forest to classify the image, and the the RF
classifier classified the Bristol clip image to create a classified map.
By visual assessment, the result for random forest with a pixel-wise
split a significant improvement.

\hypertarget{interpreting-the-model-metrics}{%
\subsection*{Interpreting the Model
Metrics}\label{interpreting-the-model-metrics}}
\addcontentsline{toc}{subsection}{Interpreting the Model Metrics}

Confusion matrix for training:

\begin{figure}

{\centering \includegraphics{images/wk7/confusion_matrix_train.png}

}

\end{figure}

The overall training accuracy is calculated at:

\begin{figure}

{\centering \includegraphics{images/wk7/training_accu.png}

}

\end{figure}

For testing the confusion matrix is:

\begin{figure}

{\centering \includegraphics{images/wk7/confusion_matrix_test.png}

}

\end{figure}

Testing Accuracy

\begin{figure}

{\centering \includegraphics{images/wk7/test_accu.png}

}

\end{figure}

This suggest a high testing accuracy, but it may also indicate
over-fitting for we did not calibrate the model carefully and perform
solid feature engineering. However, the results still proved that with
performing train-test split the model tend to curb the over-fitting
down. (the math component of Quarto had an upgrade with latex, my
previous latex stopped working, thus I have replaced the math with
pictures.)

\hypertarget{outlook-of-ssl-in-remote-sensing}{%
\section*{Outlook of SSL in Remote
Sensing}\label{outlook-of-ssl-in-remote-sensing}}
\addcontentsline{toc}{section}{Outlook of SSL in Remote Sensing}

\markright{Outlook of SSL in Remote Sensing}

The current development in the literature of remote sensing image
classification, CNN and other supervised learning approaches, although
have gained much attention in the past 10 years, may be limited. Large
amount of labelled cat images is hard to acquire, not to mention the
satellite images covering years of daily images over vast lands, on top
of that the human labeling of large EO images can be unreliable
somtimes., thus small amount of labelled data paradigm will be
promising. (Alosaimi et al. 2023)

\hypertarget{reflection-3}{%
\section*{Reflection}\label{reflection-3}}
\addcontentsline{toc}{section}{Reflection}

\markright{Reflection}

For this week's practical, I picked the water polygon from the sea near
Bristol, on further remark, I realised that the model probably did not
capture Bristol's rivers, due to the fact that I have selected sea water
as sample class for water. Such a result may also stem from the fact
that the Bristol river in the image was not significant, thus, such
algorithms for now could improve performance for a finer resolution
satellite image collection. I'm curious about water identification in an
urban context, will conduct further research on it in
\protect\hyperlink{week-8-classification-with-google-earth-engine-ii}{week
8}.

On further notice, I do find reviewing the Intertwined Roots of Machine
Learning: Statistics and Computer Science interesting. As my past
experience I haave alyyas. I could recall a concept named B-Tree that I
learnt in the big data courses back then. With Upon reviewing the
history of machine learning, I do realised how powerful
inter-disciplinary research and studies can boost the development of an
area of study. It felt like as if statistics is the Midas golden touch
for ML to boom. It was only the combination of many subjects and
paradigms that give birth to the modern statistical learning.

Naive Bayes and GMM are also interesting models that I could not revisit
in detail due to the limited time-frame. Both models are handy with
everyday light tasks such as email spam filtering and customer
segmentation, but for imagery tasks in remote sensing their capacity may
be limited, especially when it is difficult for GMM to determine optimal
number of classes.

\hypertarget{week-8-classification-with-google-earth-engine-ii}{%
\chapter*{Week 8 Classification with Google Earth Engine
II}\label{week-8-classification-with-google-earth-engine-ii}}
\addcontentsline{toc}{chapter}{Week 8 Classification with Google Earth
Engine II}

\markboth{Week 8 Classification with Google Earth Engine II}{Week 8
Classification with Google Earth Engine II}

asdasd

1.~~~~ Add more Critiques to the literature you reviewed, extend the
current research.

2.~~~~ Structure the contents better, give a clear architecture on the
right sides of the webpage.

\hypertarget{summary}{%
\section*{Summary}\label{summary}}
\addcontentsline{toc}{section}{Summary}

\markright{Summary}

\hypertarget{classification-with}{%
\subsection*{Classification with}\label{classification-with}}
\addcontentsline{toc}{subsection}{Classification with}

\hypertarget{practical-walk-through}{%
\subsection*{Practical Walk-through}\label{practical-walk-through}}
\addcontentsline{toc}{subsection}{Practical Walk-through}

\hypertarget{literature-review-and-applications-3}{%
\section*{Literature Review and
Applications}\label{literature-review-and-applications-3}}
\addcontentsline{toc}{section}{Literature Review and Applications}

\markright{Literature Review and Applications}

\hypertarget{more-practical-on-water-identifications}{%
\section*{More Practical on Water
Identifications}\label{more-practical-on-water-identifications}}
\addcontentsline{toc}{section}{More Practical on Water Identifications}

\markright{More Practical on Water Identifications}

I was curious about the identification of water back in the week 7
practical, while searching online, I discovered this an open GEE module
to identify water with deep learning developed by:
\href{https://blog.csdn.net/adhuahd/category_11656718.html}{GEE
StudyRoom on CSDN} (CSDN is the Chinese version of StackOverflow). The
code to use the module is:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{var}\NormalTok{ dataset\_lv2 }\OperatorTok{=}\NormalTok{ ee}\OperatorTok{.}\FunctionTok{FeatureCollection}\NormalTok{(}\StringTok{"FAO/GAUL/2015/level2"}\NormalTok{)}\OperatorTok{;}

\KeywordTok{var}\NormalTok{ Bristol\_point }\OperatorTok{=}\NormalTok{ ee}\OperatorTok{.}\AttributeTok{Geometry}\OperatorTok{.}\FunctionTok{Point}\NormalTok{([}\OperatorTok{{-}}\FloatTok{2.5879}\OperatorTok{,} \FloatTok{51.4545}\NormalTok{])}\OperatorTok{;}

\CommentTok{/*}
\CommentTok{The visualization and image selected ommitted}
\CommentTok{*/}

\KeywordTok{var}\NormalTok{ model }\OperatorTok{=} \PreprocessorTok{require}\NormalTok{(}\StringTok{\textquotesingle{}users/nietaoyuan/aGEECommonModule:geeDLWaterModel.js\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\KeywordTok{var}\NormalTok{ imgPredict }\OperatorTok{=}\NormalTok{ model}\OperatorTok{.}\FunctionTok{waterModel}\NormalTok{(imgMedian}\OperatorTok{,}\NormalTok{roi)}\OperatorTok{;}
\BuiltInTok{Map}\OperatorTok{.}\FunctionTok{addLayer}\NormalTok{(imgPredict}\OperatorTok{,}\NormalTok{\{}\StringTok{\textquotesingle{}palette\textquotesingle{}}\OperatorTok{:}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{\}}\OperatorTok{,}\StringTok{\textquotesingle{}water\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

Now we are able to use

\begin{figure}

{\centering \includegraphics[width=3.54167in,height=3.02083in]{images/wk8/result1_bristol_water.png}

}

\caption{Result with image layer transparent}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=3.54167in,height=3.02083in]{images/wk8/result2_bristol_water.png}

}

\caption{Result on image layer}

\end{figure}

\hypertarget{literature-review}{%
\section*{Literature Review}\label{literature-review}}
\addcontentsline{toc}{section}{Literature Review}

\markright{Literature Review}

asd

asdas

Liasd

\hypertarget{reflection-4}{%
\section*{Reflection}\label{reflection-4}}
\addcontentsline{toc}{section}{Reflection}

\markright{Reflection}

\hypertarget{week-9-synthetic-aperture-radar}{%
\chapter*{Week 9 Synthetic Aperture
Radar}\label{week-9-synthetic-aperture-radar}}
\addcontentsline{toc}{chapter}{Week 9 Synthetic Aperture Radar}

\markboth{Week 9 Synthetic Aperture Radar}{Week 9 Synthetic Aperture
Radar}

This week we cover the Synthetic Aperture Radar (SAR) and application of
SAR data. SAR employs radar to create images of the Earth's surface
which makes it an active sensor.

1.~~~~ Add more Critiques to the literature you reviewed, extend the
current research.

2.~~~~ Structure the contents better, give a clear architecture on the
right sides of the webpage.

\url{https://andrewmaclachlan.github.io/CASA0023-lecture-9/\#50}

\hypertarget{introduction-to-sar}{%
\section*{Introduction to SAR}\label{introduction-to-sar}}
\addcontentsline{toc}{section}{Introduction to SAR}

\markright{Introduction to SAR}

SAR is a powerful remote sensing method for gathering high-resolution
images of the Earth's surface.

This charac

can collect data using different polarization.

\hypertarget{sar-overview}{%
\subsection*{SAR Overview}\label{sar-overview}}
\addcontentsline{toc}{subsection}{SAR Overview}

\hypertarget{a1}{%
\subsubsection*{A1}\label{a1}}
\addcontentsline{toc}{subsubsection}{A1}

\hypertarget{aperture}{%
\subsubsection*{Aperture}\label{aperture}}
\addcontentsline{toc}{subsubsection}{Aperture}

Longer Apertures = Finer Resolution

\hypertarget{optical-vs.-radar-remote-sensing}{%
\subsection*{Optical vs.~Radar Remote
Sensing}\label{optical-vs.-radar-remote-sensing}}
\addcontentsline{toc}{subsection}{Optical vs.~Radar Remote Sensing}

Pros and Cons

Penetrating cloud cover

getting the SAR collections in GEE:

\hypertarget{recent-research}{%
\subsection*{Recent Research}\label{recent-research}}
\addcontentsline{toc}{subsection}{Recent Research}

embed js code:

\hypertarget{practical-london-outdoor-gym-identification}{%
\subsection*{Practical: London Outdoor Gym
Identification}\label{practical-london-outdoor-gym-identification}}
\addcontentsline{toc}{subsection}{Practical: London Outdoor Gym
Identification}

\bookmarksetup{startatroot}

\hypertarget{week-10-group-presentations}{%
\chapter*{Week 10 Group
Presentations}\label{week-10-group-presentations}}
\addcontentsline{toc}{chapter}{Week 10 Group Presentations}

\markboth{Week 10 Group Presentations}{Week 10 Group Presentations}

This week we had two intense sessions packed with interesting remote
sensing pitch-style presentations. The following is a Google Earth
Engine App that I have made for viewing parts of the intended raw data
for our pitch. The app may take a while to load as the Google Open
Buildings dataset is large.

Our team's name is developed from Roman Goddess of Flowers and Olympian
Goddess of the harvest and agriculture. The slides are
\href{https://dereksgithub.github.io/group_RS_casa0023/}{here}.

We are honored to be invited to ask question for TeamXYZ and their pitch
on an Air Pollution management solution-suite designed for Mumbai. We
asked about their future plans and the reply was solid and informative,
their overall design and open approach to stakeholder engagement plans
are quite impressive.

\begin{figure}

{\centering \includegraphics[width=6.54167in,height=\textheight]{images/wk10/presentationday.jpg}

}

\caption{Me Presenting Our Methodology Design}

\end{figure}

\bookmarksetup{startatroot}

\hypertarget{remote-sensing-term-recap}{%
\chapter*{Remote Sensing Term Recap}\label{remote-sensing-term-recap}}
\addcontentsline{toc}{chapter}{Remote Sensing Term Recap}

\markboth{Remote Sensing Term Recap}{Remote Sensing Term Recap}

In summary, our module was a valuable process for using remote sensing
to analyze and understand the complexities of urban environments. During
the term we learnt about creating reproducible remote sensing workflows,
policy application of remote sensing, using SNAP for basic operations
and Google Earth Engine. I have recorded some of the resources that was
helpful to my journey:

\hypertarget{remote-sensing-basics-machine-learning-and-gee}{%
\section*{Remote Sensing Basics, Machine Learning and
GEE}\label{remote-sensing-basics-machine-learning-and-gee}}
\addcontentsline{toc}{section}{Remote Sensing Basics, Machine Learning
and GEE}

\markright{Remote Sensing Basics, Machine Learning and GEE}

\begin{itemize}
\item
  \href{https://grantkim94.medium.com/cracking-principal-components-analysis-pca-part-1-1372736ebac7}{PCA}
  explained by \href{https://grantkim94.medium.com/}{Hyeon Gu Kim}
\item
  Lecture series on
  \href{https://youtu.be/VfDAd-MO94o?si=GsM4nYqmlHjU8WO1}{GIS and remote
  sensing}
\end{itemize}

\begin{itemize}
\item
  \href{https://www.youtube.com/watch?v=E2MyYxkV7oY}{Clipping Polygons}
  in GEE by Youtuber \href{https://www.youtube.com/@fech1285}{Fech}
\item
  \href{https://developers.google.com/earth-engine/datasets}{GEE data
  catalog}
\item
  \href{https://gee-community-catalog.org/}{GEE community catalog}
\item
  GEE
  \href{https://developers.google.com/earth-engine/apidocs/ee-geometry-polygon}{polygon}
\item
  GEE upload
  \href{https://stackoverflow.com/questions/68663744/how-do-you-use-a-shapefile-asset-as-your-aoi-in-gee}{your
  own shapefile}
\item
  \href{https://book.geemap.org/}{GEE Map} by Professor.
  \href{https://dot.cards/giswqs}{Qiusheng Wu}
\item
  GEE
  \href{https://developers.google.com/earth-engine/apidocs/ui-slider}{slider}
  (feature to be added to Jakarta NDVI/Building Lookup APP later )
\end{itemize}

\hypertarget{html-css-and-quarto}{%
\section*{HTML, CSS and Quarto}\label{html-css-and-quarto}}
\addcontentsline{toc}{section}{HTML, CSS and Quarto}

\markright{HTML, CSS and Quarto}

Here are a list of resources that I have used and adapted for this
Quarto book:

\begin{itemize}
\item
  \href{https://www.shecodes.io/athena/1837-how-to-put-2-images-on-one-line-with-css}{Embedding
  multiple images}
\item
  \href{https://quarto.org/docs/authoring/videos.html}{Embedding videos}
\item
  Data visualisation in
  \href{https://quarto.org/docs/interactive/ojs/}{quarto with Observable
  JS}
\item
  General introduction to
  \href{https://www.w3schools.com/css/default.asp}{CSS}
\end{itemize}

\hypertarget{future-works}{%
\section*{Future Works}\label{future-works}}
\addcontentsline{toc}{section}{Future Works}

\markright{Future Works}

The trend now seems to be building application with Python + GEE,
therefore, I am planning to use the Python / GEE tool suite for parts of
the visualization work with thesis work. Hopefully it will be
interactive and with a touch of satellite images showing the road
networks for the transport in a city.

\bookmarksetup{startatroot}

\hypertarget{acronyms-and-key-concepts-in-remote-sensing}{%
\chapter*{Acronyms and Key Concepts in Remote
Sensing}\label{acronyms-and-key-concepts-in-remote-sensing}}
\addcontentsline{toc}{chapter}{Acronyms and Key Concepts in Remote
Sensing}

\markboth{Acronyms and Key Concepts in Remote Sensing}{Acronyms and Key
Concepts in Remote Sensing}

AOI: Area of Interest.

ARVI: \protect\hyperlink{week-3-atmosphere-corrections}{Atmospherically
Resistant Vegetation Index}.

AVHRR:
\href{https://en.wikipedia.org/wiki/Advanced_very-high-resolution_radiometer}{Advanced
Very High Resolution Radiometer}.

Azimuth: This is the
\href{https://support.esri.com/en-us/gis-dictionary/azimuth}{angle
between a reference direction and the line between the observer/sensor
and a specific point of interest on the ground}.

CART:
\protect\hyperlink{week-7-classification-with-google-earth-engine-i}{Classification
and Regression Trees} builds a tree-like structure where splits are
based on the features that yield optimal separation the data.

DEM/DSM/DTM:
\href{https://www.usgs.gov/faqs/what-digital-elevation-model-dem}{Digital
Elevation Model} / Digital Surface Model / Digital Terrain Model. This
\href{https://geodetics.com/dem-dsm-dtm-digital-elevation-models/}{article}
and explains well the differences between DEM, DTM and DSM.

EO: Earth Observation, usually used as ``EO Data''.

EOS:
\href{https://eospso.nasa.gov/content/nasas-earth-observing-system-project-science-office}{Earth
Observing System}.

ESA: \href{https://www.esa.int/}{European Space Agency}.

EVI: \protect\hyperlink{week-3-atmosphere-corrections}{Enhanced
Vegetation Index}.

GIS:
\href{https://www.usgs.gov/faqs/what-geographic-information-system-gis}{Geographic
Information System}.

GAUL: Global Administrative Unit Layer is defined:
``\href{https://data.apps.fao.org/catalog/iso/1c45d658-591c-455c-b29c-cda8bc161f72}{compiles
and disseminates the best available information on administrative units
for all the countries in the world, providing a contribution to the
standardization of the spatial dataset representing administrative
units.}''.

GLCM: Gray-Level Co-occurrence Matrix.
\href{https://web.pdx.edu/~jduh/courses/Archive/geog481w07/Students/Hayes_GreyScaleCoOccurrenceMatrix.pdf}{This
lecture note} is really helpful in understanding the details of GLCM.

GNDVI: \protect\hyperlink{week-3-atmosphere-corrections}{Green
Normalized Difference Vegetation Index}.

GOES: \href{https://science.nasa.gov/mission/goes/}{Geostationary
Operational Environmental Satellite}.

GSO: Geosynchronous Orbit, this is an orbit where, from the observer's
(directly on the ground below the point of the object in orbit)
reference frame, the object in orbit is stationary.
\href{https://science.nasa.gov/learn/basics-of-space-flight/chapter5-1/}{More
detailed explanation is found here}.

IFOV: Instantaneous Field of View. Refers to the area on the ground,
that a sensor in orbit can see and measure whenever it is ordered to.

LandSat: \href{https://en.wikipedia.org/wiki/Landsat_program}{Land
Satellite}, more details on deciding which Landsat collection to import
\href{https://calekochenour.github.io/remote-sensing-textbook/01-catalog/chapter01-landsat.html}{see
here}.

LiDAR: \href{https://en.wikipedia.org/wiki/Lidar}{Light Detection and
Ranging}.

LULC: Land Use and Land Cover.

MODIS: Moderate Resolution Imaging Spectroradiometer.

MSAVI: \protect\hyperlink{week-3-atmosphere-corrections}{Modified
Soil-Adjusted Vegetation Index}.

NDMI: \protect\hyperlink{week-3-atmosphere-corrections}{Normalized
Difference Moisture Index.}.

NDVI: \protect\hyperlink{week-3-atmosphere-corrections}{Normalized
Difference Vegetation Index}.

NDWI: \protect\hyperlink{week-3-atmosphere-corrections}{Normalized
Difference Water Index}.

NOAA: \href{https://oceanservice.noaa.gov/about/}{National Oceanic and
Atmospheric Administration}.

PCA: Principal Component Analysis, a statistical technique used to
reduce data dimensionality, prioritising significant features by
transforming correlated bands into uncorrelated components.

POI: Point of Interest.

RF:
\protect\hyperlink{week-7-classification-with-google-earth-engine-i}{Random
Forest}.

ROI: Region of Interest.

SAVI: \protect\hyperlink{week-3-atmosphere-corrections}{Soil-Adjusted
Vegetation Index}.

SNAP: \href{https://step.esa.int/main/toolboxes/snap/}{Sentinel
Application Platform}, a software platform used for processing and
analyzing satellite data, mostly European Space Agency's Sentinel series
Earth Observation data.

SVM: \href{https://www.youtube.com/watch?v=efR1C6CvhmE}{Support Vector
Machine}.

UAV: Unmanned Aerial Vehicle.

\bookmarksetup{startatroot}

\hypertarget{references}{%
\chapter*{References}\label{references}}
\addcontentsline{toc}{chapter}{References}

\markboth{References}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-alosaimi2023self}{}}%
Alosaimi, Najd, Haikel Alhichri, Yakoub Bazi, Belgacem Ben Youssef, and
Naif Alajlan. 2023. {``Self-Supervised Learning for Remote Sensing Scene
Classification Under the Few Shot Scenario.''} \emph{Scientific Reports}
13 (1): 433.

\leavevmode\vadjust pre{\hypertarget{ref-amani2020google}{}}%
Amani, Meisam, Arsalan Ghorbanian, Seyed Ali Ahmadi, Mohammad Kakooei,
Armin Moghimi, S Mohammad Mirmazloumi, Sayyed Hamed Alizadeh Moghaddam,
et al. 2020. {``Google Earth Engine Cloud Computing Platform for Remote
Sensing Big Data Applications: A Comprehensive Review.''} \emph{IEEE
Journal of Selected Topics in Applied Earth Observations and Remote
Sensing} 13: 5326--50.

\leavevmode\vadjust pre{\hypertarget{ref-batty2013big}{}}%
Batty, Michael. 2013. {``Big Data, Smart Cities and City Planning.''}
\emph{Dialogues in Human Geography} 3 (3): 274--79.

\leavevmode\vadjust pre{\hypertarget{ref-bechle2013remote}{}}%
Bechle, Matthew J, Dylan B Millet, and Julian D Marshall. 2013.
{``Remote Sensing of Exposure to NO2: Satellite Versus Ground-Based
Measurement in a Large Urban Area.''} \emph{Atmospheric Environment} 69:
345--53.

\leavevmode\vadjust pre{\hypertarget{ref-bhatta2010urban}{}}%
Bhatta, Basudeb, S Saraswati, and D Bandyopadhyay. 2010. {``Urban Sprawl
Measurement from Remote Sensing Data.''} \emph{Applied Geography} 30
(4): 731--40.

\leavevmode\vadjust pre{\hypertarget{ref-boser1992training}{}}%
Boser, Bernhard E, Isabelle M Guyon, and Vladimir N Vapnik. 1992. {``A
Training Algorithm for Optimal Margin Classifiers.''} In
\emph{Proceedings of the Fifth Annual Workshop on Computational Learning
Theory}, 144--52.

\leavevmode\vadjust pre{\hypertarget{ref-breiman1984classification}{}}%
Breiman, L., J. Friedman, C. J. Stone, and R. A. Olshen. 1984.
\emph{Classification and Regression Trees}. Taylor \& Francis.
\url{https://books.google.co.uk/books?id=JwQx-WOmSyQC}.

\leavevmode\vadjust pre{\hypertarget{ref-budde2017smartaqnet}{}}%
Budde, Matthias, Till Riedel, Michael Beigl, Klaus Schäfer, Stefan
Emeis, Josef Cyrys, Jürgen Schnelle-Kreis, et al. 2017. {``SmartAQnet:
Remote and in-Situ Sensing of Urban Air Quality.''} In \emph{Remote
Sensing of Clouds and the Atmosphere XXII}, 10424:19--26. SPIE.

\leavevmode\vadjust pre{\hypertarget{ref-cardille2022cloud}{}}%
Cardille, Jeffrey A, Nicholas E Clinton, Morgan A Crowley, and David S
Saah. 2022. {``Cloud-Based Remote Sensing with Google Earth Engine:
Process and Prospects from a Large Edited Open-Access Book.''} In
\emph{AGU Fall Meeting Abstracts}, 2022:ED32D--0552.

\leavevmode\vadjust pre{\hypertarget{ref-christaki2022building}{}}%
Christaki, Marianna, Christos Vasilakos, Ermioni-Eirini Papadopoulou,
Georgios Tataris, Ilias Siarkos, and Nikolaos Soulakellis. 2022.
{``Building Change Detection Based on a Gray-Level Co-Occurrence Matrix
and Artificial Neural Networks.''} \emph{Drones} 6 (12): 414.

\leavevmode\vadjust pre{\hypertarget{ref-gao1996ndwi}{}}%
Gao, Bo-Cai. 1996. {``NDWI---a Normalized Difference Water Index for
Remote Sensing of Vegetation Liquid Water from Space.''} \emph{Remote
Sensing of Environment} 58 (3): 257--66.

\leavevmode\vadjust pre{\hypertarget{ref-geng2021tracking}{}}%
Geng, Guannan, Qingyang Xiao, Shigan Liu, Xiaodong Liu, Jing Cheng,
Yixuan Zheng, Tao Xue, et al. 2021. {``Tracking Air Pollution in China:
Near Real-Time PM2. 5 Retrievals from Multisource Data Fusion.''}
\emph{Environmental Science \& Technology} 55 (17): 12106--15.

\leavevmode\vadjust pre{\hypertarget{ref-ghaffarian2018remote}{}}%
Ghaffarian, Saman, Norman Kerle, and Tatiana Filatova. 2018. {``Remote
Sensing-Based Proxies for Urban Disaster Risk Management and Resilience:
A Review.''} \emph{Remote Sensing} 10 (11): 1760.

\leavevmode\vadjust pre{\hypertarget{ref-gitelson1996use}{}}%
Gitelson, Anatoly A, Yoram J Kaufman, and Mark N Merzlyak. 1996. {``Use
of a Green Channel in Remote Sensing of Global Vegetation from
EOS-MODIS.''} \emph{Remote Sensing of Environment} 58 (3): 289--98.

\leavevmode\vadjust pre{\hypertarget{ref-gorelick2017google}{}}%
Gorelick, Noel, Matt Hancher, Mike Dixon, Simon Ilyushchenko, David
Thau, and Rebecca Moore. 2017. {``Google Earth Engine: Planetary-Scale
Geospatial Analysis for Everyone.''} \emph{Remote Sensing of
Environment} 202: 18--27.

\leavevmode\vadjust pre{\hypertarget{ref-gupta2006satellite}{}}%
Gupta, Pawan, Sundar A Christopher, Jun Wang, Robert Gehrig, YC Lee, and
Naresh Kumar. 2006. {``Satellite Remote Sensing of Particulate Matter
and Air Quality Assessment over Global Cities.''} \emph{Atmospheric
Environment} 40 (30): 5880--92.

\leavevmode\vadjust pre{\hypertarget{ref-hansen2013high}{}}%
Hansen, Matthew C, Peter V Potapov, Rebecca Moore, Matt Hancher,
Svetlana A Turubanova, Alexandra Tyukavina, David Thau, et al. 2013.
{``High-Resolution Global Maps of 21st-Century Forest Cover Change.''}
\emph{Science} 342 (6160): 850--53.

\leavevmode\vadjust pre{\hypertarget{ref-huang2022rapid}{}}%
Huang, Yuhan, Casey KC Lee, Yat-Shing Yam, Wai-Chuen Mok, John L Zhou,
Yuan Zhuang, Nic C Surawski, Bruce Organ, and Edward FC Chan. 2022.
{``Rapid Detection of High-Emitting Vehicles by on-Road Remote Sensing
Technology Improves Urban Air Quality.''} \emph{Science Advances} 8 (5):
eabl7575.

\leavevmode\vadjust pre{\hypertarget{ref-huete2002overview}{}}%
Huete, Alfredo, Kamel Didan, Tomoaki Miura, E Patricia Rodriguez, Xiang
Gao, and Laerte G Ferreira. 2002. {``Overview of the Radiometric and
Biophysical Performance of the MODIS Vegetation Indices.''} \emph{Remote
Sensing of Environment} 83 (1-2): 195--213.

\leavevmode\vadjust pre{\hypertarget{ref-huete1988soil}{}}%
Huete, AR. 1988. {``A Soil-Adjusted Vegetation Index (SAVI). Remote
Ssensing of Environment, 25, 295-309.''}

\leavevmode\vadjust pre{\hypertarget{ref-jensen1996introductory}{}}%
Jensen, John R. 1996. {``Introductory Digital Image Processing: A Remote
Sensing Perspective.''}

\leavevmode\vadjust pre{\hypertarget{ref-kaufman1992atmospherically}{}}%
Kaufman, Yoram J, and Didier Tanre. 1992. {``Atmospherically Resistant
Vegetation Index (ARVI) for EOS-MODIS.''} \emph{IEEE Transactions on
Geoscience and Remote Sensing} 30 (2): 261--70.

\leavevmode\vadjust pre{\hypertarget{ref-kochanski2021integration}{}}%
Kochanski, Adam K, Farren Herron-Thorpe, Derek V Mallia, Jan Mandel, and
Joseph K Vaughan. 2021. {``Integration of a Coupled Fire-Atmosphere
Model into a Regional Air Quality Forecasting System for Wildfire
Events.''} \emph{Frontiers in Forests and Global Change} 4: 728726.

\leavevmode\vadjust pre{\hypertarget{ref-lan2018study}{}}%
Lan, Zeying, and Yang Liu. 2018. {``Study on Multi-Scale Window
Determination for GLCM Texture Description in High-Resolution Remote
Sensing Image Geo-Analysis Supported by GIS and Domain Knowledge.''}
\emph{ISPRS International Journal of Geo-Information} 7 (5): 175.

\leavevmode\vadjust pre{\hypertarget{ref-li2016improved}{}}%
Li, Linyi, Tingbao Xu, and Yun Chen. 2016. {``Improved Urban Flooding
Mapping from Remote Sensing Images Using Generalized Regression Neural
Network-Based Super-Resolution Algorithm.''} \emph{Remote Sensing} 8
(8): 625.

\leavevmode\vadjust pre{\hypertarget{ref-ma2019deep}{}}%
Ma, Lei, Yu Liu, Xueliang Zhang, Yuanxin Ye, Gaofei Yin, and Brian Alan
Johnson. 2019. {``Deep Learning in Remote Sensing Applications: A
Meta-Analysis and Review.''} \emph{ISPRS Journal of Photogrammetry and
Remote Sensing} 152: 166--77.

\leavevmode\vadjust pre{\hypertarget{ref-maclachlan2017urban}{}}%
MacLachlan, Andrew, Eloise Biggs, Gareth Roberts, and Bryan Boruff.
2017. {``Urban Growth Dynamics in Perth, Western Australia: Using
Applied Remote Sensing for Sustainable Future Planning.''} \emph{Land} 6
(1): 9.

\leavevmode\vadjust pre{\hypertarget{ref-milne1986use}{}}%
Milne, Anthony Kinnaird. 1986. {``The Use of Remote Sensing in Mapping
and Monitoring Vegetational Change Associated with Bushfire Events in
Eastern Australia.''} \emph{Geocarto International} 1 (1): 25--32.

\leavevmode\vadjust pre{\hypertarget{ref-moya20193d}{}}%
Moya, Luis, Homa Zakeri, Fumio Yamazaki, Wen Liu, Erick Mas, and
Shunichi Koshimura. 2019. {``3D Gray Level Co-Occurrence Matrix and Its
Application to Identifying Collapsed Buildings.''} \emph{ISPRS Journal
of Photogrammetry and Remote Sensing} 149: 14--28.

\leavevmode\vadjust pre{\hypertarget{ref-navalgund2007remote}{}}%
Navalgund, Ranganath R, V Jayaraman, and PS Roy. 2007. {``Remote Sensing
Applications: An Overview.''} \emph{Current Science}, 1747--66.

\leavevmode\vadjust pre{\hypertarget{ref-pham2023trends}{}}%
Pham-Duc, Binh, Ho Nguyen, Hien Phan, and Quan Tran-Anh. 2023. {``Trends
and Applications of Google Earth Engine in Remote Sensing and Earth
Science Research: A Bibliometric Analysis Using Scopus Database.''}
\emph{Earth Science Informatics} 16 (3): 2355--71.

\leavevmode\vadjust pre{\hypertarget{ref-prados2010access}{}}%
Prados, Ana I, Gregory Leptoukh, Chris Lynnes, James Johnson, Hualan
Rui, Aijun Chen, and Rudolf B Husar. 2010. {``Access, Visualization, and
Interoperability of Air Quality Remote Sensing Data Sets via the
Giovanni Online Tool.''} \emph{IEEE Journal of Selected Topics in
Applied Earth Observations and Remote Sensing} 3 (3): 359--70.

\leavevmode\vadjust pre{\hypertarget{ref-qi1994modified}{}}%
Qi, Jiaguo, Abdelghani Chehbouni, Alfredo R Huete, Yann H Kerr, and
Soroosh Sorooshian. 1994. {``A Modified Soil Adjusted Vegetation
Index.''} \emph{Remote Sensing of Environment} 48 (2): 119--26.

\leavevmode\vadjust pre{\hypertarget{ref-reis2020primer}{}}%
Reis, Rui S, Nuno Datia, and Matilde Pós-de-Mina Pato. 2020. {``A Primer
on Understanding Google Earth Engine APIs.''} \emph{I-ETC: ISEL Academic
Journal of Electronics, Telecommunications and Computers} 6 (1): 1--11.

\leavevmode\vadjust pre{\hypertarget{ref-sabri2022editorial}{}}%
Sabri, S, A Rajabifard, Y Chen, N Chen, and H Sheng. 2022. {``Editorial:
Geospatial Understanding of Sustainable Urban Analytics Using Remote
Sensing. Remote Sens. 2022, 14, 2748.''} s Note: MDPI stays neutral with
regard to jurisdictional claims in published~\ldots.

\leavevmode\vadjust pre{\hypertarget{ref-sokhi2022advances}{}}%
Sokhi, Ranjeet S, Nicolas Moussiopoulos, Alexander Baklanov, John
Bartzis, Isabelle Coll, Sandro Finardi, Rainer Friedrich, et al. 2022.
{``Advances in Air Quality Research--Current and Emerging Challenges.''}
\emph{Atmospheric Chemistry and Physics} 22 (7): 4615--4703.

\leavevmode\vadjust pre{\hypertarget{ref-stefanov2007challenges}{}}%
Stefanov, William L, and Anthony J Brazel. 2007. {``Challenges in
Characterizing and Mitigating Urban Heat Islands---a Role for Integrated
Approaches Including Remote Sensing.''} In \emph{Applied Remote Sensing
for Urban Planning, Governance and Sustainability}, 117--35. Springer.

\leavevmode\vadjust pre{\hypertarget{ref-sutlieff2021using}{}}%
Sutlieff, Gary, Lucy Berthoud, and Mark Stinchcombe. 2021. {``Using
Satellite Data for CBRN (Chemical, Biological, Radiological, and
Nuclear) Threat Detection, Monitoring, and Modelling.''} \emph{Surveys
in Geophysics} 42: 727--55.

\leavevmode\vadjust pre{\hypertarget{ref-tassi2020object}{}}%
Tassi, Andrea, and Marco Vizzari. 2020. {``Object-Oriented Lulc
Classification in Google Earth Engine Combining Snic, Glcm, and Machine
Learning Algorithms.''} \emph{Remote Sensing} 12 (22): 3776.

\leavevmode\vadjust pre{\hypertarget{ref-tiernan2020australias}{}}%
Tiernan, Finbar, and Eamonn O'Mallon. 2020. {``Australia's 2019--20
Bushfire Season.''} \emph{The Canberra Times}, January.
\url{https://www.canberratimes.com.au/story/6600958/australias-2019-20-bushfire-season/}.

\leavevmode\vadjust pre{\hypertarget{ref-trinder2020}{}}%
Trinder, John, and Qingxiang Liu. 2020. {``Assessing Environmental
Impacts of Urban Growth Using Remote Sensing.''} \emph{Geo-Spatial
Information Science} 23 (1): 20--39.
\url{https://doi.org/10.1080/10095020.2019.1710438}.

\leavevmode\vadjust pre{\hypertarget{ref-van2015use}{}}%
Van Donkelaar, Aaron, Randall V Martin, Michael Brauer, and Brian L
Boys. 2015. {``Use of Satellite Observations for Long-Term Exposure
Assessment of Global Concentrations of Fine Particulate Matter.''}
\emph{Environmental Health Perspectives} 123 (2): 135--43.

\leavevmode\vadjust pre{\hypertarget{ref-van2000remote}{}}%
Van Westen, CJ. 2000. {``Remote Sensing for Natural Disaster
Management.''} \emph{International Archives of Photogrammetry and Remote
Sensing} 33 (B7/4; PART 7): 1609--17.

\leavevmode\vadjust pre{\hypertarget{ref-veefkind2007applicability}{}}%
Veefkind, P, RF Van Oss, H Eskes, Annette Borowiak, F Dentner, and
Julian Wilson. 2007. {``The Applicability of Remote Sensing in the Field
of Air Pollution.''} \emph{Institute for Environment and Sustainability,
Italy} 59: 1--54.

\leavevmode\vadjust pre{\hypertarget{ref-verma2023urban}{}}%
Verma, Sunita, Tanu Gangwar, Janhavi Singh, Divya Prakash, and Swagata
Payra. 2023. {``Urban Air Quality Monitoring and Modelling Using Ground
Monitoring, Remote Sensing, and GIS.''} In \emph{Geospatial Analytics
for Environmental Pollution Modeling: Analysis, Control and Management},
213--47. Springer.

\leavevmode\vadjust pre{\hypertarget{ref-sentinel}{}}%
Victoria, Environmental Protection Authority. 2019. {``EPA AirWatc.''}
\url{https://www.epa.vic.gov.au/for-community/airwatch}.

\leavevmode\vadjust pre{\hypertarget{ref-wang2024deep}{}}%
Wang, Qingyi, Shenhao Wang, Yunhan Zheng, Hongzhou Lin, Xiaohu Zhang,
Jinhua Zhao, and Joan Walker. 2024. {``Deep Hybrid Model with Satellite
Imagery: How to Combine Demand Modeling and Computer Vision for Travel
Behavior Analysis?''} \emph{Transportation Research Part B:
Methodological} 179: 102869.

\leavevmode\vadjust pre{\hypertarget{ref-wilkinson1999recent}{}}%
Wilkinson, Graeme G. 1999. {``Recent Developments in Remote Sensing
Technology and the Importance of Computer Vision Analysis Techniques.''}
\emph{Machine Vision and Advanced Image Processing in Remote Sensing:
Proceedings of Concerted Action MAVIRIC (Machine Vision in Remotely
Sensed Image Comprehension)}, 5--11.

\leavevmode\vadjust pre{\hypertarget{ref-wilson2002detection}{}}%
Wilson, Emily Hoffhine, and Steven A Sader. 2002. {``Detection of Forest
Harvest Type Using Multiple Dates of Landsat TM Imagery.''} \emph{Remote
Sensing of Environment} 80 (3): 385--96.

\leavevmode\vadjust pre{\hypertarget{ref-winstral2002spatial}{}}%
Winstral, Adam, Kelly Elder, and Robert E Davis. 2002. {``Spatial Snow
Modeling of Wind-Redistributed Snow Using Terrain-Based Parameters.''}
\emph{Journal of Hydrometeorology} 3 (5): 524--38.

\leavevmode\vadjust pre{\hypertarget{ref-xue2017significant}{}}%
Xue, Jinru, Baofeng Su, et al. 2017. {``Significant Remote Sensing
Vegetation Indices: A Review of Developments and Applications.''}
\emph{Journal of Sensors} 2017.

\leavevmode\vadjust pre{\hypertarget{ref-yan2020semi}{}}%
Yan, Peiyao, Feng He, Yajie Yang, and Fei Hu. 2020. {``Semi-Supervised
Representation Learning for Remote Sensing Image Classification Based on
Generative Adversarial Networks.''} \emph{IEEE Access} 8: 54135--44.

\leavevmode\vadjust pre{\hypertarget{ref-zhou2021optimal}{}}%
Zhou, Jie, Li Jia, Massimo Menenti, and Xuan Liu. 2021. {``Optimal
Estimate of Global Biome---Specific Parameter Settings to Reconstruct
NDVI Time Series with the Harmonic ANalysis of Time Series (HANTS)
Method.''} \emph{Remote Sensing} 13 (21): 4251.

\leavevmode\vadjust pre{\hypertarget{ref-zhu2017change}{}}%
Zhu, Zhe. 2017. {``Change Detection Using Landsat Time Series: A Review
of Frequencies, Preprocessing, Algorithms, and Applications.''}
\emph{ISPRS Journal of Photogrammetry and Remote Sensing} 130: 370--84.

\end{CSLReferences}



\end{document}
