[
  {
    "objectID": "week7.html#classification-methods",
    "href": "week7.html#classification-methods",
    "title": "Week 7 Classification with Google Earth Engine I",
    "section": "Classification Methods",
    "text": "Classification Methods\nIn the context of remote sensing, there are several common classification methods. These algorithms are designed to categorise data points. CART (Classification and Regression Trees) builds a tree-like structure where splits are based on the features that yield optimal separation the data. Random Forest (RF) combines multiple CARTs, which is a form of ensemble learning, to produce a more reliable outcome. SVM (Support Vector Machine) finds a best-fit hyperplane that divides the data into classes, offering good performance for high-dimensional data. Here is a visualised tree that I have found here:\n\n\n\n\n\nClassification and Regression Trees (CART)\nCART was originally proposed by Breiman et al. Breiman et al. (1984) It is a type of predictive algorithm that can be used for both classification and regression. It splits data at each “node” with the goal of sub-setting the data into the target classes.\n\n\n\nDecision Tree used in the work by Winstral et al. (Winstral, Elder, and Davis 2002) Pruned 16-node regression tree grown on Sx100 (°), D0 (dimensionless), elevation (m), net potential radiation index (W m–2), and slope. None of the splits were based on slope. Values within the ellipses and rectangles (terminal nodes) are the mean depth (m) of all samples falling within that node.\n\n\nStarting at the root node, CART determines which split is optimal by examining all possible splits for each feature. Gini Impurity or MSE (Mean Square Error) are usually used for measuring the quality of classification or regression. Gini impurity is often measured to evaluate the purity of nodes. Other metrics like accuracy, recall, precision, and the F1 scores are also considered.\n\n\nRandom Forest\nAn ensemble learning method for classification, there is also RF regression, but I will focus on classification here. RF is often constructed by building a large group of decision trees i.e. CARTs. The classification result is the class selected by most trees.\nThe model building of RF starts with Bagging or Bootstrap Aggregating, this is essentially a re-sampling methods designed to improve RF’s stability. Each tree is then allowed to consider only a subset of data, then the class with most trees’ vote, was considered as the model output.\nThe performance of RF is assessed by Out-of-Bag Error. OOB is usually used for measuring models that involve Bagging. OOB records the average prediction error on each training sample and are often referred to in guiding the hyper-parameter-tuning regarding the trees. RF’s performance measurement also involves the ones derived from confusion matrices such as accuracy, recall etc. similar to CART.\n\n\nSupport Vector Machine (SVM)\nSVM is a popular yet relatively lightweight model, with linear and nonlinear classification capacity. The fundamental principle for SVM is to search for a optimal hyper plane that best separates the data classes. SVM has many roots, many statisticians and computer scientists have proposed hyperplane-like methods before, but it was formally proposed and coined by Boser et al. (Boser, Guyon, and Vapnik 1992) in 1992.\nTo build a SVM, first we need to transform the data into a feature space in which each data point was regarded as a vector. Then we construct the hyperplane and adjust it till an optimal state (maximum margin hyperplanes) where the distance from it to the nearest data point on each side is maximised. The performance measurement for SVM is similar to other classification methods.\n\n\nConfusion Matrix:\nA confusion matrix is calculated to evaluate the performance of classification methods. It records, for example, each row indicates the actual class A is labelled by the model to be class A to Z. With confusion matrix we can calculate the following:\n\nTrue Positives (TP): Correctly classified positive cases.\nTrue Negatives (TN): Correctly classified negative cases.\nFalse Positives (FP): Negative cases incorrectly classified as positive (Type I error).\nFalse Negatives (FN): Positive cases incorrectly classified as negative (Type II error).\n\nFrom such a matrix, we can calculate metrics such as Consumer’s/User’s accuracy used in the practical, which refers to the probability that a pixel classified into a given class actually represents that category in reality. It is calculated as follows:\n\nCorrect (True Positives): The number of instances where the model correctly identified as a specific class.\nTotal (True Positives + False Positives): The sum of instances where the model recognised to be in a specific class (correctly or incorrectly).\n\nThe Resubstitution Error Matrix used in the practical, is a confusion matrix calculated for training process. Testing confusion matrix is the confusion matrix for testing. The resubstitution Error Matrix is a tool for understanding the performance of a classification model, and it’s particularly useful for identifying the archetypes of errors a model tend to make, helping users to identify model limits. However, because it uses the same data for both training and testing, it does not realistically reflect the model’s performance on unseen data due to over-fitting, for that, testing confusion matrix is more relied upon. It is crucial to treat confusion matrix with caution, as it alone will not reduce variance or bias of the model, usually it is combined with additional validation techniques, such as k-fold cross-validation or a test on another validation dataset."
  },
  {
    "objectID": "week7.html#practical-results-analysis",
    "href": "week7.html#practical-results-analysis",
    "title": "Week 7 Classification with Google Earth Engine I",
    "section": "Practical Results Analysis",
    "text": "Practical Results Analysis\n\n\n\n\nGreater Bristol Clip Median\n\n\n\n\n\nComparison of Satellite Layer from GEE and Classification Result\n\n\n\n\n\nCART Classification\n\n\n\n\n\nRF with Pixel-wise Split\n\n\n\nOverall results in GEE App:\n\n\n\n\n\nModels Used in Practical\nIn the practical, we used the pixel-wise approach to divide the image pixel points randomly into training (70%) and validation (30%) sets. The training set was then fed to train a random forest to classify the image, and the the RF classifier classified the Bristol clip image to create a classified map. By visual assessment, the result for random forest with a pixel-wise split a significant improvement.\n\n\nInterpreting the Model Metrics\nConfusion matrix for training:\n\n\n\n\n\nThe overall training accuracy is calculated at:\n\n\n\n\n\nFor testing the confusion matrix is:\n\n\n\n\n\nTesting Accuracy\n\n\n\n\n\nThis suggest a high testing accuracy, but it may also indicate over-fitting for we did not calibrate the model carefully and perform solid feature engineering. However, the results still proved that with performing train-test split the model tend to curb the over-fitting down. (the math component of Quarto had an upgrade with latex, my previous latex stopped working, thus I have replaced the math with screenshots of them.)"
  },
  {
    "objectID": "week7.html#outlook-of-ssl-in-remote-sensing",
    "href": "week7.html#outlook-of-ssl-in-remote-sensing",
    "title": "Week 7 Classification with Google Earth Engine I",
    "section": "Outlook of SSL in Remote Sensing",
    "text": "Outlook of SSL in Remote Sensing\nThe current development in the literature of remote sensing image classification, CNN and other supervised learning approaches, although have gained much attention in the past 10 years, may be limited. Large amount of labelled cat images is hard to acquire, not to mention the satellite images covering years of daily images over vast lands, on top of that the human labeling of large EO images can be unreliable somtimes., thus small amount of labelled data paradigm will be promising. (Alosaimi et al. 2023)"
  },
  {
    "objectID": "week7.html#reflection",
    "href": "week7.html#reflection",
    "title": "Week 7 Classification with Google Earth Engine I",
    "section": "Reflection",
    "text": "Reflection\nFor this week’s practical, the model probably did not capture Bristol’s rivers, this is probably caused by the model’s inability to tell the water from high urban input, likely attributed by my selection of input polygons. I’m curious about water identification in an urban context, as it is important to identify the water body at a pixel and object level. (Huang et al. 2015)\nIn addition, recalling my previous learning experience with learnt index in B-tree and R-tree algorithms, I could not help but to review the symbiotic relationship between statistics and computer science as the fundamentals of ML are often intriguing. This review further illustrate me the transformative impact of inter-disciplinary research on statistical learning, sort of a Midas touch to what fueled the current phase of machine learning research and application.\nWhile time and space constraints precluded my more detailed revisit to Naive Bayes and GMM, they are interesting models nonetheless, both are still popular in the IT industry. These models are swift for everyday tasks such as email spam filtering and customer segmentation, but for imagery tasks in remote sensing their capacity may be limited, especially when it is difficult for GMM to determine optimal number of classes. With my endeavor into remote sensing and other machine learning paradigms such as SSL or RL, I would also propose that continuing updating the framework and innovating towards potential paradigm shifts of classification for remote sensing is still of high significant.\n\n\n\n\nAlosaimi, Najd, Haikel Alhichri, Yakoub Bazi, Belgacem Ben Youssef, and Naif Alajlan. 2023. “Self-Supervised Learning for Remote Sensing Scene Classification Under the Few Shot Scenario.” Scientific Reports 13 (1): 433.\n\n\nBoser, Bernhard E, Isabelle M Guyon, and Vladimir N Vapnik. 1992. “A Training Algorithm for Optimal Margin Classifiers.” In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, 144–52.\n\n\nBreiman, L., J. Friedman, C. J. Stone, and R. A. Olshen. 1984. Classification and Regression Trees. Taylor & Francis. https://books.google.co.uk/books?id=JwQx-WOmSyQC.\n\n\nCheng, Gong, Xingxing Xie, Junwei Han, Lei Guo, and Gui-Song Xia. 2020. “Remote Sensing Image Scene Classification Meets Deep Learning: Challenges, Methods, Benchmarks, and Opportunities.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 13: 3735–56.\n\n\nHuang, Xin, Cong Xie, Xing Fang, and Liangpei Zhang. 2015. “Combining Pixel-and Object-Based Machine Learning for Identification of Water-Body Types from Urban High-Resolution Remote-Sensing Imagery.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 8 (5): 2097–2110.\n\n\nMuhtar, Dilxat, Xueliang Zhang, Pengfeng Xiao, Zhenshi Li, and Feng Gu. 2023. “Cmid: A Unified Self-Supervised Learning Framework for Remote Sensing Image Understanding.” IEEE Transactions on Geoscience and Remote Sensing.\n\n\nSubramanian, Sriram Ganapathi, and Mark Crowley. 2017. “Learning Forest Wildfire Dynamics from Satellite Images Using Reinforcement Learning.” In Conference on Reinforcement Learning and Decision Making. Ann Arbor MI.\n\n\nTornede, Tanja, Alexander Tornede, Jonas Hanselle, Felix Mohr, Marcel Wever, and Eyke Hüllermeier. 2023. “Towards Green Automated Machine Learning: Status Quo and Future Directions.” Journal of Artificial Intelligence Research 77: 427–57.\n\n\nWang, Yuhao, Lifan Yao, Gang Meng, Xinye Zhang, Jiayun Song, and Haopeng Zhang. 2024. “Addressing Sample Inconsistency for Semi-Supervised Object Detection in Remote Sensing Images.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing.\n\n\nWinstral, Adam, Kelly Elder, and Robert E Davis. 2002. “Spatial Snow Modeling of Wind-Redistributed Snow Using Terrain-Based Parameters.” Journal of Hydrometeorology 3 (5): 524–38.\n\n\nYuan, Xiaohui, Jianfang Shi, and Lichuan Gu. 2021. “A Review of Deep Learning Methods for Semantic Segmentation of Remote Sensing Imagery.” Expert Systems with Applications 169: 114417."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Personal Introduction",
    "section": "",
    "text": "Hi, welcome to my CASA0023 Learning Diary. I’ll be using this website built with Quarto as my major recording system for weekly practicals and writings. This could not be possible without our amazing lecturer Dr. Maclachlan, our awesome PGTAs (Postgraduate Teaching Assisstants) and additional GEE support (and cool GEE app demos) from Dr. Ballinger.\nMy name is Mengyu Ding, I am an engineer by training. Graduated from University of Alberta, Edmonton, Canada, in 2015 with a B.Sc in Electrical Engineering. Afterwards, I have been lucky to work in the ITS (Intelligent Transportation Systems) industry and engineering consulting industry for many years. I’ve also got a Master of Information Technology from Monash University, Melbourne, Australia, in 2019. After my first master’s degree, I have been working as a graduate engineer for John Holland Group in Melbourne for a few years and then moved back to China to join a few interesting start-ups and scale-ups as product/solutions manager in the autonomous driving (V2X side) and smart cities industry. Career-wise, I am interested in transportation and smart cities mostly. I attend UCL to gear my career towards urban mobility and smart cities. The support and learning at UCL has been quite great. (and of course, the studying is sometimes sleep-depriving too!)\nBesides working, I like hiking, cooking and reading science literature or science fictions. I also enjoy building small gadgets, for example, I was into hand-made Corsi-Rosenthal boxes during 2021 and 2022.\n\n\n\n\nGIS Crew and I (in the middle) Surveying Hu-Hang-Yong Highway in Zhejiang Province for Installing RSUs (Road Side Units)\n\n\n\n\n\nMy Hand-made Portable Air-Filtering System (HEPA-13 + 2 x turbo fans + Active charcoal Filter) Powered bn a Powerbank"
  },
  {
    "objectID": "week1.html#overview",
    "href": "week1.html#overview",
    "title": "Week 1 Introduction to Remote Sensing",
    "section": "Overview",
    "text": "Overview\nFor the first week, we are introduced to the concept of remote sensing and how to download data and images collected by satellites and perform basic analysis with SNAP and R. Remote sensing is part of GIS but it involves more than just GIS."
  },
  {
    "objectID": "week1.html#remote-sensing-overview",
    "href": "week1.html#remote-sensing-overview",
    "title": "Week 1 Introduction to Remote Sensing",
    "section": "Remote Sensing Overview",
    "text": "Remote Sensing Overview\n\nActive vs Passive sensors\n\nActive: Emits EM waves or other energy from its own sensors (radar, LiDar etc.) to measure and capture the landscapes/features of the area of interest.\nPassive: Detects and captures the natural energy signals radiated by of the area of interest.\n\nA key concept in remote sensingis the electromagnetic waves and their interaction with the atmosphere and the Earth’s surface. I have also revisited the concept of diffusion, reflection, refraction and diffraction. Understanding such interactions is pivotal for interpreting satellite imagery and other remote sensing data. Thus, correction of atmospheric interference accurately is one of the most important directions of work for remote sensing.\nData in Remote Sensing:\nThere are many types of data in remote sensing, we can broadly categorise them into the following:\n\nSpectral Data: Measure of light reflectance or emission across the electromagnetic spectrum (different wavelength).\nSpatial Data: Location information, size, shape or other features of the Earth’s surface.\nTemporal Data: “Simply data that represents a state in time.” i.e. the remote sensing data with time stamps.\nRadiometric Data: The intensity of radiation captured by sensors, indicating the percentage of the sensed wavelength is being emitted or reflected.\nResolution Data: Describing the raw data’s spatial, spectral, temporal, and radiometric resolution.\nMultispectral Data: It means the data that contains two or more sets of EM frequencies.\nHyperspectral Data: Data from all available bands on spectrum.\nThermal Data: Data collected from thermal remote sensing, usually collected from a sensor’s thermal band.\nLiDar Data: Utilise laser to collect point-cloud data thus capturing three-dimensional information about the Earth’s surface.\nRadar Data: Data collected from sampling back-scattered electromagnetic waves.\nOptical Data: Data from the visible, near infrared and short-wave-infrared bands on the EM spectrum.\nGravity and Magnetic Field Data: offering insights into the gravitational and magnetic conditions of the Earth.\nMetadata: information that describes a dataset, usually covering dataset’s content, time of collection, quality, publisher, and other characteristics.\n\nFour resolutions: Remotely sensed data and applications will vary based on the four resolutions\n\nSpatial: Refers to the size of one pixel on the ground (e.g. 20cm or 30m) Higher Spatial resolution means finer details.\nSpectral: Describes the number and width of spectral bands the sensor records data in.\nTemporal: The frequency with which a sensor revisits the same location. A chart of temporal resolution for Visible/NIR Satellites. (Sutlieff, Berthoud, and Stinchcombe 2021)\nRadiometric: identify differences in light or reflectance, in practice this is the range of possible values, for example, an 8-bit sensor has values between 0 and 255 (256 possibilities), captures much fewer energy levels than an 11-bit sensor has values between 0 and 2047 (2048 possibilities), in this sense, the 11-bit sensors captures finer-grained data."
  },
  {
    "objectID": "week1.html#case-study-of-bristol",
    "href": "week1.html#case-study-of-bristol",
    "title": "Week 1 Introduction to Remote Sensing",
    "section": "Case Study of Bristol",
    "text": "Case Study of Bristol\nFor this practical, I picked Bristol as its distinct urban and suburb area split. The data is downloaded from EU Copernicus Data Portal and Earth Explorer. The Sentinel 2A data is from April 17th, 2022 and the Landsat 8 data is from Sept 07th, 2023.\nFirst, I performed a scatter analysis for the greater Bristol area that I have picked for the analysis, this includes a large portion of suburb land, which indicates high biomass in the analysis.\n\n\n\nScatter Plot Analysis\n\n\n\n\n\nHistogram Analysis\n\n\n\n\n\nSentinel 2 Image\n\n\n\n\n\nSentinel and Landsat Side-by-side\n\n\nLater on, I performed the down sampling for Sentinel-2A images to align its resolution with Landsat images.\n\n\n\nDown-sampling of Sentinel-2A Images"
  },
  {
    "objectID": "week1.html#results",
    "href": "week1.html#results",
    "title": "Week 1 Introduction to Remote Sensing",
    "section": "Results",
    "text": "Results\nAfter down-sampling Sentinel images and cross-analyzing it with LandSat of the POI, downtown area of Bristol, the result is recorded below, indicating high urban.\n\n\n\nWeek 1 Result"
  },
  {
    "objectID": "week1.html#literature-review-and-applications",
    "href": "week1.html#literature-review-and-applications",
    "title": "Week 1 Introduction to Remote Sensing",
    "section": "Literature Review and Applications",
    "text": "Literature Review and Applications\nRemote sensing is certainly more than what I have previously pictured, which involves mainly a great deal of image processing. On the contrary, it consists of a balanced amount of geophysics, actual physics, and last but not least geography. (Navalgund, Jayaraman, and Roy 2007) In short, it is an interdisciplinary realm comprised of decades of studies and applications. While traditional image processing or computer vision certainly plays an important role, (Wilkinson 1999) further development in remote sensing certainly will demand a deep understanding of GIS, atmospheric science, spectral analysis, environmental science, ecology, and sensor technology etc. (Batty 2013) Recent development in the research of transportation highlights the integration of remote sensing data into hybrid models for specific urban topics such as travel behaviour prediction.(Wang et al. 2024) The utility of remote sensing has traditionally been rooted in agricultural applications, however, its relevance to urban studies is increasingly being recognized, such as its application in urban sustainability. (Sabri et al. 2022)"
  },
  {
    "objectID": "week1.html#reflections",
    "href": "week1.html#reflections",
    "title": "Week 1 Introduction to Remote Sensing",
    "section": "Reflections",
    "text": "Reflections\nAside from the metaphysical overview of remote sensing as an interesting area of research, I further browsed around EU Copernicus Data Portal and Earth Explorer to see what data I can have access to, for future research uses within and beyond the module. This week’s introduction to remote sensing is intriguing, as when I was young I was always curious about our planet and I used to spend quite some time reading Nature History / Chinese National Geography. (and later on National Geographic in English) I have known that remote sensing enables the modern map makers to create highly-accurate images but never had the opportunity to dive deep into this realm.\nRemote sensing plays a pivotal role in environment monitoring and management which could potentially benefit many aspects of urban development and research offering key insights for urban resilience and combating climate change. Back when I was working with other ITS engineers in China, I have witnessed how remote sensing and UAV sensing is used for calibrating HD Maps. During that time, my tasks was primarily centred on the Vehicle-to-Everything (V2X) implementations and solution design. However, engaging in discussions about the application of Light Detection and Ranging (LiDar) technology in Unmanned Aerial Vehicle surveys, and the subsequent processing of the acquired raw data, presented an intriguing aspect of the project. My initial industrial knowledge of how satellite works and their communication bands etc. come from the person standing on my left, Mr. Ji, who was an experienced GIS engineer fluent in UAV survey data processing. He was a happy engineer and always willing to share his technical take, many of my UCL classmates has the same energy as Mr. Ji back in the days.\n\n\n\n\nBatty, Michael. 2013. “Big Data, Smart Cities and City Planning.” Dialogues in Human Geography 3 (3): 274–79.\n\n\nNavalgund, Ranganath R, V Jayaraman, and PS Roy. 2007. “Remote Sensing Applications: An Overview.” Current Science, 1747–66.\n\n\nSabri, S, A Rajabifard, Y Chen, N Chen, and H Sheng. 2022. “Editorial: Geospatial Understanding of Sustainable Urban Analytics Using Remote Sensing. Remote Sens. 2022, 14, 2748.” s Note: MDPI stays neutral with regard to jurisdictional claims in published ….\n\n\nSutlieff, Gary, Lucy Berthoud, and Mark Stinchcombe. 2021. “Using Satellite Data for CBRN (Chemical, Biological, Radiological, and Nuclear) Threat Detection, Monitoring, and Modelling.” Surveys in Geophysics 42: 727–55.\n\n\nWang, Qingyi, Shenhao Wang, Yunhan Zheng, Hongzhou Lin, Xiaohu Zhang, Jinhua Zhao, and Joan Walker. 2024. “Deep Hybrid Model with Satellite Imagery: How to Combine Demand Modeling and Computer Vision for Travel Behavior Analysis?” Transportation Research Part B: Methodological 179: 102869.\n\n\nWilkinson, Graeme G. 1999. “Recent Developments in Remote Sensing Technology and the Importance of Computer Vision Analysis Techniques.” Machine Vision and Advanced Image Processing in Remote Sensing: Proceedings of Concerted Action MAVIRIC (Machine Vision in Remotely Sensed Image Comprehension), 5–11."
  },
  {
    "objectID": "week2.html",
    "href": "week2.html",
    "title": "Week 2 Quarto and Xaringan",
    "section": "",
    "text": "Here is the slide made with Xaringan for CloudSat and its Cloud Profiling Radar (CPR).\n\n\n\n\n\n\n\n\nHere is the full screen slide!"
  },
  {
    "objectID": "week3.html#case-study-of-bristol---continued",
    "href": "week3.html#case-study-of-bristol---continued",
    "title": "Week 3 Atmosphere Corrections",
    "section": "Case Study of Bristol - Continued",
    "text": "Case Study of Bristol - Continued\nThis week’s topic is on correction, the author carried on with the Bristol study as the Bristol area happens to have overlap of Landsat 8 and 9. (Landsat 8 C1L1 for the first part of the practical time of capture is Jan19 to Jan29, 2024)\n\n\n\nSelecting the Landsat 8 and 9 Data with Overlap at Bristol for the second part of practical\n\n\nLandsat 8 image corrected (Dark Object Subtraction method) with adjusted reflectance values that account for atmospheric effects, leading to a more accurate image.\n\n\n\nDN Results\n\n\nThe values of the estimated minimum digital number (DN) across the image for Band 2, 3, and 4 that are considered to be affected by haze are 19, 12, and 8. Among the three, band 2 shows the most atmospheric scattering effects while band 4 shows the least around the red part of the spectrum.\n\n\n\nCorrected Bristol Area for Landsat 8 (Or a large area that includes Bristol)\n\n\n\nMerging Images\nIn the second part of the practical, I downloaded Landsat 8 and 9, L2, with cloud cover 0 to 5% taken at Nov/25/2023 and Sep/05/2023 respectively.\n\n\n\nNDVI at different levels\n\n\nAfter the NDVI plots, I clipped the shapefile of the Bristol surrounding area. (Again, it is acually a large area that includes Bristol and multiple other cities/towns)\n\n\n\nQGis Clipping\n\n\n\n\n\nPCA Result Summary\n\n\nAfter performing dimensioality reduction with PCA, the first 3 components contain 86.6% of variances. To analyse the source data collected here, the dimension can be reduced to PC1 to PC4 only, as they, in total, explains 95.098% of variance. Compared to the smaller study area tailored towards the urban areas of Bristol in week 6, this result is certainly less . This is because the area is largely grass and forest, not building-dense, Bristol city areas.\n\n\n\nPCA Result Plots\n\n\n\n\nGLCM\nUsing GLCM (Grey-Level C0-occurrence Matrix) to perform texture analysis for Landsat images. Homogeneity measures the closeness of the distribution of elements in the GLCM to the GLCM diagonal, indicating areas where pixel values are similar to their neighbors, while reflecting uniform textures within the image. Second moment, or energy, quantifies texture uniformity an indicator where higher values denote areas with more consistent or smooth textures.\nCorrelation assesses how a pixel’s value is predictably related to its neighbors, with high values indicating a linear or predictable relationship in gray-level values across the texture. For GLCM, Mean refers to the average intensity or gray level within the analyzed window, offering insights into the overall brightness or reflectance of the area being studied. GLCM’s applications are covered here.\n\n\n\nGLCM 4 Methods"
  },
  {
    "objectID": "week3.html#summary-of-correction-indices",
    "href": "week3.html#summary-of-correction-indices",
    "title": "Week 3 Atmosphere Corrections",
    "section": "Summary of Correction Indices",
    "text": "Summary of Correction Indices\nThis week’s lecture covered the corrections in remote sensing data processing, highlighting various techniques designed to enhance the accuracy of satellite imagery. It began with geometric correction which is performed using backward mapping which ensures that every pixel corresponds to a value in the original input image. Correction is also essential for minimising the effect of atmospheric interference, methods such as DOS (Dark Object Subtraction), and empirical line correction are introduced to convert digital brightness to scaled surface reflectance.\nThe discussion extended to data joining and enhancement, highlighting “mosaicking” which creates seamless image mosaic from multiple satellite images for analysis. The area of correction methods or ratio enhancement in remote sensing is rich and growing, I have summarised some key band ratioing methods below:\nNormalized Difference Vegetation Index (NDVI) is a remote sensing index that measures the health and density of vegetation on the Earth’s surface. NIR represents the near-infrared light reflected by vegetation, and RED represents the visible red light reflected by vegetation.\n\n\\(NDVI = \\frac{(NIR - RED)}{(NIR+RED)}\\)\n\nAside from the NDVI, here are some other indicators that I have investigated and summarized:\n\nARVI (Atmospherically Resistant Vegetation Index) is commonly employed to tackle regions of high atmospheric aerosol content. It was originally proposed for remote sensing of EOS MODIS sensor. (Kaufman and Tanre 1992) The range for an ARVI is -1 to 1 where green vegetation generally falls between values of 0.20 to 0.80.\n\n\\(ARVI = \\frac{(NIR - RED - y * (RED - BLUE))}{(NIR + RED - y*(RED-BLUE))}\\)\nAccording to ArcGIS: “ARVI is a vegetation-based index that minimizes the effects of atmospheric scattering caused by rain, fog, dust, smoke, or air pollution.”\ny is a constant to correct for the atmospheric effects caused by aerosol scattering in red channel.\n\nNDMI (Normalized Difference Moisture Index) helps monitoring and mapping water content in soil and vegetation. (Wilson and Sader 2002) It is calculated as Near-Infrared’s and Short-Wave Infrared’s difference over sum.:\n\\(NDMI = \\frac{NIR - SWIR}{NIR + SWIR}\\)\n\nIn Landsat 8-9: \\(NDMI =\\frac{(Band5 - Band6)}{(Band5+Band6)}​\\)\n\nSAVI (Soil-Adjusted Vegetation Index) is originally constructed to minimize soil brightness influences on NDVI, especially useful in areas with sparse vegetation. (AR Huete 1988)\n\n\\(SAVI =\\frac{(NIR+Red+L)}{(NIR−Red)}​∗(1+L)\\)\nSAVI introduces a soil brightness correction factor (L) to adjust the NDVI calculation, making it more accurate in sparse vegetation areas.\nIn Landsat 8-9: \\(SAVI - \\frac{Band5 - Band4}{Band5 + Band4 +0.5}\\)\n\nMSAVI (Modified Soil-Adjusted Vegetation Index) is an adjustment of SAVI, designed to minimize bare soil background effects more effectively and optimize vegetation monitoring. (Qi et al. 1994)\n\n\\(MSAVI = \\frac{2*NIR + 1 - \\sqrt{(2*NIR+1)-8*(NIR-RED)}}{2}\\)\nIn Landsat 8-9: \\(SAVI - \\frac{2*Band5 + 1 - \\sqrt{(Band5+1)^2 -8*(Band5-Band4)}}{2}\\)\n\nGNDVI (Green Normalized Difference Vegetation Index) focuses on chlorophyll content, primarily on the green spectral region for enhanced sensitivity to vegetation density. (Gitelson, Kaufman, and Merzlyak 1996)\n\n\\[NDVI = \\frac{NIR - Green}{NIR +Green}\\]\nLandsat 8-9: \\(NDMI =\\frac{Band5 - Band3}{Band5 + Band3}\\)\n\nEVI (Enhanced Vegetation Index) was proposed to improve the sensitivity in high biomass regions and improves vegetation monitoring through a de-coupling of the canopy background signal and a reduction in atmospheric influences. (Alfredo Huete et al. 2002)\n\n\\(EVI=G \\frac{(NIR−Red)}{(NIR+C1⋅Red−C2⋅Blue+L)}\\)\nIn Landsat 8-9: \\(EVI = 2.5 * \\frac{Band5 - Band4}{Band5 + 6* Band4 - 7.5*Band2 + 1}\\)\n\nNDWI (Normalized Difference Water Index) was formulated for the detection of liquid water and moisture content of vegetation and soil.(Gao 1996)\n\n\\(NDWI_{veg​}= \\frac{(NIR+Green)}{(NIR−Green)}\\)\nIn Landsat 8-9: \\(NDWI = \\frac{Band4-Band2}{Band4+band2}\\)\n\n\nThese indicators, are sufficient only for basic analysis demands, to derive more hidden relations, more modern techniques such as deep learning,(Ma et al. 2019) and Semi-supervised representation learning (Yan et al. 2020) can also be employed."
  },
  {
    "objectID": "week3.html#literature-review-and-applications",
    "href": "week3.html#literature-review-and-applications",
    "title": "Week 3 Atmosphere Corrections",
    "section": "Literature Review and Applications",
    "text": "Literature Review and Applications\nIntegrating the most appropriate remote sensing metrics for urban analytics is pivotal effective city planning or addressing other built-environment challenges. Among these measures, NDVI as is widely used for monitoring urban vegetation and urban green space,(Xue, Su, et al. 2017) whereas NDWI plays a critical role in urban water body and drought monitoring, it is also important for urban fire prevention.(Zhu 2017) Other metrics, such as ARVI, EVI, SAVI, MSAVI, and NDMI are still relevant in the industry and contemporary urban science, but recent studies call for both enhancement of the indices and nuanced applications. As suggested in many publications, such correction methods can be further refined to optimized the data contamination brought by atmospheric or other factors. Zhou et al. (2021) proposed the use of HANTS model (Harmonic Analysis of Time Series) to further optimize the parameter settings for NDVI reconstruction at a global scale and over longer period of time. Such methods are unique and well-rooted under today’s hype of LLM or “trying DL/complex machine learning frameworks on all problems”. Thus, investigations on further optimising the traditional correction methods with explainable models future research remains relevant and important."
  },
  {
    "objectID": "week3.html#reflection",
    "href": "week3.html#reflection",
    "title": "Week 3 Atmosphere Corrections",
    "section": "Reflection",
    "text": "Reflection\nReflecting on this week’s learning, it’s evident that the field of remote sensing is both complex and immensely powerful. Although, I am still seeing just seeing the tip of the iceberg by far, I do believe in the future of interdisciplinary potentials of remote sensing with city research. The process of selecting appropriate RS imagery, applying correction techniques, and utilizing various indices for environmental analysis is intricate but essential for extracting meaningful and reliable insights from the satellite data.(Jensen 1996) This knowledge not only enriched my technical skills but also enhanced myappreciation for the meticulous work behind the scenes of environmental monitoring and analysis.\nIn my continued case study of Bristol, served as a concept preview of the corrections in remote sensing and its applicability in real-world scenarios such as vegetation monitoring. The processed illustrated the importance of careful data selection, processing, and analytically flow in achieving robust outcomes. Furthermore, the discussion on correction methods and the summary of details of various vegetation and soil indices provided a comprehensive understanding of the tools and techniques.\nAs we continue our journey through the remote sensing module, this week’s focus on image correction and enhancements has laid a solid foundation for understanding the nuanced internal mechanics of satellite data processing. It has also sparked my curiosity about how these techniques can be applied to other areas of study, especially for urban resilience, smart cities design and disaster management. The insights gained this week will prepare us well regarding the basics for our future remote sensing projects.\n\n\n\n\nGao, Bo-Cai. 1996. “NDWI—a Normalized Difference Water Index for Remote Sensing of Vegetation Liquid Water from Space.” Remote Sensing of Environment 58 (3): 257–66.\n\n\nGitelson, Anatoly A, Yoram J Kaufman, and Mark N Merzlyak. 1996. “Use of a Green Channel in Remote Sensing of Global Vegetation from EOS-MODIS.” Remote Sensing of Environment 58 (3): 289–98.\n\n\nHuete, Alfredo, Kamel Didan, Tomoaki Miura, E Patricia Rodriguez, Xiang Gao, and Laerte G Ferreira. 2002. “Overview of the Radiometric and Biophysical Performance of the MODIS Vegetation Indices.” Remote Sensing of Environment 83 (1-2): 195–213.\n\n\nHuete, AR. 1988. “A Soil-Adjusted Vegetation Index (SAVI). Remote Ssensing of Environment, 25, 295-309.”\n\n\nJensen, John R. 1996. “Introductory Digital Image Processing: A Remote Sensing Perspective.”\n\n\nKaufman, Yoram J, and Didier Tanre. 1992. “Atmospherically Resistant Vegetation Index (ARVI) for EOS-MODIS.” IEEE Transactions on Geoscience and Remote Sensing 30 (2): 261–70.\n\n\nMa, Lei, Yu Liu, Xueliang Zhang, Yuanxin Ye, Gaofei Yin, and Brian Alan Johnson. 2019. “Deep Learning in Remote Sensing Applications: A Meta-Analysis and Review.” ISPRS Journal of Photogrammetry and Remote Sensing 152: 166–77.\n\n\nQi, Jiaguo, Abdelghani Chehbouni, Alfredo R Huete, Yann H Kerr, and Soroosh Sorooshian. 1994. “A Modified Soil Adjusted Vegetation Index.” Remote Sensing of Environment 48 (2): 119–26.\n\n\nWilson, Emily Hoffhine, and Steven A Sader. 2002. “Detection of Forest Harvest Type Using Multiple Dates of Landsat TM Imagery.” Remote Sensing of Environment 80 (3): 385–96.\n\n\nXue, Jinru, Baofeng Su, et al. 2017. “Significant Remote Sensing Vegetation Indices: A Review of Developments and Applications.” Journal of Sensors 2017.\n\n\nYan, Peiyao, Feng He, Yajie Yang, and Fei Hu. 2020. “Semi-Supervised Representation Learning for Remote Sensing Image Classification Based on Generative Adversarial Networks.” IEEE Access 8: 54135–44.\n\n\nZhou, Jie, Li Jia, Massimo Menenti, and Xuan Liu. 2021. “Optimal Estimate of Global Biome—Specific Parameter Settings to Reconstruct NDVI Time Series with the Harmonic ANalysis of Time Series (HANTS) Method.” Remote Sensing 13 (21): 4251.\n\n\nZhu, Zhe. 2017. “Change Detection Using Landsat Time Series: A Review of Frequencies, Preprocessing, Algorithms, and Applications.” ISPRS Journal of Photogrammetry and Remote Sensing 130: 370–84."
  },
  {
    "objectID": "week4.html#summary-of-remote-sensing-and-policy-implications",
    "href": "week4.html#summary-of-remote-sensing-and-policy-implications",
    "title": "Week 4 Remote Sensing and Policy Implications",
    "section": "Summary of Remote Sensing and Policy Implications",
    "text": "Summary of Remote Sensing and Policy Implications\nThe lecture first introduced the significance of multi-temporal land cover mapping in understanding and managing environmental changes, where remote sensing is particularly meaningful for researchers and policymakers to identify land use changes, deforestation, and urban sprawl. (Bhatta, Saraswati, and Bandyopadhyay 2010)\nAnother major application of remote sensing in terms of shaping the policies, is environmental monitoring. With remote sensing measures such as Synthetic Aprture Radar (SAR), for example, environmental researchers and policy makers are able to monitor the changes of the forests (Hansen et al. 2013) therefore formulate instruments that directly contributes to containing illegal logging. Another key application for remote sensing is for disaster monitoring and response, with high-quality and timely remote sensing data, policymakers now have the capability to tackle large-scale disastrous events such as droughts and forest fires. (Van Westen 2000)\nWith the insights extracted from remote sensing, cities can now monitor and construct strategies for complicated problems such as urban heat island, (Stefanov and Brazel 2007) inner city floods, (Li, Xu, and Chen 2016) further increasing the urban resilience. (Ghaffarian, Kerle, and Filatova 2018) By incorporating spatial data and remote sensing into the making of policy instruments through aligning with the global sustainability agendas such as the New Urban Agenda and the Sustainable Development Goals (SDGs), stakeholders from all aspects can work together to face challenges raised by rapid urbanisation."
  },
  {
    "objectID": "week4.html#wildfire-monitoring-for-air-quality-with-remote-sensing",
    "href": "week4.html#wildfire-monitoring-for-air-quality-with-remote-sensing",
    "title": "Week 4 Remote Sensing and Policy Implications",
    "section": "Wildfire Monitoring for Air Quality with Remote Sensing",
    "text": "Wildfire Monitoring for Air Quality with Remote Sensing\nRemote sensing and policy implications are intricately linked, playing a pivotal role in addressing some of our most pressing challenges. The United Nations Sustainable Development Goals (UN SDGs) outline numerous objectives for urban and built environments. Recently, mountain and bush fires have significantly deteriorated the air quality in major parts of the world, to make the matters worse, the smogs can travel far from time to time, threatening the health and quality of life for a great many residents.\nConsequently, my case study, centred around Melbourne and its adjacent suburbs, aims to explore the integration of remote sensing for wildfire monitoring with urban air quality forecasting. This short study intends identify the potentials of integrating remote sensing into the current pipelines, thus provide policymakers and city councils with sufficient lead time to implement measures before smog affects residential areas. Potential policy integration could include distribution of air filtration systems, masks or the evacuation of affected populations. This pilot study seeks to investigate the policy ramifications of establishing such operational frameworks.\n\nA Case Study and Policy Implications for the Greater Melbourne Area\nThe goal for this short study is to explore the forecasting of smoke plume bush-fire to areas that may impact the residents and discuss policy implications. Focusing on Melbourne, with framework that includes remote sensing, intended to be integrated with sustainable future planning inspired the use of remote sensing for monitoring urban growth in Perth, WA. (MacLachlan et al. 2017)\nAs for the imagery, Sentinel 5P from Copernicus Program is offering sufficient resolutions. In terms of spatial resolution it covers up to a 5.5km x 3.5km area, Sentinel 5P also offers a daily global coverage, (revisits less than one day) providing enough temporal resolution. As for spectral resolution, Sentinel 5P offers accurate quantification of various atmospheric gases and pollutants including carbon monoxide(CO), nitrogen dioxide (NO2), ozone(O3), formaldehyde(HCHO), sulfur dioxide(SO2), methane(CH4) and aerosols thanks to its extensive spectral coverage which includes UV, VIS and NIR bands.(Victoria 2019) It also possesses radiometric resolution at satisfactory level which enables the sensors on-board to distinguish different levels of signal intensity. The following is a video summary of the 2019 to 2020 bushfire air pollutant spread in Australia which resulted in low AQI in a great portion of the country’s residential areas not to mention destruction of many homes. (Tiernan and O’Mallon 2020)\n\n\n\n\nUtilizing the Sentinel 5P data(Trinder and Liu 2020), this proposed project aligns with the following UN Sustainable Development Goals:\n\n\n\n\n\n\n\n\nContribution (by Implementing Proposed Research/Development)\nSDG\nDescription\n\n\n\n\nContribute to the prevention of human respiratory damage caused by smogs from burning forest.\nUN SDG 3\nGood Health and Well-being, ensure people lives a healthy environment, promote well-being for all ages.\n\n\nProper monitoring and timely alerts, contributes to the enhancement of the built environment and overall community safety.\nUN SDG 11\nSustainable, resilient, inclusive and safe cities/communities.\n\n\nActively monitoring wild fires and smogs produced can offer insights and guidance for mitigation.\nUN SDG 13\nTake urgent actions against climate change.\n\n\nThis proposal can contribute to the sustainable of land resources and terrestrial ecosystems.\nUN SDG 15\nSustainable management of land, forests and biodiversity.\n\n\n\nNASA’s Fire Information for Resource Management System (FIRMS) can also be integrated into the workflow. Using remote sensing for monitoring wild fire monitoring has long been proposed for Eastern Australia. (Milne 1986)\nBy extracting values from the daily Sentinel 5p data, we can monitor the wildfire and combine the ground weather/air quality monitoring data, to determine how the polluted air will impact the densely populated area (mapping the air monitoring data over the building footprint in Australia) thus, provide intervention implications for distributing devices and masks. In the literature, similar was workflow proposed for monitoring wild fires in the US. (Kochanski et al. 2021)\n\n\n\nSentinel 5P, Feb 01 to Feb 06, 2020, NO2 level Around Melbourne\n\n\nAs in many regions of the world, residential AQI can be dramatically affected by wildfires, therefore, it is crucial to have a integrated pipeline for the monitoring of harmful aerosol spike which is also incorporated into the policy stack and is dedicated with ample resources.\n\n\n\nSentinel 5P image Overlay with Map around (well, again, “around” and covering VIC and NSW) Melbourne (Time window set to December 01, 2019 to February 06, 2020)\n\n\n\n\nCurrent Policy Stack and Wofkflow\nAs a vibrant metropolis in the state of Victoria, the local councils and state government around the Greater Melbourne Area, has implemented air quality policies aimed at safeguarding the health and well-being of their residents.\n\n\n\nProposed Workflow Serving as an Add-on to the Current Government Workflow and Policy Stack\n\n\n\nEnvironment Protection Amendment Act 2018: This act introduces a general environmental duty that requires proactive risk management to prevent harm to public health and the environment, including air quality impacts.\nEPA Victoria’s AirWatch: A platform that offers real-time air quality information to the public, including an AQI that categorizes air quality levels and provides health advisories, enabling residents to make informed decisions about outdoor activities.\n\n\n\nProposed Mitigation\nIn light of this,this proposal aims to integrate state-of-art remote sensing technology into Melbourne’s current air quality management workflow, offering a multifaceted approach to mitigate air pollution risks:\n\nEstablishment of High-Frequency Sentinel Points: By setting up strategically positioned sentinel points equipped with remote sensing capabilities, we propose to advance the forecasting of Points of Interest (POIs) for early air quality alerts. This system will enable the detection of potential air quality deterioration before polluted air approaches the residential vicinity, allowing for proactive measures to be taken.\nDeployment of Air Filtration and Respiratory Device Stations: Recognizing the immediate health impacts of poor air quality, we plan to establish stations across key urban and suburban locations for the distribution of air filtering and respiratory devices. These stations will ensure that citizens have timely access to necessary protective equipment against smoke and pollutants. Ideally, the station can be established inside local hospitals or clinics.\nEnhanced Emergency Evacuation Planning: Agent-Based Modeling (ABM) methods alongside satellite imagery analysis will be employed to refine emergency evacuation planning. By identifying optimal evacuation routes and safe zones in advance, the updated workflow will facilitate efficient relocation strategies during smoky conditions, ensuring public safety and minimizing chaos.\n\n\n\nImplementation and Emergency Planning Techniques\nOur methodology leverages state-of-the-art remote sensing technology, including the analysis of satellite data from platforms such as Sentinel-5P, to offer unparalleled insights into air quality dynamics. The integration of ABM methods will further enhance our predictive capabilities, allowing for the simulation of various scenarios and the formulation of robust emergency response strategies.\n\n\nConclusion\nBy integrating advanced remote sensing technology into the city’s existing workflow, we aim not only to protect the well-being of Melbourne’s citizens but also to set a new standard for urban air quality management worldwide."
  },
  {
    "objectID": "week4.html#literature-review-and-applications",
    "href": "week4.html#literature-review-and-applications",
    "title": "Week 4 Remote Sensing and Policy Implications",
    "section": "Literature Review and Applications",
    "text": "Literature Review and Applications\nThe exploration of policy and global agenda, combined with the review of current literature, made this week’s class and practical a proper warm-up for the group assessment. I have also combed through the literature and policy base briefly, to further analyze the interplay between remote sensing, policy and air quality.\nEarly in the 2000s, studies have proposed the potentials of incorporating remote sesing into the air quality policy frameworks. (Veefkind et al. 2007) Aside from the potential of forecasting emergency air pollution caused by wildfires or other incidents, recent literature has also indicated the transformative potential of remote sensing in air quality monitoring.(Sokhi et al. 2022) Other researchers has highlighted the application of remote sensing for timely data on aerosol pollutants. (Gupta et al. 2006) therefore, the integration of remote sensing air quality monitoring, could also contribute to the approximate verification and robust back-up for ground air quality data collection. (Bechle, Millet, and Marshall 2013) To generalize the impact of urban air quality monitoring with remote sensing, researchers also suggested that remote sensing can be incorporated in the long-term exposure assessment of air pollutants (Van Donkelaar et al. 2015) and be utilized to discover hidden air pollution patterns. (Verma et al. 2023) Although the current policy frameworks around the world are including satellite images into air quality monitoring, the spatial and temporal resolution of such observation data could be further improved, offering policy makers and environmental agencies more detailed and timely knowledge on the air quality. Potential research directions could explore finer pollution detection,(Huang et al. 2022) establishing near real-time or real time data acquisitions,(Geng et al. 2021) compatibility or fusion with other key environmental metrics monitoring systems. (Prados et al. 2010) and a more hybrid approached in terms of acquiring air quality data such as combining the satellite observations with UAV images. (Budde et al. 2017)"
  },
  {
    "objectID": "week4.html#reflection",
    "href": "week4.html#reflection",
    "title": "Week 4 Remote Sensing and Policy Implications",
    "section": "Reflection",
    "text": "Reflection\nThe implications of such research are profound for policy-making, it can even contribute to a more data-driven paradigm shift as the data-driven planning. By leveraging remote sensing data, policymakers can benefit from timely and accurate information on the hazardous smoke plumes, their potential movement patterns and impact on overall urban air quality. From the test use of Sentinel 5P data shown above, we can conclude that the resolution overall is inadequate for predicting the movement of the pollutant clusters in the air. In addition, the remote sensing detection and forecasting, ought to be a value-addition or redundant measure to existing air quality measurement and monitoring frameworks and policy instruments. It is also obvious that integrating remote sensing data with real-time sensor data or other air quality monitoring data can pose technique challenges.\nIn summary, the exploration of air quality monitoring, policy interventions, and remote sensing, opens up a new realm of environmental protection and policy implementation. It exemplifies the innovation synergy between technology and policy by introducing remote sensing data analysis to both local and global environmental strategies."
  },
  {
    "objectID": "week4.html#concept-illustration-video",
    "href": "week4.html#concept-illustration-video",
    "title": "Week 4 Remote Sensing and Policy Implications",
    "section": "Concept Illustration Video",
    "text": "Concept Illustration Video\nAs a rather off-track side-quest, I also generated this with text-prompt on Stable Video:\n\n\n\n\nIt is not a geographically accurate depiction as the forest distribution is clearly off, but as a measure for quickly formatting initial illustrations the current development is beyond my expectation.\n\n\n\n\nBechle, Matthew J, Dylan B Millet, and Julian D Marshall. 2013. “Remote Sensing of Exposure to NO2: Satellite Versus Ground-Based Measurement in a Large Urban Area.” Atmospheric Environment 69: 345–53.\n\n\nBhatta, Basudeb, S Saraswati, and D Bandyopadhyay. 2010. “Urban Sprawl Measurement from Remote Sensing Data.” Applied Geography 30 (4): 731–40.\n\n\nBudde, Matthias, Till Riedel, Michael Beigl, Klaus Schäfer, Stefan Emeis, Josef Cyrys, Jürgen Schnelle-Kreis, et al. 2017. “SmartAQnet: Remote and in-Situ Sensing of Urban Air Quality.” In Remote Sensing of Clouds and the Atmosphere XXII, 10424:19–26. SPIE.\n\n\nGeng, Guannan, Qingyang Xiao, Shigan Liu, Xiaodong Liu, Jing Cheng, Yixuan Zheng, Tao Xue, et al. 2021. “Tracking Air Pollution in China: Near Real-Time PM2. 5 Retrievals from Multisource Data Fusion.” Environmental Science & Technology 55 (17): 12106–15.\n\n\nGhaffarian, Saman, Norman Kerle, and Tatiana Filatova. 2018. “Remote Sensing-Based Proxies for Urban Disaster Risk Management and Resilience: A Review.” Remote Sensing 10 (11): 1760.\n\n\nGupta, Pawan, Sundar A Christopher, Jun Wang, Robert Gehrig, YC Lee, and Naresh Kumar. 2006. “Satellite Remote Sensing of Particulate Matter and Air Quality Assessment over Global Cities.” Atmospheric Environment 40 (30): 5880–92.\n\n\nHansen, Matthew C, Peter V Potapov, Rebecca Moore, Matt Hancher, Svetlana A Turubanova, Alexandra Tyukavina, David Thau, et al. 2013. “High-Resolution Global Maps of 21st-Century Forest Cover Change.” Science 342 (6160): 850–53.\n\n\nHuang, Yuhan, Casey KC Lee, Yat-Shing Yam, Wai-Chuen Mok, John L Zhou, Yuan Zhuang, Nic C Surawski, Bruce Organ, and Edward FC Chan. 2022. “Rapid Detection of High-Emitting Vehicles by on-Road Remote Sensing Technology Improves Urban Air Quality.” Science Advances 8 (5): eabl7575.\n\n\nKochanski, Adam K, Farren Herron-Thorpe, Derek V Mallia, Jan Mandel, and Joseph K Vaughan. 2021. “Integration of a Coupled Fire-Atmosphere Model into a Regional Air Quality Forecasting System for Wildfire Events.” Frontiers in Forests and Global Change 4: 728726.\n\n\nLi, Linyi, Tingbao Xu, and Yun Chen. 2016. “Improved Urban Flooding Mapping from Remote Sensing Images Using Generalized Regression Neural Network-Based Super-Resolution Algorithm.” Remote Sensing 8 (8): 625.\n\n\nMacLachlan, Andrew, Eloise Biggs, Gareth Roberts, and Bryan Boruff. 2017. “Urban Growth Dynamics in Perth, Western Australia: Using Applied Remote Sensing for Sustainable Future Planning.” Land 6 (1): 9.\n\n\nMilne, Anthony Kinnaird. 1986. “The Use of Remote Sensing in Mapping and Monitoring Vegetational Change Associated with Bushfire Events in Eastern Australia.” Geocarto International 1 (1): 25–32.\n\n\nPrados, Ana I, Gregory Leptoukh, Chris Lynnes, James Johnson, Hualan Rui, Aijun Chen, and Rudolf B Husar. 2010. “Access, Visualization, and Interoperability of Air Quality Remote Sensing Data Sets via the Giovanni Online Tool.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 3 (3): 359–70.\n\n\nSokhi, Ranjeet S, Nicolas Moussiopoulos, Alexander Baklanov, John Bartzis, Isabelle Coll, Sandro Finardi, Rainer Friedrich, et al. 2022. “Advances in Air Quality Research–Current and Emerging Challenges.” Atmospheric Chemistry and Physics 22 (7): 4615–4703.\n\n\nStefanov, William L, and Anthony J Brazel. 2007. “Challenges in Characterizing and Mitigating Urban Heat Islands—a Role for Integrated Approaches Including Remote Sensing.” In Applied Remote Sensing for Urban Planning, Governance and Sustainability, 117–35. Springer.\n\n\nTiernan, Finbar, and Eamonn O’Mallon. 2020. “Australia’s 2019–20 Bushfire Season.” The Canberra Times, January. https://www.canberratimes.com.au/story/6600958/australias-2019-20-bushfire-season/.\n\n\nTrinder, John, and Qingxiang Liu. 2020. “Assessing Environmental Impacts of Urban Growth Using Remote Sensing.” Geo-Spatial Information Science 23 (1): 20–39. https://doi.org/10.1080/10095020.2019.1710438.\n\n\nVan Donkelaar, Aaron, Randall V Martin, Michael Brauer, and Brian L Boys. 2015. “Use of Satellite Observations for Long-Term Exposure Assessment of Global Concentrations of Fine Particulate Matter.” Environmental Health Perspectives 123 (2): 135–43.\n\n\nVan Westen, CJ. 2000. “Remote Sensing for Natural Disaster Management.” International Archives of Photogrammetry and Remote Sensing 33 (B7/4; PART 7): 1609–17.\n\n\nVeefkind, P, RF Van Oss, H Eskes, Annette Borowiak, F Dentner, and Julian Wilson. 2007. “The Applicability of Remote Sensing in the Field of Air Pollution.” Institute for Environment and Sustainability, Italy 59: 1–54.\n\n\nVerma, Sunita, Tanu Gangwar, Janhavi Singh, Divya Prakash, and Swagata Payra. 2023. “Urban Air Quality Monitoring and Modelling Using Ground Monitoring, Remote Sensing, and GIS.” In Geospatial Analytics for Environmental Pollution Modeling: Analysis, Control and Management, 213–47. Springer.\n\n\nVictoria, Environmental Protection Authority. 2019. “EPA AirWatc.” https://www.epa.vic.gov.au/for-community/airwatch."
  },
  {
    "objectID": "week5.html#group-assessment-key-points",
    "href": "week5.html#group-assessment-key-points",
    "title": "Week 5 More on Temperature and Group Work Discussion",
    "section": "Group Assessment Key Points",
    "text": "Group Assessment Key Points\nHere are some key takeaways for group assessment references:\n\nHow to perform the feasibility analysis? Approach from the following aspects:\n\nTechnical: Can we do it with the tech we have?\nEconomic: Is it worth the hassle?\nLegal: Are we breaking any local or international laws?\nOperational: How will the project unravel?\nScheduling: When to do what?\n\nCommon global agendas related to remote sensing and urban studies:\n\nUN Sustainable Development Goals\nthe Sendai Framework for Disaster Risk Reduction 2015-2030\nThe New Urban Agenda (NUA)\nThe Global Green New Deal (GND)\n\nOther key points are that we agreed upon for this project is to develop the work covered in the pitch into Work-packs, perform SWOT analysis, design automated Data Pipelines, and adding a Gantt Chart.\nThe topic and scope is also settle with the help of Andy and our PGTAS! (But the brainstorming is utterly a thrill since everyone has contributed novel ideas to develop!)"
  },
  {
    "objectID": "week6.html#javascript-and-gee-basics",
    "href": "week6.html#javascript-and-gee-basics",
    "title": "Week 6 Introduction to Google Earth Engine",
    "section": "JavaScript and GEE Basics",
    "text": "JavaScript and GEE Basics\nGoogle Earth Engine is a cloud platform developed by Google for scientific data analysis. (Reis, Datia, and Pós-de-Mina Pato 2020) GEE can handle large datasets in a relatively faster than many other traditional commercial software, its mature integration with JavaScript also enables new users to master the tool suite with a shallower learning curve.(Cardille et al. 2022) JavaScript is the language used in GEE code editor, it is an essential tool for front-end development, enabling developers to implement complex and interactive websites. JavaScript also interacts with HTML and CSS to style and build the user interface. I have also summarised some useful introduction materials for learning GEE here in the term summary.\nHere is a basic block to code a function:\nfunction sayHi(name) {\n    alert(\"Hi,\" + name + \"!\");\n}\n\nsayHi(\" guys, \"); // Pops up \"Hi, guys, !\""
  },
  {
    "objectID": "week6.html#gee-case-study-bristol",
    "href": "week6.html#gee-case-study-bristol",
    "title": "Week 6 Introduction to Google Earth Engine",
    "section": "GEE Case Study Bristol",
    "text": "GEE Case Study Bristol\nFirst, I gathered air quality data over Bristol. But the resolution from Sentinel 5P was not ideal, thus I zoomed out in order to find places with high NO2 levels, in the end, even London is included. At this resolution, it is obvious that we will need more image processing and better resolution to have fine-grained data on urban air quality\n\n\n\nNO2 Level Over Bristol to London from Sentinel 5P\n\n\n\nAir Quality, SAVI, ARVI, NDVI and NDWI\nIn this introductory study of GEE, I have examined the NO2 from Sentinel 5P and calculated NDVI, NDWI, SAVI and ARVI for Bristol. Time from the data used are Sentinel 2 and Sentinel 5P (for NO2) from 2021-Jan-01 to 2023-Dec-30, I made a date slider for the user to drag to select which data from a 30-day window. Rarely, the selection won’t be valid, that is because the filter that I coded for the cloud is very low 5, and the Sentinel 2 images tend to be more cloudy for Bristol, please refresh the webpage and try another month. Most of the time it is working, due to the confined time-frame, I will be optimising this app and its UI in the future. For now, NO2 is not bounded, as the Sentinel 5P data is low-resolution compared to Sentinel 2, thus NDVI, NDWI, SAVI and ARVI are bounded within the Bristol image, where as NO2 layer is not.\n\n\nPutting Everything Together\nHere is the link to the lookup app for Bristol, it is also embedded as follows:\n\n\n\n\nFor the legend in this GEE app, I have modified and derived the code from: Open Geo Blog. The colour code gradients for the four metrics are gathered from this website. The boundary for the app is Greater Bristol (or the Global Administrative Unit (GAUL) Layer Level 2 that contains Bristol) which is larger than the shapefile used to clip for the city boundary in the week 6 practical or the following screenshots.\nFor week 6, the author downloaded the Bristol City Boundary shape file, uploaded to the assets and clipped out images with only Bristol. This is smaller compared to the GAUL level 2 boundary used in the GEE app.\n\n\n\n\n\n\n\nNDVI, NDWI, SAVI and ARVI bounded by the Bristol Boundary"
  },
  {
    "objectID": "week6.html#principle-component-analysis",
    "href": "week6.html#principle-component-analysis",
    "title": "Week 6 Introduction to Google Earth Engine",
    "section": "Principle Component Analysis",
    "text": "Principle Component Analysis\nGEE can run PCA for satellite data, but the browser froze for a few minutes before the results are ready.\n\n\n\n\nBristol Boundary\n\n\n\n\n\nBristol Clip\n\n\n\n\n\nGLCM\n\n\n\n\n\nP1 and P2\n\n\n\n\n\nPC1\n\n\n\n\n\nPC2\n\n\n\n\n\nPC5\n\n\n\n\n\nPC6\n\n\n\nPC1 explains 77.17% of the variance in the dataset. With my visual assessment of the GLCM and PCA results, it is obvious PC1 and PC2 captures high NDWI and highly building dense areas repectively, whereas GLCM captures high-reflectance areas, it performs well on flat rooftops, making it suitable for identifying buildings.(Christaki et al. 2022) Such results are essential as the input of more complex models, such as classification models with dimensionalities reduced to PC1, PC2 and PC3 (together they explain 95.88% of variances). However, PC4 to PC6, or the other PCs on condition that they are still meaningful and not rounded to 0, can provide crucial insights to anomalies and noises.\n\nPCA Variance Results\n\n\nPC\nPercentage Variance Explained\n\n\n\n\nPC1\n77.17\n\n\nPC2\n11.40\n\n\nPC3\n7.31\n\n\nPC4\n2.41\n\n\nPC5\n1.58\n\n\nPC6\n0.11\n\n\nPC7\n0.03\n\n\nPC8 - PC20\n0.00"
  },
  {
    "objectID": "week6.html#more-on-gee-and-literature-review",
    "href": "week6.html#more-on-gee-and-literature-review",
    "title": "Week 6 Introduction to Google Earth Engine",
    "section": "More on GEE and Literature Review",
    "text": "More on GEE and Literature Review\nGoogle Earth Engine (GEE) offers a wide range of Earth data for researchers to use with easy access, this has led to a surge in publication using GEE. As a ECE major, I first explored the how GEE can support a vast user group scattered around the planet, and then looked into the recent trend in the scientific community with GEE.\n\nGEE Capabilities and Trend in Scientific Research\nThe system architecture of GEE is well designed to offer services at a large scale. This design allows the GEE to host massive amount of data while being scalable. Of course, being the birth-company for the famous MapReduce, it is obvious that hadnling massive amount of data is what Google does best. GEE offers a seamless integration with machine learning, whether it is using the API from python or code editor, the ML-workflow is well-integrated.\nDue to its advantages, more members from the scientific researchcommunity have been adapting GEE into their research in recent years.(Pham-Duc et al. 2023) GEE can provide geo-spatial analysis at a large-scale for free and its ample image series datasets are attractive features that drive a growing usage from the researchers. Furthermore, GEE can meet the needs of urban researcher since the nature or urban science research requires multi-faceted data and spatiotemporal data at large scale. GEE’s abundant city bird-view images in with sufficient temporal richness, encourages urban researcher to utilise it in conjunction with other urban data, sparking novel research findings about our growing metropolitan environment.\n\n\n\n\n System architecture of GEE (left) (Gorelick et al. 2017) and Annual number of articles published by the top ten journals (Pham-Duc et al. 2023)\n\n\n\nGEE Applications with GLCM\nIn the context of remote sensing for cities, GEE is employed in a diverse range of new applications and publications.(Amani et al. 2020) During the practical, I dived into the GLCM (Grey-Level Co-occurrence Matrix) and its applications in identifying potentials buildings in remote sensing datasets, this enabled me to have a wider range of methodology design for both my future research and the group assessment.\nThere are many unique publications regarding GLCM, in the aspects of the theoretical construct of GLCM methods, Lan et al. Lan and Liu (2018) proposed a method to optimse the multi-scale parameters for the GLCM texture window. On the application side, researchers developed methods that combine the GLCM with SNIC (Simple Non-Iterative Clustering) to process the data for classification input in LULC (Land Use and Land Cover) research. (Tassi and Vizzari 2020)\nOther researchers have extended the GLCM methods to process multi-layered data and combined the modified GLCM with SVM (Support Vector Machine) to analyse collapsed buildings.(Moya et al. 2019) These new developments highlight the innovative usage and adaptation of GLCM for remote sensing research, however, it also indicates that such numeric methods should be handled with caution as its limitations may hinder the analysis accuracy if proper fine-tuning are not performed."
  },
  {
    "objectID": "week6.html#reflection",
    "href": "week6.html#reflection",
    "title": "Week 6 Introduction to Google Earth Engine",
    "section": "Reflection",
    "text": "Reflection\nEmploying Google Earth Engine (GEE) certainly rekindles memories of my extensive collaborative efforts on diverse projects in Melbourne. During those projects, I relied on Nearmap (which was really high-res!) to coordinate with the design team and worked closely alongside engineers and GIS specialists. Together, we organised design modifications and participated the digital transformation of the construction processes. Unlike those days when my role was more supervisory and collaborative, I now find myself directly engaged with GEE, utilizing its powerful remote sensing capabilities for urban analysis and environmental applications.\nAlthough GEE offers a powerful tool suite for urban research, there may be certain limitations. First, it fosters a level of vendor lock-in where extensive use of GEE can lead to dependence on Google’s infrastructure, terms and conditions, posing risks to the research workflow. Furthermore, GEE is challenging to work with offline. Researchers with poor internet connection or working on remote sites may struggle to gain access to GEE. Data sharing with GEE can be more complicated than other platforms. Sharing custom-made shapefiles might turn out to be a complex process due to GEE’s data management policies and permissions. Finally but not the least, GEE can be difficult for non-coders to learn, what is available to it JavaScript library can be occasionally counter-intuitive.\nDespite GEE’s limitations, its versatile and lightweight nature, combined with its openness and collection of data, still weighs out its limitations. Researching with GEE on urban topics is still promising as the platform itself is robust and powerful for large datasets, besides, the GEE community is open and vibrant, there are many resources and people for turn to for collaboration and assistance.\n\n\n\n\nAmani, Meisam, Arsalan Ghorbanian, Seyed Ali Ahmadi, Mohammad Kakooei, Armin Moghimi, S Mohammad Mirmazloumi, Sayyed Hamed Alizadeh Moghaddam, et al. 2020. “Google Earth Engine Cloud Computing Platform for Remote Sensing Big Data Applications: A Comprehensive Review.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 13: 5326–50.\n\n\nCardille, Jeffrey A, Nicholas E Clinton, Morgan A Crowley, and David S Saah. 2022. “Cloud-Based Remote Sensing with Google Earth Engine: Process and Prospects from a Large Edited Open-Access Book.” In AGU Fall Meeting Abstracts, 2022:ED32D–0552.\n\n\nChristaki, Marianna, Christos Vasilakos, Ermioni-Eirini Papadopoulou, Georgios Tataris, Ilias Siarkos, and Nikolaos Soulakellis. 2022. “Building Change Detection Based on a Gray-Level Co-Occurrence Matrix and Artificial Neural Networks.” Drones 6 (12): 414.\n\n\nGorelick, Noel, Matt Hancher, Mike Dixon, Simon Ilyushchenko, David Thau, and Rebecca Moore. 2017. “Google Earth Engine: Planetary-Scale Geospatial Analysis for Everyone.” Remote Sensing of Environment 202: 18–27.\n\n\nLan, Zeying, and Yang Liu. 2018. “Study on Multi-Scale Window Determination for GLCM Texture Description in High-Resolution Remote Sensing Image Geo-Analysis Supported by GIS and Domain Knowledge.” ISPRS International Journal of Geo-Information 7 (5): 175.\n\n\nMoya, Luis, Homa Zakeri, Fumio Yamazaki, Wen Liu, Erick Mas, and Shunichi Koshimura. 2019. “3D Gray Level Co-Occurrence Matrix and Its Application to Identifying Collapsed Buildings.” ISPRS Journal of Photogrammetry and Remote Sensing 149: 14–28.\n\n\nPham-Duc, Binh, Ho Nguyen, Hien Phan, and Quan Tran-Anh. 2023. “Trends and Applications of Google Earth Engine in Remote Sensing and Earth Science Research: A Bibliometric Analysis Using Scopus Database.” Earth Science Informatics 16 (3): 2355–71.\n\n\nReis, Rui S, Nuno Datia, and Matilde Pós-de-Mina Pato. 2020. “A Primer on Understanding Google Earth Engine APIs.” I-ETC: ISEL Academic Journal of Electronics, Telecommunications and Computers 6 (1): 1–11.\n\n\nTassi, Andrea, and Marco Vizzari. 2020. “Object-Oriented Lulc Classification in Google Earth Engine Combining Snic, Glcm, and Machine Learning Algorithms.” Remote Sensing 12 (22): 3776."
  },
  {
    "objectID": "week8.html#summary",
    "href": "week8.html#summary",
    "title": "Week 8 Classification with Google Earth Engine II",
    "section": "Summary",
    "text": "Summary\nWith our quest in classifying land cover types from remote sensing imagery continued, this week’s lecture also emphasised on the significance of accuracy and other performance measures of classification. First, we were introduced to pre-classified datasets available online. I was impressed by the scale of Dynamic World, which uses a CNN-based classification model on Sentinel-2 imagery. Such sources highlights the evolution and ever-growing field of integrating AI or GeoAI with remote sensing.\n\nRemarks on the Weekly Practical\nThis week’s practical is an enhanced version of last week’s version, GEE limitation, GEE in-house classification methods seems to"
  },
  {
    "objectID": "week8.html#literature-review-and-applications",
    "href": "week8.html#literature-review-and-applications",
    "title": "Week 8 Classification with Google Earth Engine II",
    "section": "Literature Review and Applications",
    "text": "Literature Review and Applications\nIn this week, I have investigated more on the temporal complexity of classifying remote sensing imagery data.\nDetailed (Truong et al. 2024)\nLook into the landuse data, the paper can be found here."
  },
  {
    "objectID": "week8.html#more-practical-on-water-identifications",
    "href": "week8.html#more-practical-on-water-identifications",
    "title": "Week 8 Classification with Google Earth Engine II",
    "section": "More Practical on Water Identifications",
    "text": "More Practical on Water Identifications\nI was curious about the identification of water back in the week 7 practical, while searching online, I discovered this an open GEE module to identify water with deep learning developed by: GEE StudyRoom on CSDN (CSDN is the Chinese version of StackOverflow). The code to use the module is:\nvar dataset_lv2 = ee.FeatureCollection(\"FAO/GAUL/2015/level2\");\n\nvar Bristol_point = ee.Geometry.Point([-2.5879, 51.4545]);\n\n/*\nThe visualization and image selected ommitted\n*/\n\nvar model = require('users/nietaoyuan/aGEECommonModule:geeDLWaterModel.js');\nvar imgPredict = model.waterModel(imgMedian,roi);\nMap.addLayer(imgPredict,{'palette':'red'},'water');\nNow we are able to use\n\n\n\n\nResult with image layer transparent\n\n\n\n\n\nResult on image layer"
  },
  {
    "objectID": "week8.html#literature-review",
    "href": "week8.html#literature-review",
    "title": "Week 8 Classification with Google Earth Engine II",
    "section": "Literature Review",
    "text": "Literature Review\nasd\nasdas\nLiasd"
  },
  {
    "objectID": "week8.html#reflection",
    "href": "week8.html#reflection",
    "title": "Week 8 Classification with Google Earth Engine II",
    "section": "Reflection",
    "text": "Reflection\nGEEStyduRoom is just one of the many well-established GEE user-organised online communities that contributes to my learning this semester.\n\n\n\n\nTruong, Van Thinh, Sota Hirayama, Duong Cao Phan, Thanh Tung Hoang, Takeo Tadono, and Kenlo Nishida Nasahara. 2024. “JAXA’s New High-Resolution Land Use Land Cover Map for Vietnam Using a Time-Feature Convolutional Neural Network.” Scientific Reports 14 (1): 3926."
  },
  {
    "objectID": "week9.html#introduction-to-sar",
    "href": "week9.html#introduction-to-sar",
    "title": "Week 9 Synthetic Aperture Radar",
    "section": "Introduction to SAR",
    "text": "Introduction to SAR\nSAR is a powerful remote sensing method for gathering high-resolution images of the Earth’s surface.\nThis charac\ncan collect data using different polarization.\n\nSAR Overview\n\nA1\n\n\nAperture\nLonger Apertures = Finer Resolution\n\n\n\nOptical vs. Radar Remote Sensing\nPros and Cons\nPenetrating cloud cover\ngetting the SAR collections in GEE:\n\n\nRecent Research\nembed js code:\n\n\nPractical: Running SAR Analysis\nUsing Ollie’s code for Blast Damage Assessment, I was able to assess the 2019 Xiangshui Chemical Plant Explosion which happened in Jiangsu Province, China.\n\n\n\n\nSentinel 2 Image Before Explosion\n\n\n\n\n\nSentinel 2 Image After Explosion\n\n\n\n\n\nOverall Assessment Area\n\n\n\n\n\nZoomed-in to the Factory and Surrounding Residential Area\n\n\n\nThere is no open-building-footprint datasets that is freely available via GEE, I will be searching for available data to perform more detailed reports later on.\nThe results are published in a GEE App, with layer of 2018 and 2022 Sentinel Images of the explosion site."
  },
  {
    "objectID": "week10.html",
    "href": "week10.html",
    "title": "Week 10 Group Presentations",
    "section": "",
    "text": "This week we had two intense sessions packed with interesting remote sensing pitch-style presentations. The following is a Google Earth Engine App that I have made for viewing parts of the intended raw data for our pitch. The app may take a while to load as the Google Open Buildings dataset is large.\n\n\n\n\nOur team’s name is developed from Roman Goddess of Flowers and Olympian Goddess of the harvest and agriculture. The slides are here.\nWe are honored to be invited to ask question for TeamXYZ and their pitch on an Air Pollution management solution-suite designed for Mumbai. We asked about their future plans and the reply was solid and informative, their overall design and open approach to stakeholder engagement plans are quite impressive.\n\n\n\nMe Presenting Our Methodology Design"
  },
  {
    "objectID": "summary.html#remote-sensing-basics-machine-learning-and-gee",
    "href": "summary.html#remote-sensing-basics-machine-learning-and-gee",
    "title": "Remote Sensing Term Recap",
    "section": "Remote Sensing Basics, Machine Learning and GEE",
    "text": "Remote Sensing Basics, Machine Learning and GEE\n\nPCA explained by Hyeon Gu Kim\nLecture series on GIS and remote sensing\n\n\n\nClipping Polygons in GEE by Youtuber Fech\nGEE data catalog\nGEE community catalog\nGEE polygon\nGEE upload your own shapefile\nGEE Map by Professor. Qiusheng Wu\nGEE slider (feature to be added to Jakarta NDVI/Building Lookup APP later )"
  },
  {
    "objectID": "summary.html#html-css-and-quarto",
    "href": "summary.html#html-css-and-quarto",
    "title": "Remote Sensing Term Recap",
    "section": "HTML, CSS and Quarto",
    "text": "HTML, CSS and Quarto\nHere are a list of resources that I have used and adapted for this Quarto book:\n\nEmbedding multiple images\nEmbedding videos\nData visualisation in quarto with Observable JS\nGeneral introduction to CSS"
  },
  {
    "objectID": "summary.html#future-works",
    "href": "summary.html#future-works",
    "title": "Remote Sensing Term Recap",
    "section": "Future Works",
    "text": "Future Works\nThe trend now seems to be building application with Python + GEE, therefore, I am planning to use the Python / GEE tool suite for parts of the visualization work with thesis work. Hopefully it will be interactive and with a touch of satellite images showing the road networks for the transport in a city."
  },
  {
    "objectID": "rs_acronym.html",
    "href": "rs_acronym.html",
    "title": "Acronyms and Key Concepts in Remote Sensing",
    "section": "",
    "text": "AOI: Area of Interest.\nARD: Analysis Ready Data\nARVI: Atmospherically Resistant Vegetation Index.\nAVHRR: Advanced Very High Resolution Radiometer.\nAzimuth: This is the angle between a reference direction and the line between the observer/sensor and a specific point of interest on the ground.\nCART: Classification and Regression Trees builds a tree-like structure where splits are based on the features that yield optimal separation the data.\nDEM/DSM/DTM: Digital Elevation Model / Digital Surface Model / Digital Terrain Model. This article and explains well the differences between DEM, DTM and DSM.\nEO: Earth Observation, usually used as “EO Data”.\nEOS: Earth Observing System.\nESA: European Space Agency.\nEVI: Enhanced Vegetation Index.\nGIS: Geographic Information System.\nGAUL: Global Administrative Unit Layer is defined: “compiles and disseminates the best available information on administrative units for all the countries in the world, providing a contribution to the standardization of the spatial dataset representing administrative units.”.\nGLCM: Gray-Level Co-occurrence Matrix. This lecture note is really helpful in understanding the details of GLCM.\nGNDVI: Green Normalized Difference Vegetation Index.\nGOES: Geostationary Operational Environmental Satellite.\nGSO: Geosynchronous Orbit, this is an orbit where, from the observer’s (directly on the ground below the point of the object in orbit) reference frame, the object in orbit is stationary. More detailed explanation is found here.\nIFOV: Instantaneous Field of View. Refers to the area on the ground, that a sensor in orbit can see and measure whenever it is ordered to.\nLandSat: Land Satellite, more details on deciding which Landsat collection to import see here.\nLiDAR: Light Detection and Ranging.\nLULC: Land Use and Land Cover.\nMODIS: Moderate Resolution Imaging Spectroradiometer.\nMSAVI: Modified Soil-Adjusted Vegetation Index.\nNDMI: Normalized Difference Moisture Index..\nNDVI: Normalized Difference Vegetation Index.\nNDWI: Normalized Difference Water Index.\nNOAA: National Oceanic and Atmospheric Administration.\nPCA: Principal Component Analysis, a statistical technique used to reduce data dimensionality, prioritising significant features by transforming correlated bands into uncorrelated components.\nPOI: Point of Interest.\nRF: Random Forest.\nROI: Region of Interest.\nSAVI: Soil-Adjusted Vegetation Index.\nSNAP: Sentinel Application Platform, a software platform used for processing and analyzing satellite data, mostly European Space Agency’s Sentinel series Earth Observation data.\nSVM: Support Vector Machine.\nUAV: Unmanned Aerial Vehicle."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Alosaimi, Najd, Haikel Alhichri, Yakoub Bazi, Belgacem Ben Youssef, and\nNaif Alajlan. 2023. “Self-Supervised Learning for Remote Sensing\nScene Classification Under the Few Shot Scenario.” Scientific\nReports 13 (1): 433.\n\n\nAmani, Meisam, Arsalan Ghorbanian, Seyed Ali Ahmadi, Mohammad Kakooei,\nArmin Moghimi, S Mohammad Mirmazloumi, Sayyed Hamed Alizadeh Moghaddam,\net al. 2020. “Google Earth Engine Cloud Computing Platform for\nRemote Sensing Big Data Applications: A Comprehensive Review.”\nIEEE Journal of Selected Topics in Applied Earth Observations and\nRemote Sensing 13: 5326–50.\n\n\nBatty, Michael. 2013. “Big Data, Smart Cities and City\nPlanning.” Dialogues in Human Geography 3 (3): 274–79.\n\n\nBechle, Matthew J, Dylan B Millet, and Julian D Marshall. 2013.\n“Remote Sensing of Exposure to NO2: Satellite Versus Ground-Based\nMeasurement in a Large Urban Area.” Atmospheric\nEnvironment 69: 345–53.\n\n\nBhatta, Basudeb, S Saraswati, and D Bandyopadhyay. 2010. “Urban\nSprawl Measurement from Remote Sensing Data.” Applied\nGeography 30 (4): 731–40.\n\n\nBoser, Bernhard E, Isabelle M Guyon, and Vladimir N Vapnik. 1992.\n“A Training Algorithm for Optimal Margin Classifiers.” In\nProceedings of the Fifth Annual Workshop on Computational Learning\nTheory, 144–52.\n\n\nBreiman, L., J. Friedman, C. J. Stone, and R. A. Olshen. 1984.\nClassification and Regression Trees. Taylor & Francis. https://books.google.co.uk/books?id=JwQx-WOmSyQC.\n\n\nBudde, Matthias, Till Riedel, Michael Beigl, Klaus Schäfer, Stefan\nEmeis, Josef Cyrys, Jürgen Schnelle-Kreis, et al. 2017.\n“SmartAQnet: Remote and in-Situ Sensing of Urban Air\nQuality.” In Remote Sensing of Clouds and the Atmosphere\nXXII, 10424:19–26. SPIE.\n\n\nCardille, Jeffrey A, Nicholas E Clinton, Morgan A Crowley, and David S\nSaah. 2022. “Cloud-Based Remote Sensing with Google Earth Engine:\nProcess and Prospects from a Large Edited Open-Access Book.” In\nAGU Fall Meeting Abstracts, 2022:ED32D–0552.\n\n\nChristaki, Marianna, Christos Vasilakos, Ermioni-Eirini Papadopoulou,\nGeorgios Tataris, Ilias Siarkos, and Nikolaos Soulakellis. 2022.\n“Building Change Detection Based on a Gray-Level Co-Occurrence\nMatrix and Artificial Neural Networks.” Drones 6 (12):\n414.\n\n\nGao, Bo-Cai. 1996. “NDWI—a Normalized Difference Water Index for\nRemote Sensing of Vegetation Liquid Water from Space.” Remote\nSensing of Environment 58 (3): 257–66.\n\n\nGeng, Guannan, Qingyang Xiao, Shigan Liu, Xiaodong Liu, Jing Cheng,\nYixuan Zheng, Tao Xue, et al. 2021. “Tracking Air Pollution in\nChina: Near Real-Time PM2. 5 Retrievals from Multisource Data\nFusion.” Environmental Science & Technology 55 (17):\n12106–15.\n\n\nGhaffarian, Saman, Norman Kerle, and Tatiana Filatova. 2018.\n“Remote Sensing-Based Proxies for Urban Disaster Risk Management\nand Resilience: A Review.” Remote Sensing 10 (11): 1760.\n\n\nGitelson, Anatoly A, Yoram J Kaufman, and Mark N Merzlyak. 1996.\n“Use of a Green Channel in Remote Sensing of Global Vegetation\nfrom EOS-MODIS.” Remote Sensing of Environment 58 (3):\n289–98.\n\n\nGorelick, Noel, Matt Hancher, Mike Dixon, Simon Ilyushchenko, David\nThau, and Rebecca Moore. 2017. “Google Earth Engine:\nPlanetary-Scale Geospatial Analysis for Everyone.” Remote\nSensing of Environment 202: 18–27.\n\n\nGupta, Pawan, Sundar A Christopher, Jun Wang, Robert Gehrig, YC Lee, and\nNaresh Kumar. 2006. “Satellite Remote Sensing of Particulate\nMatter and Air Quality Assessment over Global Cities.”\nAtmospheric Environment 40 (30): 5880–92.\n\n\nHansen, Matthew C, Peter V Potapov, Rebecca Moore, Matt Hancher,\nSvetlana A Turubanova, Alexandra Tyukavina, David Thau, et al. 2013.\n“High-Resolution Global Maps of 21st-Century Forest Cover\nChange.” Science 342 (6160): 850–53.\n\n\nHuang, Yuhan, Casey KC Lee, Yat-Shing Yam, Wai-Chuen Mok, John L Zhou,\nYuan Zhuang, Nic C Surawski, Bruce Organ, and Edward FC Chan. 2022.\n“Rapid Detection of High-Emitting Vehicles by on-Road Remote\nSensing Technology Improves Urban Air Quality.” Science\nAdvances 8 (5): eabl7575.\n\n\nHuete, Alfredo, Kamel Didan, Tomoaki Miura, E Patricia Rodriguez, Xiang\nGao, and Laerte G Ferreira. 2002. “Overview of the Radiometric and\nBiophysical Performance of the MODIS Vegetation Indices.”\nRemote Sensing of Environment 83 (1-2): 195–213.\n\n\nHuete, AR. 1988. “A Soil-Adjusted Vegetation Index (SAVI). Remote\nSsensing of Environment, 25, 295-309.”\n\n\nJensen, John R. 1996. “Introductory Digital Image Processing: A\nRemote Sensing Perspective.”\n\n\nKaufman, Yoram J, and Didier Tanre. 1992. “Atmospherically\nResistant Vegetation Index (ARVI) for EOS-MODIS.” IEEE\nTransactions on Geoscience and Remote Sensing 30 (2): 261–70.\n\n\nKochanski, Adam K, Farren Herron-Thorpe, Derek V Mallia, Jan Mandel, and\nJoseph K Vaughan. 2021. “Integration of a Coupled Fire-Atmosphere\nModel into a Regional Air Quality Forecasting System for Wildfire\nEvents.” Frontiers in Forests and Global Change 4:\n728726.\n\n\nLan, Zeying, and Yang Liu. 2018. “Study on Multi-Scale Window\nDetermination for GLCM Texture Description in High-Resolution Remote\nSensing Image Geo-Analysis Supported by GIS and Domain\nKnowledge.” ISPRS International Journal of\nGeo-Information 7 (5): 175.\n\n\nLi, Linyi, Tingbao Xu, and Yun Chen. 2016. “Improved Urban\nFlooding Mapping from Remote Sensing Images Using Generalized Regression\nNeural Network-Based Super-Resolution Algorithm.” Remote\nSensing 8 (8): 625.\n\n\nMa, Lei, Yu Liu, Xueliang Zhang, Yuanxin Ye, Gaofei Yin, and Brian Alan\nJohnson. 2019. “Deep Learning in Remote Sensing Applications: A\nMeta-Analysis and Review.” ISPRS Journal of Photogrammetry\nand Remote Sensing 152: 166–77.\n\n\nMacLachlan, Andrew, Eloise Biggs, Gareth Roberts, and Bryan Boruff.\n2017. “Urban Growth Dynamics in Perth, Western Australia: Using\nApplied Remote Sensing for Sustainable Future Planning.”\nLand 6 (1): 9.\n\n\nMilne, Anthony Kinnaird. 1986. “The Use of Remote Sensing in\nMapping and Monitoring Vegetational Change Associated with Bushfire\nEvents in Eastern Australia.” Geocarto International 1\n(1): 25–32.\n\n\nMoya, Luis, Homa Zakeri, Fumio Yamazaki, Wen Liu, Erick Mas, and\nShunichi Koshimura. 2019. “3D Gray Level Co-Occurrence Matrix and\nIts Application to Identifying Collapsed Buildings.” ISPRS\nJournal of Photogrammetry and Remote Sensing 149: 14–28.\n\n\nNavalgund, Ranganath R, V Jayaraman, and PS Roy. 2007. “Remote\nSensing Applications: An Overview.” Current Science,\n1747–66.\n\n\nPham-Duc, Binh, Ho Nguyen, Hien Phan, and Quan Tran-Anh. 2023.\n“Trends and Applications of Google Earth Engine in Remote Sensing\nand Earth Science Research: A Bibliometric Analysis Using Scopus\nDatabase.” Earth Science Informatics 16 (3): 2355–71.\n\n\nPrados, Ana I, Gregory Leptoukh, Chris Lynnes, James Johnson, Hualan\nRui, Aijun Chen, and Rudolf B Husar. 2010. “Access, Visualization,\nand Interoperability of Air Quality Remote Sensing Data Sets via the\nGiovanni Online Tool.” IEEE Journal of Selected Topics in\nApplied Earth Observations and Remote Sensing 3 (3): 359–70.\n\n\nQi, Jiaguo, Abdelghani Chehbouni, Alfredo R Huete, Yann H Kerr, and\nSoroosh Sorooshian. 1994. “A Modified Soil Adjusted Vegetation\nIndex.” Remote Sensing of Environment 48 (2): 119–26.\n\n\nReis, Rui S, Nuno Datia, and Matilde Pós-de-Mina Pato. 2020. “A\nPrimer on Understanding Google Earth Engine APIs.” I-ETC:\nISEL Academic Journal of Electronics, Telecommunications and\nComputers 6 (1): 1–11.\n\n\nSabri, S, A Rajabifard, Y Chen, N Chen, and H Sheng. 2022.\n“Editorial: Geospatial Understanding of Sustainable Urban\nAnalytics Using Remote Sensing. Remote Sens. 2022, 14, 2748.” s\nNote: MDPI stays neutral with regard to jurisdictional claims in\npublished ….\n\n\nSokhi, Ranjeet S, Nicolas Moussiopoulos, Alexander Baklanov, John\nBartzis, Isabelle Coll, Sandro Finardi, Rainer Friedrich, et al. 2022.\n“Advances in Air Quality Research–Current and Emerging\nChallenges.” Atmospheric Chemistry and Physics 22 (7):\n4615–4703.\n\n\nStefanov, William L, and Anthony J Brazel. 2007. “Challenges in\nCharacterizing and Mitigating Urban Heat Islands—a Role for Integrated\nApproaches Including Remote Sensing.” In Applied Remote\nSensing for Urban Planning, Governance and Sustainability, 117–35.\nSpringer.\n\n\nSutlieff, Gary, Lucy Berthoud, and Mark Stinchcombe. 2021. “Using\nSatellite Data for CBRN (Chemical, Biological, Radiological, and\nNuclear) Threat Detection, Monitoring, and Modelling.”\nSurveys in Geophysics 42: 727–55.\n\n\nTassi, Andrea, and Marco Vizzari. 2020. “Object-Oriented Lulc\nClassification in Google Earth Engine Combining Snic, Glcm, and Machine\nLearning Algorithms.” Remote Sensing 12 (22): 3776.\n\n\nTiernan, Finbar, and Eamonn O’Mallon. 2020. “Australia’s 2019–20\nBushfire Season.” The Canberra Times, January. https://www.canberratimes.com.au/story/6600958/australias-2019-20-bushfire-season/.\n\n\nTrinder, John, and Qingxiang Liu. 2020. “Assessing Environmental\nImpacts of Urban Growth Using Remote Sensing.” Geo-Spatial\nInformation Science 23 (1): 20–39. https://doi.org/10.1080/10095020.2019.1710438.\n\n\nVan Donkelaar, Aaron, Randall V Martin, Michael Brauer, and Brian L\nBoys. 2015. “Use of Satellite Observations for Long-Term Exposure\nAssessment of Global Concentrations of Fine Particulate Matter.”\nEnvironmental Health Perspectives 123 (2): 135–43.\n\n\nVan Westen, CJ. 2000. “Remote Sensing for Natural Disaster\nManagement.” International Archives of Photogrammetry and\nRemote Sensing 33 (B7/4; PART 7): 1609–17.\n\n\nVeefkind, P, RF Van Oss, H Eskes, Annette Borowiak, F Dentner, and\nJulian Wilson. 2007. “The Applicability of Remote Sensing in the\nField of Air Pollution.” Institute for Environment and\nSustainability, Italy 59: 1–54.\n\n\nVerma, Sunita, Tanu Gangwar, Janhavi Singh, Divya Prakash, and Swagata\nPayra. 2023. “Urban Air Quality Monitoring and Modelling Using\nGround Monitoring, Remote Sensing, and GIS.” In Geospatial\nAnalytics for Environmental Pollution Modeling: Analysis, Control and\nManagement, 213–47. Springer.\n\n\nVictoria, Environmental Protection Authority. 2019. “EPA\nAirWatc.” https://www.epa.vic.gov.au/for-community/airwatch.\n\n\nWang, Qingyi, Shenhao Wang, Yunhan Zheng, Hongzhou Lin, Xiaohu Zhang,\nJinhua Zhao, and Joan Walker. 2024. “Deep Hybrid Model with\nSatellite Imagery: How to Combine Demand Modeling and Computer Vision\nfor Travel Behavior Analysis?” Transportation Research Part\nB: Methodological 179: 102869.\n\n\nWilkinson, Graeme G. 1999. “Recent Developments in Remote Sensing\nTechnology and the Importance of Computer Vision Analysis\nTechniques.” Machine Vision and Advanced Image Processing in\nRemote Sensing: Proceedings of Concerted Action MAVIRIC (Machine Vision\nin Remotely Sensed Image Comprehension), 5–11.\n\n\nWilson, Emily Hoffhine, and Steven A Sader. 2002. “Detection of\nForest Harvest Type Using Multiple Dates of Landsat TM Imagery.”\nRemote Sensing of Environment 80 (3): 385–96.\n\n\nWinstral, Adam, Kelly Elder, and Robert E Davis. 2002. “Spatial\nSnow Modeling of Wind-Redistributed Snow Using Terrain-Based\nParameters.” Journal of Hydrometeorology 3 (5): 524–38.\n\n\nXue, Jinru, Baofeng Su, et al. 2017. “Significant Remote Sensing\nVegetation Indices: A Review of Developments and Applications.”\nJournal of Sensors 2017.\n\n\nYan, Peiyao, Feng He, Yajie Yang, and Fei Hu. 2020.\n“Semi-Supervised Representation Learning for Remote Sensing Image\nClassification Based on Generative Adversarial Networks.”\nIEEE Access 8: 54135–44.\n\n\nZhou, Jie, Li Jia, Massimo Menenti, and Xuan Liu. 2021. “Optimal\nEstimate of Global Biome—Specific Parameter Settings to Reconstruct NDVI\nTime Series with the Harmonic ANalysis of Time Series (HANTS)\nMethod.” Remote Sensing 13 (21): 4251.\n\n\nZhu, Zhe. 2017. “Change Detection Using Landsat Time Series: A\nReview of Frequencies, Preprocessing, Algorithms, and\nApplications.” ISPRS Journal of Photogrammetry and Remote\nSensing 130: 370–84."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CASA0023 Remote Sensing Learning Diary",
    "section": "",
    "text": "Overview\nThis is the learning diary for CASA0023 Remotely Sensing Cities and Environments."
  },
  {
    "objectID": "index.html#table-of-content",
    "href": "index.html#table-of-content",
    "title": "CASA0023 Remote Sensing Learning Diary",
    "section": "Table of Content",
    "text": "Table of Content\n\nPersonal Introduction\nPart I. Intro to Remote Sensing\n\nWeek 1 Introduction to Remote Sensing\nWeek 2 Quarto and Xaringan\nWeek 3 Atmosphere Corrections\nWeek 4 Remote Sensing and Policy Implications\nWeek 5 More on Temperature and Group Work Discussion\n\nPart II. Remote Sensing with GEE\n\nWeek 6 Introduction to Google Earth Engine\nWeek 7 Classification with GEE Part 1\nWeek 8 Classification with GEE Part 2\nWeek 9 Synthetic Aperture Radar\n\nWeek 10 Group Presentation Week\nSummary\nRemote Sensing Acronyms\nReference"
  },
  {
    "objectID": "week7.html#classification-in-remote-sensing",
    "href": "week7.html#classification-in-remote-sensing",
    "title": "Week 7 Classification with Google Earth Engine I",
    "section": "Classification in Remote Sensing",
    "text": "Classification in Remote Sensing\nClassification plays a key role in information retrieval from the satellite images covering a wide range of applications. The first is identifying land cover types such as water, forest, grasslands, high/low urban areas, and agricultural fields. This application is crucial for environmental monitoring. Currently the researchers are employing deep learning techniques on RS classifications, with many pivotal works using methods such as Convolutional Neural Networks (CNN), autoencoders and Boltzmann Machines. (Yuan, Shi, and Gu 2021) Another important application with regard to urban context is to monitor the urban sprawl, mapping infrastructural changes.\nHowever, there are major challenges to the current development in remote sensing classification:\n\nLabelled Data Volume and Accuracy: The overarching reliance on supervised techniques such as CNN resulted in an ever-increasing demand of properly labelled data which is made more challenging by the fact that the complexity of remote sensing data makes extremely difficult to label. (Cheng et al. 2020)\nObject Class Imbalances: EO data naturally have imbalanced classes in which certain classes are more prevalent than other classes which is risky for training models, (Wang et al. 2024) for instance, in a RS image of a city, the buildings will be more common than water. This can lead to the models performing well on popular classes and losing accuracy for rare cases similar to what happened in my practical run, where the model failed to identify the water in Bristol.\nComputing Cost: Performing model training for more complicated models cost more energy and time, generating more demand on high-performance computation resources. (Tornede et al. 2023)\n\nTherefore, paradigms such as Self-Supervised Learning, which can perform consistently on small amount of labelled data, are gaining more momentum recently. (Alosaimi et al. 2023) Further investigation into such areas, including Semi-Supervised Learning, is necessary and crucial for lowering the demand on labelling datasets. The innovation at the framework level for existing models is also important, for example, CMID (Contrastive Mask Image Distillation) is proposed to offer a novel Self-Supervised Learning approach that performs more consistently across various tasks types. (Muhtar et al. 2023) Another prespective is that innovation could also focus on is the improvement of understanding the RS image series, with potential integration of reinforcement learning (Subramanian and Crowley 2017) to optimise model performance across time which I will focus in week 8."
  },
  {
    "objectID": "week8.html#remarks-on-the-weekly-practical",
    "href": "week8.html#remarks-on-the-weekly-practical",
    "title": "Week 8 Classification with Google Earth Engine II",
    "section": "Remarks on the Weekly Practical",
    "text": "Remarks on the Weekly Practical"
  },
  {
    "objectID": "week8.html#more-practical-on-identification-of-water-bodies",
    "href": "week8.html#more-practical-on-identification-of-water-bodies",
    "title": "Week 8 Classification with Google Earth Engine II",
    "section": "More Practical on Identification of Water Bodies",
    "text": "More Practical on Identification of Water Bodies\nI was curious about the identification of water back in the week 7 practical, while searching online, I discovered this an open GEE module to identify water with deep learning developed by: GEE StudyRoom on CSDN (CSDN is the Chinese version of StackOverflow). The code to use the module is:\nvar dataset_lv2 = ee.FeatureCollection(\"FAO/GAUL/2015/level2\");\n\nvar Bristol_point = ee.Geometry.Point([-2.5879, 51.4545]);\n\n/*\nThe visualization and image selected ommitted\n*/\n\nvar model = require('users/nietaoyuan/aGEECommonModule:geeDLWaterModel.js');\nvar imgPredict = model.waterModel(imgMedian,roi);\nMap.addLayer(imgPredict,{'palette':'red'},'water');\nWith the help of deep learning, (potentially CNN, the author did not give details to their published module) the accuracy of identifying water has improved significantly, some creeks still not identified, but the overall performance in water body identification is much better.\n\n\n\n\nResult with image layer transparent\n\n\n\n\n\nResult on image layer\n\n\n\nDetailed result presented in a GEE App:"
  },
  {
    "objectID": "week9.html#sar-and-its-application-in-research",
    "href": "week9.html#sar-and-its-application-in-research",
    "title": "Week 9 Synthetic Aperture Radar",
    "section": "SAR and Its Application in Research",
    "text": "SAR and Its Application in Research\nSAR in the current literature"
  },
  {
    "objectID": "week9.html#reflection",
    "href": "week9.html#reflection",
    "title": "Week 9 Synthetic Aperture Radar",
    "section": "Reflection",
    "text": "Reflection"
  }
]