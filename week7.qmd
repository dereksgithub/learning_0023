# Week 7 Classification with Google Earth Engine I {.unnumbered}

For this week's diary, find studies that use GEE with classification

Existing study using classification GEE, critique/pros/cons

note down some of the classifiers, and describe in your own words.

Summarize how I would like to use for other research

Your own case not the focus,

theory extension / literature

A couple of paragraphs on each classification method.

RF – Boosting — CNN

1.     Add more Critiques to the literature you reviewed, extend the current research.

2.     Structure the contents better, give a clear architecture on the right sides of the webpage.

## Classification Methods in Remote Sensing

In the context of remote sensing, there are several classification methods, ranging from logistic regression to

### CART

CART was originally proposed by Breiman et al. @breiman1984classification

### Random Forest

Consumer's/User's accuracy refers to the probability that a pixel classified into a given category actually represents that category on the ground. It is calculated for each class by dividing the number of correctly classified pixels (true positives) for that class by the total number of pixels that were classified into that class (both true positives and false positives).

Here's a more detailed breakdown:

-   **Correct (True Positives)**: The number of instances where the model correctly predicted a specific class.

-   **Total (True Positives + False Positives)**: The sum of instances where the model predicted a specific class (correctly or incorrectly).

The Resubstitution Error Matrix, also known as a confusion matrix or error matrix, is a specific type of confusion matrix generated by evaluating the classification algorithm on the same dataset that was used to train the model. In other words, it's the classification report you get when you test your model on the training data itself.

Here's what it typically includes:

-   **True Positives (TP)**: Correctly classified positive cases.

-   **True Negatives (TN)**: Correctly classified negative cases.

-   **False Positives (FP)**: Negative cases incorrectly classified as positive (Type I error).

-   **False Negatives (FN)**: Positive cases incorrectly classified as negative (Type II error).

The Resubstitution Error Matrix is a tool for understanding the performance of a classification model, and it's particularly useful for identifying the types of errors a model is making. However, because it uses the same data for both training and testing, it may not provide a realistic estimate of the model's performance on unseen data due to overfitting.

It's important to use this matrix with caution and to complement it with additional validation techniques, such as cross-validation or a test on a separate validation dataset, to evaluate the model's ability to generalize to new, unseen data.

### Practical Results Analysis

Pixel wise, 70/30 split,

how is it splitted?

::: {.image-container style="display: grid; grid-template-columns: repeat(2,1fr); grid-template-rows: repeat(2,1fr); gap:5px;"}
![Greater Bristol Clip Median](images/wk7/bristol_clip.png){width="340" height="290"}

![Comparison of Satellite Layer from GEE and Classification Result](images/wk7/comparison.png){width="340" height="290"}

![First Classification](images/wk7/basic_classifier.png){width="340" height="290"}

![Pixel-wise Classification](images/wk7/pixel_wise_classifier.png){width="340" height="290"}
:::

Overall results in GEE App:

<p align="left">

`<iframe title="gee_app_02" width="680" height="700" src="`{=html}https://ee-mengyu-derek-ding.projects.earthengine.app/view/bristolclassifierv1`" title="GEE App Bristol Look-up" frameborder="0">`{=html}

</iframe>

</p>

### Models Used in Practical

The pixel-wise approach divided the image pixel points randomly into training (70%) and validation (30%) sets. The training set was then fed to train a random forest to classify the image, and the the RF classifier classified the Bristol clip image to create a classified map.

### Interpreting the Model Metrics

Confusion matrix for training results:

$$
Confusion Matrix = \begin{bmatrix} 
\ 709&0&2&0&0&1 \\
\ 0&0&0&0&0&0 \\
\ 9&0&712&0&0&1 \\
\ 0&0&0&738&0&0 \\
\ 0&0&0&0&704&0 \\
\ 0&0&0&0&0&678 \\
\end{bmatrix}
$$

The overall training accuracy is calculated at:

$$
Training \ Accucracy = \frac{3541}{3554} = 99.63\%
$$

For testing the confusion matrix is:

$$
Confusion Matrix = \begin{bmatrix} 
\ 265&0&14&0&0&9 \\
\ 0&0&0&0&0&0 \\
\ 43&0&233&1&0&1 \\
\ 1&0&0&261&0&1 \\
\ 0&0&0&0&296&0 \\
\ 0&0&0&0&0&322 \\
\end{bmatrix}
$$

Testing Accuracy

$$
Validation \ Accucracy = \frac{1377}{1446} = 95.23\%
$$

This suggest a high testing accuracy, but it may also indicate over-fitting for we did not calibrate the model carefully and perform solid feature engineering.

## Outlook of SSL in Remote Sensing

The current development in the literature of remote sensing image classification, CNN and other supervised learning approaches, although have gained much attention in the past 10 years, may be limited. Large amount of labelled cat images is hard to acquire, not to mention the satellite images covering years of daily images over vast lands, thus small amount of labelled data paradigm will be promising. [@alosaimi2023self] The challenge, however, still lies in the fact that massive amount of data cannot be sole relied on unsupervised clustering methods, on top of that, iterative clustering methods lacks explanability too. Therefore, researcher checking out the usage of SSL do find out that

<https://pubmed.ncbi.nlm.nih.gov/36624136/#full-view-affiliation-1>

## Reflection

For this week's practical, I picked the water polygon from the sea near Bristol, on further remark, I realised that the model probably did not capture Bristol's rivers, due to the fact that I have selected sea water as sample class for water. Such a result may also stem from the fact that the Bristol river in the image was not significant, thus, such algorithms for now could improve performance for a finer resolution satellite image collection.

On further notice, I do find tree-based methods deeply rooted from traditional statistics and computer science. I could recall a concept named B-Tree that I learnt in the big data courses back then. With Upon reviewing the history of machine learning, I do realised how powerful inter-disciplinary research and studies can boost the development of an area of study. It felt like as if statistics is the Midas golden touch for ML to boom. It was only the combination of many subjects and paradigms that give birth to the modern statistical learning.
